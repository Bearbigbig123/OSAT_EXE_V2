import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from math import gcd, floor, ceil
from functools import reduce
from scipy.stats import skew, median_abs_deviation, kurtosis, johnsonsu

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class CLTightenCalculator:
    """Control Limit Tighten Calculator - ç®¡åˆ¶ç·šæ”¶ç·Šè¨ˆç®—å™¨"""
    
    def __init__(self, chart_info_path=None, raw_data_dir=None):
        """
        åˆå§‹åŒ– CL Tighten Calculator
        
        Args:
            chart_info_path: Chart è³‡è¨Šæª”æ¡ˆè·¯å¾‘
            raw_data_dir: åŸå§‹æ•¸æ“šç›®éŒ„è·¯å¾‘
        """
        self.chart_info_path = chart_info_path
        self.raw_data_dir = raw_data_dir
        self.results = []
        
    # === Utility Functions ===
    
    def compute_resolution(self, values):
        """SOP 1.3: ä¼°è¨ˆè³‡æ–™è§£æåº¦ (Resolution)"""
        if len(values) < 2: 
            return None
        sorted_vals = sorted(set(values))
        if len(sorted_vals) < 2: 
            return None
        
        diffs = [j - i for i, j in zip(sorted_vals[:-1], sorted_vals[1:])]
        non_zero_diffs = [d for d in diffs if d != 0]
        if not non_zero_diffs: 
            return None
        
        # æ”¾å¤§é¿å…æµ®é»æ•¸èª¤å·®ï¼Œç„¶å¾Œè¨ˆç®—æœ€å¤§å…¬ç´„æ•¸
        scaled_diffs = [int(round(d * 10000)) for d in non_zero_diffs]
        # ä½¿ç”¨ reduce(gcd, ...) è™•ç†åˆ—è¡¨
        res_scaled = reduce(gcd, scaled_diffs)
        return res_scaled / 10000

    def robust_zscore_sop2(self, values):
        """SOP 2.4/4.2: è¨ˆç®— Robust Z-score (zi)"""
        med = np.median(values)
        sd = np.std(values)
        if sd == 0: 
            return np.zeros(len(values))
            
        mad = median_abs_deviation(values) 
        
        if mad == 0:
            mean_dev = np.mean(np.abs(values - med))
            if mean_dev == 0: 
                return np.zeros(len(values))
            z = 0.7979 * np.abs(values - med) / mean_dev
        else:
            z = 0.6745 * np.abs(values - med) / mad
        return z

    def compute_CB(self, values):
        """æ¨™æº– Bimodality Coefficient (BC)"""
        N = len(values)
        if N < 4: 
            return 0 
        sk = skew(values, bias=False)
        ku = kurtosis(values, fisher=True) + 3 
        if ku <= 0: 
            return 0 
        return (sk**2 + 1) / ku

    def compute_robust_sigma(self, values):
        """SOP 4.1: ä¾è³‡æ–™ç­†æ•¸è¨ˆç®— UR/LR (Robust Sigma)"""
        N = len(values)
        P = np.percentile
        median_val = np.median(values)
        
        if N < 4: 
            return np.std(values), np.std(values)
            
        # SOP 4.1 Robust Sigma ä¼°è¨ˆé‚è¼¯
        if N >= 10000:
            UR = (P(values, 99.5) - median_val)/3.09
            LR = (median_val - P(values, 0.1))/3.09
        elif 3000 <= N < 10000:
            UR = (P(values, 99.5) - median_val)/2.576
            LR = (median_val - P(values, 0.5))/2.576
        elif 300 <= N < 3000:
            UR = (P(values, 95) - median_val)/2.326
            LR = (median_val - P(values, 1))/2.326
        elif 100 <= N < 300:
            UR = (P(values, 97) - median_val)/1.881
            LR = (median_val - P(values, 3))/1.881
        else: 
            UR = (P(values, 95) - median_val)/1.645
            LR = (median_val - P(values, 5))/1.645
            
        # ç¢ºä¿ä¸æœƒè¿”å›è² å€¼
        return max(0, UR), max(0, LR)

    # === SOP 1: Data Integrity ===

    def data_integrity(self, df, date_col, value_col, oos_col):
        """SOP 1.1 & 1.2: ç¯©é¸æœ€è¿‘2å¹´æœ‰æ•ˆè³‡æ–™ä¸¦æ’é™¤ OOS"""
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        cutoff = pd.Timestamp.today() - pd.DateOffset(years=2)
        df_filtered = df[df[date_col] >= cutoff].dropna(subset=[value_col])
        
        if oos_col in df_filtered.columns:
            # SOP 1.2: æ’é™¤ OOS é»
            df_filtered = df_filtered[~df_filtered[oos_col].astype(bool)]
        
        values = df_filtered[value_col].values
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
        if len(values) == 0:
            print(f"    [Warning] ç¶“éç¯©é¸å¾Œæ²’æœ‰æœ‰æ•ˆæ•¸æ“šé»")
            return values, None
            
        resolution = self.compute_resolution(values)
        return values, resolution

    # === Hard Rule Function ===

    def apply_discrete_hard_rules(self, values, resolution, N):
        if N < 4:
            return None, None, False, None 

        values = np.array(values)
        val_counts = pd.Series(values).value_counts()
        
        if val_counts.empty:
            return None, None, False, None

        mode_val = val_counts.idxmax()
        category_num = len(val_counts)
        
        # Rule 1: If Constant or non mode data# = 1
        non_mode_count = N - val_counts.max()
        if non_mode_count == 0 or non_mode_count == 1:
            return mode_val, mode_val, True, "Hard Rule 1: Constant/Near Constant"

        min_val = np.min(values)
        max_val = np.max(values)
        
        # Rule 2: If data value category# = 2
        if category_num == 2:
            return max_val, min_val, True, "Hard Rule 2: Two Categories"

        # Rule 3: If data value category# = 3 and max â€“ min = 2*resolution
        if category_num == 3 and resolution is not None and resolution > 0:
            if abs((max_val - min_val) - 2 * resolution) < 1e-6: 
                return max_val, min_val, True, "Hard Rule 3: Three Categories Spaced by Resolution"
                
        return None, None, False, None

    # === SOP 2 & 3: Pattern Diagnosis ===

    def data_prep_for_pattern(self, values):
        N_orig = len(values)
        if N_orig == 0: 
            return values
        if N_orig < 4: 
            return values
            
        val_counts = pd.Series(values).value_counts()
        mode_val = val_counts.idxmax()
        mode_count = val_counts.max()
        non_mode_count = N_orig - mode_count
        
        # SOP 2.1: æ•¸æ“šé¸æ“‡
        if non_mode_count / N_orig < 0.25:
            max_mode_use = min(mode_count, non_mode_count * 3)
            
            # ç‰¹æ®Šæƒ…æ³ï¼šç•¶æ‰€æœ‰æ•¸æ“šéƒ½ç›¸åŒæ™‚ï¼ˆnon_mode_count = 0ï¼‰ï¼Œç›´æ¥è¿”å›åŸæ•¸æ“š
            if non_mode_count == 0:
                print(f"    [Debug] data_prep_for_pattern: æ‰€æœ‰æ•¸æ“šéƒ½ç›¸åŒ ({mode_val})ï¼Œç›´æ¥è¿”å›åŸæ•¸æ“š")
                return values
                
            mode_data = values[values == mode_val][:max_mode_use]
            non_mode_data = values[values != mode_val]
            w = np.concatenate((mode_data, non_mode_data))
        else:
            w = values.copy()
            
        # æª¢æŸ¥ w æ˜¯å¦ç‚ºç©º
        if len(w) == 0:
            print(f"    [Debug] data_prep_for_pattern: é è™•ç†å¾Œæ•¸æ“šç‚ºç©ºï¼Œè¿”å›åŸæ•¸æ“š")
            return values
            
        # SOP 2.2: Min-Max æ¨™æº–åŒ–
        min_w, max_w = np.min(w), np.max(w)
        if min_w == max_w: 
            print(f"    [Debug] data_prep_for_pattern: æ•¸æ“šç„¡è®Šç•°æ€§ (min=max={min_w})ï¼Œè¿”å›åŸæ•¸æ“š")
            return values 
            
        x = (w - min_w) / (max_w - min_w)
        
        # SOP 2.3: Johnson SU Transformation
        try:
            params = johnsonsu.fit(x)
            y = johnsonsu.rvs(*params, size=len(x))
        except Exception:
            y = x
            
        # SOP 2.4/2.5: Robust Z-score & éæ¿¾ç•°å¸¸å€¼
        z = self.robust_zscore_sop2(y)
        
        # æ‰¾å‡º ZI > 6 çš„é»
        outlier_mask = z > 6.0
        outlier_indices = np.where(outlier_mask)[0]
        
        if len(outlier_indices) > 0:
            # æª¢æŸ¥æ¿¾é™¤æ¯”ä¾‹
            filter_ratio = len(outlier_indices) / len(w)
            
            if filter_ratio <= 0.05:  # 5% ä»¥å…§ï¼Œæ¿¾é™¤æ‰€æœ‰ç•°å¸¸å€¼
                w_filtered = w[~outlier_mask]
            else:  # è¶…é 5%ï¼Œåªæ¿¾é™¤æ‰€æœ‰æœ€å¤§ ZI çš„é»
                max_z_value = np.max(z)
                max_z_mask = z == max_z_value
                w_filtered = w[~max_z_mask]
        else:
            w_filtered = w  # æ²’æœ‰ç•°å¸¸å€¼
            
        return w_filtered

    def pattern_diagnosis(self, values, resolution=None):
        N = len(values)
        if N < 4: 
            return "Insufficient Data", np.nan, np.nan
        
        sigma = np.std(values)
        val_counts = pd.Series(values).value_counts()
        category_num = len(val_counts)
        sk = skew(values, bias=False)
        cb = self.compute_CB(values)
        
        if 4 <= N < 20: 
            if sigma == 0: 
                return "Constant", sk, cb
            
            near_constant_cond = (N - val_counts.max() == 1)
            attribute_cond = (category_num / N <= 1/3 and category_num <= 5)
            
            if near_constant_cond: 
                return "Near Constant", sk, cb
            if attribute_cond: 
                return "Attribute", sk, cb
            if category_num / N >= 1/2:
                if sk > 0.6: 
                    return "Skew-Right", sk, cb
                if sk < -0.6: 
                    return "Skew-Left", sk, cb
            return "Normal", sk, cb
        
        if N >= 20: 
            if sigma == 0: 
                return "Constant", sk, cb
                
            near_constant_cond = (val_counts.max() / N >= 0.9)
            
            attr_cond_A = (category_num / N <= 1/3 and category_num <= 5)
            attr_cond_B = (resolution and sigma != 0 and resolution / (6*sigma) >= 0.1 and category_num < 10)
            attr_cond_C = (N >= 30 and category_num <= 10)
            attribute_cond = (attr_cond_A or attr_cond_B or attr_cond_C)
            
            if near_constant_cond: 
                return "Near Constant", sk, cb
            if attribute_cond: 
                return "Attribute", sk, cb
                
            if cb > 0.6 and -0.6 <= sk <= 0.6: 
                return "Bimodal", sk, cb
                
            if category_num / N >= 1/2:
                if sk > 0.6: 
                    return "Skew-Right", sk, cb
                if sk < -0.6: 
                    return "Skew-Left", sk, cb
            return "Normal", sk, cb
        return "Normal", sk, cb

    # === SOP 4: Outlier Exclusion ===

    def outlier_filter(self, values, pattern):
        values = np.array(values)
        N_orig = len(values)
        if N_orig == 0 or np.std(values) == 0: 
            return values
        
        if pattern in ["Skew-Right","Skew-Left"]:
            # SOP 4.1: Skew Pattern - 6Ïƒ Robust Sigma æ¿¾é™¤
            median_val = np.median(values)
            U_R, L_R = self.compute_robust_sigma(values)
            
            mask = (values <= median_val + 6*U_R) & (values >= median_val - 6*L_R)
            values_pre_filtered = values[mask]
            
            filter_ratio = (N_orig - len(values_pre_filtered)) / N_orig
            
            if filter_ratio <= 0.05:  # 5% ä»¥å…§ï¼Œä½¿ç”¨æ¿¾é™¤å¾Œçš„çµæœ
                return values_pre_filtered
            else:  # è¶…é 5%ï¼Œåªæ¿¾é™¤æ‰€æœ‰æœ€å¤§æ¯”ä¾‹çš„é»
                ratio_upper = (values - median_val) / U_R
                ratio_lower = (median_val - values) / L_R
                ratio = np.maximum(ratio_upper, ratio_lower) 
                max_ratio_value = np.max(ratio)
                max_ratio_mask = ratio == max_ratio_value
                values_filtered = values[~max_ratio_mask] 
                return values_filtered
        
        else:
            # SOP 4.2: Other Pattern - 4.5 Robust Z-score æ¿¾é™¤
            z = self.robust_zscore_sop2(values)
            values_pre_filtered = values[z <= 4.5] 
            
            filter_ratio = (N_orig - len(values_pre_filtered)) / N_orig
            
            if filter_ratio <= 0.05:  # 5% ä»¥å…§ï¼Œä½¿ç”¨æ¿¾é™¤å¾Œçš„çµæœ
                return values_pre_filtered
            else:  # è¶…é 5%ï¼Œåªæ¿¾é™¤æ‰€æœ‰æœ€å¤§ ZI çš„é»
                max_z_value = np.max(z)
                max_z_mask = z == max_z_value
                values_filtered = values[~max_z_mask]
                return values_filtered

    # === SOP 5: Control Limit Calculation ===

    def get_k_value(self, N, characteristic):
        """SOP 5.2: ä¾è³‡æ–™ç­†æ•¸å’Œç‰¹æ€§æ±ºå®š k å€¼"""
        if N >= 30:
            return 3.0
        if 16 <= N <= 29: 
            return 4.0
        if 4 <= N <= 15: 
            return 5.0
        return 3.0

    def calc_CL(self, values, pattern, resolution=None, characteristic='Nominal'):
        """SOP 5.1 & 5.3: è¨ˆç®— UCL/LCL"""
        N = len(values)
        if N < 4: 
            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan
            
        median_val = np.median(values)
        mean_val = np.mean(values)
        std_val = np.std(values)
        k = self.get_k_value(N, characteristic)
        
        # é å…ˆè¨ˆç®— Robust Sigma å’Œ ECDF 3-sigma å‚™ç”¨å€¼
        UR_robust, LR_robust = self.compute_robust_sigma(values)
        UCL3_ecdf, LCL3_ecdf = np.nan, np.nan 
        
        # --- 1. è™•ç† Normal æ¨¡å¼ (Mean +/- k*Std) ---
        if pattern == "Normal": 
            UCL = mean_val + k * std_val
            LCL = mean_val - k * std_val
        
        # --- 2. è™•ç† Skew æ¨¡å¼ (Median +/- k*RobustSigma) ---
        elif pattern in ["Skew-Right", "Skew-Left"]: 
            UCL = median_val + k * UR_robust
            LCL = median_val - k * LR_robust
            
        # --- 3. è™•ç† ECDF ç›¸é—œæ¨¡å¼ (Bimodal, Attribute, Constant, Near Constant) ---
        else:
            # 3.1 ECDF: å¼·åˆ¶ä½¿ç”¨ ECDF ç¢ºå®šåŸºç¤ 3-sigma æ°´ä½ (UCL3, LCL3)
            p_low_3sigma = 0.135
            p_high_3sigma = 99.865
            
            try:
                UCL3_ecdf = np.percentile(values, p_high_3sigma)
                LCL3_ecdf = np.percentile(values, p_low_3sigma)
            except Exception:
                UCL3_ecdf = median_val + 3 * UR_robust
                LCL3_ecdf = median_val - 3 * LR_robust

            # 3.2 Tolerance æ“´å±•é‚è¼¯
            T_upper = UCL3_ecdf - median_val
            T_lower = median_val - LCL3_ecdf
            T_max = max(T_upper, T_lower)
            
            if T_max <= 0:
                 UCL = UCL3_ecdf
                 LCL = LCL3_ecdf
            else:
                 T_new = T_max * (k / 3.0) 
                 UCL = median_val + T_new
                 LCL = median_val - T_new
                 
        return UCL, LCL, UR_robust, LR_robust, UCL3_ecdf, LCL3_ecdf

    # === SOP 6: Control Limit Adjustment & Tighten ===

    def adjust_CL_based_on_OOC(self, values, UCL, LCL, pattern, resolution, sigma_est_u, sigma_est_l, max_adj_units=2, characteristic='Nominal'):
        values = np.array(values)  # ç¡®ä¿ values æ˜¯ numpy array
        current_UCL, current_LCL = UCL, LCL
        N = len(values)
        
        sigma_u = sigma_est_u if sigma_est_u > 1e-9 else 1e-9
        sigma_l = sigma_est_l if sigma_est_l > 1e-9 else 1e-9

        if pattern == "Constant":
            adj_u = adj_l = 0.0
        elif pattern == "Near Constant" and resolution is not None and resolution > 0:
            # Near Constant æ¨¡å¼ä½¿ç”¨ resolution ä½œç‚ºèª¿æ•´å–®ä½
            adj_u = adj_l = resolution
        elif pattern == "Attribute" and resolution is not None and resolution > 0 and sigma_est_u == 0:
            # Attribute æ¨¡å¼ä½¿ç”¨ resolution ä½œç‚ºèª¿æ•´å–®ä½
            adj_u = adj_l = resolution
        else:
            # Normal, Skew-Right, Skew-Left, Bimodal éƒ½ä½¿ç”¨ 0.25Ïƒ
            adj_u = 0.25 * sigma_u
            adj_l = 0.25 * sigma_l
            
        # æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šæ˜¯å¦åªèª¿æ•´å–®é‚Š
        adjust_ucl = True
        adjust_lcl = True
        
        if characteristic == 'Smaller':
            # Smaller åªéœ€è¦ tighten UCL (ä¸Šé™)
            adjust_lcl = False
            adj_l = 0.0  # ä¸èª¿æ•´ LCL
        elif characteristic == 'Bigger':
            # Bigger åªéœ€è¦ tighten LCL (ä¸‹é™)
            adjust_ucl = False
            adj_u = 0.0  # ä¸èª¿æ•´ UCL
            
        # æ ¹æ®æ¨¡å¼è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°
        if pattern in ["Near Constant", "Attribute"]:
            # Near Constant å’Œ Attribute æ¨¡å¼æ¯æ¬¡è°ƒæ•´ 1*resolutionï¼Œæœ€å¤šè°ƒæ•´ max_adj_units æ¬¡
            max_iterations = max_adj_units if resolution is not None and resolution > 0 else 1
        else:
            max_iterations = int(max_adj_units / 0.25) + 1 if (sigma_u > 0 or sigma_l > 0) else 1
        
        initial_ooc_count = 0
        final_ooc_count = 0
        total_adj_units = 0

        for i in range(max_iterations):
            upper_ooc_mask = (values > current_UCL)
            lower_ooc_mask = (values < current_LCL)
            
            upper_ooc_count = np.sum(upper_ooc_mask)
            lower_ooc_count = np.sum(lower_ooc_mask)
            total_ooc_count = upper_ooc_count + lower_ooc_count
            
            if i == 0: 
                initial_ooc_count = total_ooc_count
            
            ooc_percent = total_ooc_count / N
            
            if ooc_percent <= 0.003 or total_ooc_count < 2: 
                final_ooc_count = total_ooc_count
                break
                
            if upper_ooc_count > 0 and adj_u > 0 and adjust_ucl:
                current_UCL += adj_u
                if pattern in ["Near Constant", "Attribute"]:
                    total_adj_units += 1  # Near Constant å’Œ Attribute æ¨¡å¼æ¯æ¬¡è°ƒæ•´ç®—ä½œ 1 ä¸ªå•ä½
                else:
                    total_adj_units += adj_u / sigma_u
            if lower_ooc_count > 0 and adj_l > 0 and adjust_lcl:
                current_LCL -= adj_l
                if pattern in ["Near Constant", "Attribute"]:
                    total_adj_units += 1  # Near Constant å’Œ Attribute æ¨¡å¼æ¯æ¬¡è°ƒæ•´ç®—ä½œ 1 ä¸ªå•ä½
                else:
                    total_adj_units += adj_l / sigma_l
                
            if upper_ooc_count == 0 and lower_ooc_count == 0:
                  final_ooc_count = total_ooc_count
                  break

        # SOP 6.3: æœ€çµ‚ CL æ ¹æ“š resolution ä¿®æ­£
        if resolution is not None and not np.isnan(current_UCL) and not np.isnan(current_LCL):
            if resolution == 0:
                decimals = 0
            elif isinstance(resolution, int):
                decimals = 0
            else:
                try:
                    res_parts = str(resolution).split('.')
                    if len(res_parts) > 1:
                        res_str = res_parts[1].rstrip('0')
                        decimals = len(res_str) if res_str else 0
                    else:
                        decimals = 0
                except:
                    decimals = 0
            
            if decimals >= 0:
                power_of_10 = 10**decimals
                current_UCL = floor(current_UCL * power_of_10) / power_of_10
                current_LCL = ceil(current_LCL * power_of_10) / power_of_10

            if current_LCL > current_UCL: 
                current_LCL = current_UCL
        

        return current_UCL, current_LCL, initial_ooc_count, final_ooc_count, total_adj_units

    def check_tighten(self, original_tol, new_tol, data_count):
        """SOP 6.4: åˆ¤æ–·æ˜¯å¦éœ€è¦ tighten (å®¹å·®æ¯”å°)"""
        tighten_flag, _, _ = self.check_tighten_with_details(original_tol, new_tol, data_count)
        return tighten_flag
    
    def check_tighten_with_details(self, original_tol, new_tol, data_count):
        """SOP 6.4: åˆ¤æ–·æ˜¯å¦éœ€è¦ tighten (å®¹å·®æ¯”å°) - è¿”å›è©³ç´°è³‡è¨Š"""
        N_pct_table = [(125,15),(70,18),(45,20),(30,25),(15,30),(10,35),(0,40)]
        N_pct = 40 
        for n_threshold, pct in N_pct_table:
            if data_count > n_threshold:
                N_pct = pct
                break
        
        if original_tol <= 0: 
            return False, np.nan, N_pct
        
        # è¨ˆç®—è®ŠåŒ–ç‡ï¼ˆæ­£å€¼è¡¨ç¤ºæ”¶ç·Šï¼Œè² å€¼è¡¨ç¤ºæ”¾å¯¬ï¼‰
        diff_ratio = (original_tol - new_tol) / original_tol * 100
        
        # é‚è¼¯ï¼šåªæœ‰ç•¶å®¹å·®æ”¶ç·Šï¼ˆnew_tol < original_tolï¼‰ä¸”è®ŠåŒ–ç‡ > N% æ™‚ï¼Œæ‰åˆ¤å®šç‚º TightenNeeded
        # å¦‚æœæ”¾å¯¬ï¼ˆnew_tol > original_tolï¼‰ï¼Œå‰‡ diff_ratio ç‚ºè² å€¼ï¼Œä¸æœƒè§¸ç™¼ tighten
        tighten_flag = diff_ratio > N_pct
        
        return tighten_flag, diff_ratio, N_pct

    # === æ ¸å¿ƒæµç¨‹åŒ…è£ ===
    
    def process_chart(self, df, value_col, date_col, oos_col, characteristic):
        """ä¸»é«” SOP æµç¨‹ (SOP 1-6)"""
        
        # 1. Data Integrity (SOP 1)
        values_orig, resolution = self.data_integrity(df.copy(), date_col, value_col, oos_col)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
        if len(values_orig) == 0:
            return {
                "Pattern": "No Valid Data",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": None,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "TightenNeeded": False,
                "DataCountUsed": 0,
                "HardRule": "None",
                "DetectionLimit": np.nan,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Initial_OOC_Count": 0,
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # æª¢æŸ¥æ•¸æ“šé»æ˜¯å¦ < 4 å€‹
        if len(values_orig) < 4:
            print(f"    [Warning] æ•¸æ“šé»ä¸è¶³ ({len(values_orig)} < 4)ï¼Œè·³éè¨ˆç®—")
            
            # è®€å–å¿…è¦åƒæ•¸
            detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
            original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
            original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
            
            # è¨ˆç®— Ori OOC Count
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            return {
                "Pattern": "Insufficient Data",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "TightenNeeded": False,
                "DataCountUsed": len(values_orig),
                "HardRule": "Insufficient Data",
                "DetectionLimit": detection_limit,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": 0,
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # 1.5. Hard Rule Check (å„ªå…ˆæª¢æŸ¥ï¼Œåœ¨ Pattern Diagnosis ä¹‹å‰)
        print(f"    [Debug] åŸå§‹æ•¸æ“šé»æ•¸é‡: {len(values_orig)}")
        print(f"    [Debug] åŸå§‹æ•¸æ“šç¯„åœ: {np.min(values_orig):.4f} ~ {np.max(values_orig):.4f}")
        
        UCL_hr, LCL_hr, rule_satisfied, rule_applied_name = self.apply_discrete_hard_rules(
            values_orig, resolution, len(values_orig))
        
        # è®€å–é¡å¤–åƒæ•¸
        detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
        original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
        original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
        target_val = df['Target'].iloc[0] if 'Target' in df.columns and len(df)>0 and pd.notna(df['Target'].iloc[0]) else np.nan

        if rule_satisfied:
            print(f"    [Debug] Hard Rule æ»¿è¶³: {rule_applied_name}")
            
            # æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šä½¿ç”¨ Hard Rule çš„å“ªäº›ç®¡åˆ¶ç·š
            if characteristic == 'Bigger':
                # Bigger: åªä½¿ç”¨ Hard Rule çš„ LCLï¼ŒUCL ä¿æŒåŸå§‹å€¼
                suggest_ucl_hr = original_ucl if not pd.isna(original_ucl) else UCL_hr
                suggest_lcl_hr = LCL_hr
                print(f"    [Debug] Bigger ç‰¹æ€§: åªèª¿æ•´ LCL = {LCL_hr:.4f}, UCL ä¿æŒåŸå§‹ = {suggest_ucl_hr:.4f}")
            elif characteristic == 'Smaller':
                # Smaller: åªä½¿ç”¨ Hard Rule çš„ UCLï¼ŒLCL ä¿æŒåŸå§‹å€¼
                suggest_ucl_hr = UCL_hr
                suggest_lcl_hr = original_lcl if not pd.isna(original_lcl) else LCL_hr
                print(f"    [Debug] Smaller ç‰¹æ€§: åªèª¿æ•´ UCL = {UCL_hr:.4f}, LCL ä¿æŒåŸå§‹ = {suggest_lcl_hr:.4f}")
            else:
                # Nominal: é›™é‚Šéƒ½ä½¿ç”¨ Hard Rule
                suggest_ucl_hr = UCL_hr
                suggest_lcl_hr = LCL_hr
                print(f"    [Debug] Nominal ç‰¹æ€§: é›™é‚Šéƒ½èª¿æ•´ UCL = {UCL_hr:.4f}, LCL = {LCL_hr:.4f}")
            
            # Hard Rule æˆç«‹æ™‚ï¼ŒCL_Center æ‡‰ç‚ºæœ€çµ‚ UCL å’Œ LCL çš„ä¸­é»
            cl_center_hr = (suggest_ucl_hr + suggest_lcl_hr) / 2
            
            # åˆ¤æ–·æ˜¯å¦éœ€è¦ tightenï¼ˆæª¢æŸ¥ Hard Rule èª¿æ•´çš„ç®¡åˆ¶ç·šæ˜¯å¦èˆ‡åŸå§‹å€¼ç›¸åŒï¼‰
            tighten_needed_hr = False  # é è¨­ç‚º False
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                # æ ¹æ“šç‰¹æ€§é¡å‹æª¢æŸ¥ç›¸æ‡‰çš„ç®¡åˆ¶ç·šæ˜¯å¦æ”¹è®Š
                if characteristic == 'Bigger':
                    # Bigger: æª¢æŸ¥ LCL æ˜¯å¦æ”¹è®Š
                    if abs(suggest_lcl_hr - original_lcl) > 1e-9:
                        tighten_needed_hr = True
                        print(f"    [Debug] Hard Rule (Bigger): LCL æ”¹è®Š (åŸå§‹={original_lcl:.4f}, æ–°={suggest_lcl_hr:.4f})ï¼ŒTightenNeeded = Yes")
                    else:
                        print(f"    [Debug] Hard Rule (Bigger): LCL æœªæ”¹è®Šï¼ŒTightenNeeded = No")
                elif characteristic == 'Smaller':
                    # Smaller: æª¢æŸ¥ UCL æ˜¯å¦æ”¹è®Š
                    if abs(suggest_ucl_hr - original_ucl) > 1e-9:
                        tighten_needed_hr = True
                        print(f"    [Debug] Hard Rule (Smaller): UCL æ”¹è®Š (åŸå§‹={original_ucl:.4f}, æ–°={suggest_ucl_hr:.4f})ï¼ŒTightenNeeded = Yes")
                    else:
                        print(f"    [Debug] Hard Rule (Smaller): UCL æœªæ”¹è®Šï¼ŒTightenNeeded = No")
                else:
                    # Nominal: æª¢æŸ¥ UCL æˆ– LCL æ˜¯å¦æ”¹è®Š
                    if abs(suggest_ucl_hr - original_ucl) > 1e-9 or abs(suggest_lcl_hr - original_lcl) > 1e-9:
                        tighten_needed_hr = True
                        print(f"    [Debug] Hard Rule (Nominal): ç®¡åˆ¶ç·šæ”¹è®Š (UCL: {original_ucl:.4f}â†’{suggest_ucl_hr:.4f}, LCL: {original_lcl:.4f}â†’{suggest_lcl_hr:.4f})ï¼ŒTightenNeeded = Yes")
                    else:
                        print(f"    [Debug] Hard Rule (Nominal): ç®¡åˆ¶ç·šæœªæ”¹è®Šï¼ŒTightenNeeded = No")
            else:
                # æ²’æœ‰åŸå§‹ç®¡åˆ¶ç·šæ™‚ï¼ŒHard Rule è§¸ç™¼å³ç‚ºéœ€è¦ tighten
                tighten_needed_hr = True
                print(f"    [Debug] Hard Rule: ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼ŒTightenNeeded = Yes")
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            # è¨ˆç®— Static/Final OOC Count (ä½¿ç”¨ Hard Rule çš„ç®¡åˆ¶ç·š)
            static_ooc_count = 0
            static_upper_ooc = np.sum(values_orig > suggest_ucl_hr)
            static_lower_ooc = np.sum(values_orig < suggest_lcl_hr)
            static_ooc_count = static_upper_ooc + static_lower_ooc
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            # è¨ˆç®— Static/Final OOC Count (ä½¿ç”¨ Hard Rule çš„ç®¡åˆ¶ç·š)
            static_ooc_count = 0
            static_upper_ooc = np.sum(values_orig > suggest_ucl_hr)
            static_lower_ooc = np.sum(values_orig < suggest_lcl_hr)
            static_ooc_count = static_upper_ooc + static_lower_ooc
            
            # Hard Rule æ»¿è¶³æ™‚ï¼Œè¿”å›çµæœ
            return {
                "Pattern": rule_applied_name,  # ç›´æ¥é¡¯ç¤ºå…·é«”çš„ Hard Rule
                "Skew": np.nan,  # Hard Rule ä¸éœ€è¦ Skew/CB
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": suggest_ucl_hr,
                "Suggest LCL": suggest_lcl_hr,
                "Static UCL": suggest_ucl_hr,
                "Static LCL": suggest_lcl_hr, 
                "TightenNeeded": tighten_needed_hr,  # æ ¹æ“šç®¡åˆ¶ç·šæ˜¯å¦æ”¹è®Šåˆ¤å®š
                "DataCountUsed": len(values_orig),
                "HardRule": rule_applied_name,
                "DetectionLimit": detection_limit,
                "CL_Center": cl_center_hr,
                "Sigma_Est": 0.0, 
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,  # Hard Rule ç„¡sigmaæ¦‚å¿µ
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": static_ooc_count,
                "Final_OOC_Count": static_ooc_count,  # Hard Rule æ²’æœ‰é€€æ ¼ï¼ŒStatic = Final
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        print(f"    [Debug] Hard Rule ä¸æ»¿è¶³ï¼Œç¹¼çºŒé€²è¡Œ Pattern Diagnosis")
        
        # 2-4. Pattern Diagnosis & Outlier Filter
        print(f"    [Debug] åŸå§‹æ•¸æ“šé»æ•¸é‡: {len(values_orig)}")
        print(f"    [Debug] åŸå§‹æ•¸æ“šç¯„åœ: {np.min(values_orig):.4f} ~ {np.max(values_orig):.4f}")
        
        values_for_pattern = self.data_prep_for_pattern(values_orig)
        print(f"    [Debug] é è™•ç†å¾Œæ•¸æ“šé»æ•¸é‡: {len(values_for_pattern)}")
        
        pattern, skew_value, cb_value = self.pattern_diagnosis(values_for_pattern, resolution)
        print(f"    [Debug] è¨ºæ–·å‡ºçš„æ¨¡å¼: {pattern}, Skew: {skew_value:.4f}, CB: {cb_value:.4f}")
        
        values_filtered = self.outlier_filter(values_orig, pattern)
        N_filtered = len(values_filtered)
        print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šé»æ•¸é‡: {N_filtered}")
        if N_filtered > 0:
            print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šç¯„åœ: {np.min(values_filtered):.4f} ~ {np.max(values_filtered):.4f}")
        else:
            print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šç‚ºç©º!")
        
        # æª¢æŸ¥éæ¿¾å¾Œçš„æ•¸æ“šæ˜¯å¦ç‚ºç©º
        if N_filtered == 0:
            print(f"    [Warning] éæ¿¾å¾Œæ²’æœ‰æœ‰æ•ˆæ•¸æ“šé»ï¼Œè·³éè¨ˆç®—")
            
            # è®€å–å¿…è¦åƒæ•¸
            detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
            original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
            original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹æ•¸æ“šï¼Œå³ä½¿éæ¿¾å¾Œç‚ºç©º)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            return {
                "Pattern": "No Data After Filter",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "TightenNeeded": False,
                "DataCountUsed": N_filtered,
                "HardRule": "No Data After Filter",
                "DetectionLimit": detection_limit,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": 0,  # éæ¿¾å¾Œç„¡æ•¸æ“šï¼Œç„¡æ³•è¨ˆç®— Static
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # è®€å–é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å¾ŒçºŒè¨ˆç®—ï¼‰
        detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
        original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
        original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
        target_val = df['Target'].iloc[0] if 'Target' in df.columns and len(df)>0 and pd.notna(df['Target'].iloc[0]) else np.nan

        # 5. Statistical Model Fitting (SOP 5)
        UCL_static, LCL_static, UR_robust, LR_robust, UCL3_ecdf, LCL3_ecdf = self.calc_CL(
            values_filtered, pattern, resolution, characteristic
        )
        
        # æ±ºå®š CL Center å’Œ Sigma Est
        cl_center = np.nan
        sigma_est_u = np.nan
        sigma_est_l = np.nan
        
        if pattern in ["Constant", "Near Constant"]:
            cl_center = np.median(values_filtered)
            sigma_est_u = sigma_est_l = 0.0
        elif pattern == "Normal":
            cl_center = np.mean(values_filtered)
            sigma_est_u = sigma_est_l = np.std(values_filtered)
        elif pattern in ["Skew-Right", "Skew-Left"]:
            cl_center = np.median(values_filtered)
            sigma_est_u = UR_robust
            sigma_est_l = LR_robust
        else: # ECDF ç›¸é—œæ¨¡å¼ (Bimodal, Attribute)
            cl_center = np.median(values_filtered)
            T_upper_ecdf = UCL3_ecdf - cl_center
            T_lower_ecdf = cl_center - LCL3_ecdf
            if T_upper_ecdf > 1e-9 and T_lower_ecdf > 1e-9:
                sigma_est_u = T_upper_ecdf / 3.0
                sigma_est_l = T_lower_ecdf / 3.0
            else:
                sigma_est_u = UR_robust if UR_robust > 0 else 0.0
                sigma_est_l = LR_robust if LR_robust > 0 else 0.0

        sigma_est_output = max(sigma_est_u, sigma_est_l)
        if not np.isfinite(sigma_est_output): 
            sigma_est_output = 0.0 

        # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
        ori_ooc_count = 0
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            ori_upper_ooc = np.sum(values_orig > original_ucl)
            ori_lower_ooc = np.sum(values_orig < original_lcl)
            ori_ooc_count = ori_upper_ooc + ori_lower_ooc

        # 6. Control Limit Adjustment (SOP 6.1/6.2/6.3/6.5)
        # ä½¿ç”¨åŸå§‹æ•¸æ“š (values_orig) ä¾†è¨ˆç®— OOC countï¼Œè€Œä¸æ˜¯éæ¿¾å¾Œçš„æ•¸æ“š
        UCL_suggest, LCL_suggest, static_ooc_count, final_ooc_count, total_adj_units = self.adjust_CL_based_on_OOC(
            values_orig, UCL_static, LCL_static, pattern, resolution, sigma_est_u, sigma_est_l, 2, characteristic
        )

        # 6.5. Detection Limit Rule (åƒ…é©ç”¨æ–¼ Smaller ç‰¹æ€§)
        if characteristic == 'Smaller' and not np.isnan(detection_limit):
            if UCL_suggest < detection_limit:
                UCL_suggest = detection_limit

        # 6.6. ç®¡åˆ¶ç·šå¯¬åº¦ç´„æŸ - ä¸å…è¨±å»ºè­°ç®¡åˆ¶ç·šæ¯”åŸå§‹ç®¡åˆ¶ç·šæ›´å¯¬
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            # Static UCL/LCL ç´„æŸï¼šä¸è¶…å‡ºåŸå§‹ç®¡åˆ¶ç·š
            if not np.isnan(UCL_static) and UCL_static > original_ucl:
                UCL_static = original_ucl
            if not np.isnan(LCL_static) and LCL_static < original_lcl:
                LCL_static = original_lcl
                
            # Suggest UCL/LCL ç´„æŸï¼šä¸è¶…å‡ºåŸå§‹ç®¡åˆ¶ç·š  
            if not np.isnan(UCL_suggest) and UCL_suggest > original_ucl:
                UCL_suggest = original_ucl
            if not np.isnan(LCL_suggest) and LCL_suggest < original_lcl:
                LCL_suggest = original_lcl
        
        # 6.6b. æ ¹æ“šç‰¹æ€§é¡å‹è¨­å®š Static UCL/LCL ç‚ºåŸå§‹å€¼
        if characteristic == 'Smaller':
            # Smaller ç‰¹æ€§ï¼šStatic LCL = åŸå§‹ LCL (åªéœ€è¦ USL)
            if not pd.isna(original_lcl):
                LCL_static = original_lcl
        elif characteristic == 'Bigger':
            # Bigger ç‰¹æ€§ï¼šStatic UCL = åŸå§‹ UCL (åªéœ€è¦ LSL)
            if not pd.isna(original_ucl):
                UCL_static = original_ucl
        
        # 6.7a. Attribute/Near Constant ç‰¹æ®Šé‚è¼¯ï¼šé¿å… UCL/LCL å¡åœ¨ max/min
        if pattern in ["Attribute", "Near Constant"] and resolution is not None and resolution > 0 and len(values_filtered) > 0:
            print(f"    [Debug] é€²å…¥ Attribute/Near Constant ç‰¹æ®Šé‚è¼¯")
            print(f"    [Debug] values_filtered é•·åº¦: {len(values_filtered)}")
            
            max_val = np.max(values_filtered)
            min_val = np.min(values_filtered)
            
            # æš«å­˜èª¿æ•´å‰çš„å€¼
            temp_UCL_suggest = UCL_suggest
            temp_LCL_suggest = LCL_suggest
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦èª¿æ•´ï¼ˆå¡åœ¨ max/minï¼‰
            need_adjust_ucl = not np.isnan(UCL_suggest) and abs(UCL_suggest - max_val) < 1e-6
            need_adjust_lcl = not np.isnan(LCL_suggest) and abs(LCL_suggest - min_val) < 1e-6
            
            if need_adjust_ucl:
                temp_UCL_suggest = UCL_suggest + resolution
                print(f"    [Debug] UCL å¡åœ¨ max ({max_val:.4f})ï¼Œå˜—è©¦èª¿æ•´ç‚º {temp_UCL_suggest:.4f}")
            
            if need_adjust_lcl:
                temp_LCL_suggest = LCL_suggest - resolution
                print(f"    [Debug] LCL å¡åœ¨ min ({min_val:.4f})ï¼Œå˜—è©¦èª¿æ•´ç‚º {temp_LCL_suggest:.4f}")
            
            # æª¢æŸ¥èª¿æ•´å¾Œæ˜¯å¦æ¯”åŸå§‹ç®¡åˆ¶ç·šæ›´å¯¬é¬†
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                # è¨ˆç®—åŸå§‹å®¹å·®
                if characteristic == 'Nominal':
                    ori_tolerance = original_ucl - original_lcl
                    new_tolerance = temp_UCL_suggest - temp_LCL_suggest
                elif characteristic == 'Smaller':
                    ori_tolerance = original_ucl - cl_center
                    new_tolerance = temp_UCL_suggest - cl_center
                elif characteristic == 'Bigger':
                    ori_tolerance = cl_center - original_lcl
                    new_tolerance = cl_center - temp_LCL_suggest
                else:
                    ori_tolerance = original_ucl - original_lcl
                    new_tolerance = temp_UCL_suggest - temp_LCL_suggest
                
                # å¦‚æœèª¿æ•´å¾Œæ›´å¯¬é¬†ï¼Œä¿æŒåŸå§‹å€¼
                if new_tolerance > ori_tolerance:
                    print(f"    [Debug] èª¿æ•´å¾Œå®¹å·® ({new_tolerance:.4f}) æ¯”åŸå§‹å®¹å·® ({ori_tolerance:.4f}) æ›´å¯¬ï¼Œä¿æŒåŸå§‹ç®¡åˆ¶ç·š")
                    # ä¸æ›´æ–° UCL_suggest å’Œ LCL_suggest
                else:
                    print(f"    [Debug] èª¿æ•´å¾Œå®¹å·® ({new_tolerance:.4f}) æœªæ¯”åŸå§‹å®¹å·® ({ori_tolerance:.4f}) æ›´å¯¬ï¼Œæ¡ç”¨èª¿æ•´å€¼")
                    UCL_suggest = temp_UCL_suggest
                    LCL_suggest = temp_LCL_suggest
            else:
                # æ²’æœ‰åŸå§‹ç®¡åˆ¶ç·šæ™‚ï¼Œç›´æ¥æ¡ç”¨èª¿æ•´å€¼
                print(f"    [Debug] ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼Œç›´æ¥æ¡ç”¨èª¿æ•´å€¼")
                UCL_suggest = temp_UCL_suggest
                LCL_suggest = temp_LCL_suggest
            
        if characteristic == 'Smaller':
            # Smaller åªå…è¨± UCL æ”¶ç·Šï¼ŒLCL ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ (åªéœ€è¦ USL)
            if not pd.isna(original_lcl):
                LCL_suggest = original_lcl
        elif characteristic == 'Bigger':
            # Bigger åªå…è¨± LCL æ”¶ç·Šï¼ŒUCL ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ (åªéœ€è¦ LSL)
            if not pd.isna(original_ucl):
                UCL_suggest = original_ucl

        # 7. Tighten åˆ¤å®š (SOP 6.4) 
        tighten_flag = False
        diff_ratio = np.nan
        tighten_threshold = np.nan
        original_tol = np.nan
        new_tol = np.nan
        
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            center_val = cl_center 
            
            if characteristic == 'Nominal':
                # Nominal: tolerance = UCL â€“ LCL
                original_tol = original_ucl - original_lcl
                new_tol = UCL_suggest - LCL_suggest
            elif characteristic == 'Smaller':
                # Smaller: tolerance = UCL â€“ CL_Center
                original_tol = original_ucl - center_val
                new_tol = UCL_suggest - center_val
                
            elif characteristic == 'Bigger':
                # Bigger: tolerance = CL_Center â€“ LCL
                original_tol = center_val - original_lcl
                new_tol = center_val - LCL_suggest
            else:
                # é è¨­ç‚º Nominal
                original_tol = original_ucl - original_lcl
                new_tol = UCL_suggest - LCL_suggest

            # é€²è¡Œ Tighten æª¢æŸ¥ï¼Œä¸¦å–å¾—è©³ç´°è³‡è¨Š
            if original_tol > 1e-9 and new_tol > 1e-9:
                tighten_flag, diff_ratio, tighten_threshold = self.check_tighten_with_details(
                    original_tol, new_tol, N_filtered
                )
        
        # 8. è¨ˆç®—ç¾è¡Œå’Œå»ºè­°ç®¡åˆ¶ç·šçš„Kå€æ•¸
        # ç¾è¡Œç®¡åˆ¶ç·šçš„Kå€æ•¸
        original_ucl_k_set = np.nan
        original_lcl_k_set = np.nan
        suggest_ucl_k_set = np.nan
        suggest_lcl_k_set = np.nan
        
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)) and not pd.isna(cl_center):
            if sigma_est_u > 1e-9:
                original_ucl_k_set = (original_ucl - cl_center) / sigma_est_u
                suggest_ucl_k_set = (UCL_suggest - cl_center) / sigma_est_u
            if sigma_est_l > 1e-9:
                original_lcl_k_set = (cl_center - original_lcl) / sigma_est_l
                suggest_lcl_k_set = (cl_center - LCL_suggest) / sigma_est_l

        # è¨ˆç®—Ori_k_setå’ŒSug_k_setï¼ˆæ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šè¨ˆç®—æ–¹å¼ï¼‰
        ori_k_set = np.nan
        sug_k_set = np.nan
        
        if characteristic == 'Bigger':
            # Bigger ç‰¹æ€§ï¼šåªè€ƒæ…® LCL çš„ K å€¼
            ori_k_set = original_lcl_k_set
            sug_k_set = suggest_lcl_k_set
        elif characteristic == 'Smaller':
            # Smaller ç‰¹æ€§ï¼šåªè€ƒæ…® UCL çš„ K å€¼
            ori_k_set = original_ucl_k_set
            sug_k_set = suggest_ucl_k_set
        else:
            # Nominal ç‰¹æ€§ï¼ˆé è¨­ï¼‰ï¼šå– UCL å’Œ LCL çš„ K å€¼æœ€å¤§å€¼
            if not pd.isna(original_ucl_k_set) and not pd.isna(original_lcl_k_set):
                ori_k_set = max(original_ucl_k_set, original_lcl_k_set)
            elif not pd.isna(original_ucl_k_set):
                ori_k_set = original_ucl_k_set
            elif not pd.isna(original_lcl_k_set):
                ori_k_set = original_lcl_k_set
                
            if not pd.isna(suggest_ucl_k_set) and not pd.isna(suggest_lcl_k_set):
                sug_k_set = max(suggest_ucl_k_set, suggest_lcl_k_set)
            elif not pd.isna(suggest_ucl_k_set):
                sug_k_set = suggest_ucl_k_set
            elif not pd.isna(suggest_lcl_k_set):
                sug_k_set = suggest_lcl_k_set

        # 9. è¼¸å‡ºçµæœ
        return {
            "Pattern": pattern,
            "Skew": skew_value,
            "CB": cb_value,
            "Resolution_Estimated": resolution,
            "Suggest UCL": UCL_suggest,
            "Suggest LCL": LCL_suggest,
            "Static UCL": UCL_static,
            "Static LCL": LCL_static,
            "TightenNeeded": tighten_flag,
            "DataCountUsed": N_filtered,
            "HardRule": "None",
            "DetectionLimit": detection_limit,
            "CL_Center": cl_center,
            "Sigma_Est": sigma_est_output,
            "Sigma_Est_Upper": sigma_est_u,
            "Sigma_Est_Lower": sigma_est_l,
            "Original_UCL_K_Set": original_ucl_k_set,
            "Original_LCL_K_Set": original_lcl_k_set,
            "Suggest_UCL_K_Set": suggest_ucl_k_set,
            "Suggest_LCL_K_Set": suggest_lcl_k_set,
            "Ori_K_Set": ori_k_set,
            "Sug_K_Set": sug_k_set,
            "Total_Adj_Units": total_adj_units,
            "Ori_OOC_Count": ori_ooc_count,
            "Static_OOC_Count": static_ooc_count,
            "Final_OOC_Count": final_ooc_count,
            "Original_Tolerance": original_tol,
            "New_Tolerance": new_tol,
            "Diff_Ratio_%": diff_ratio,
            "Tighten_Threshold_%": tighten_threshold
        }

    # === æª”æ¡ˆ I/O èˆ‡æµç¨‹æ§åˆ¶ ===

    def load_chart_information(self, filepath):
        """è®€å– Chart è¨­å®šæª” (Excel)"""
        try:
            df_charts = pd.read_excel(filepath, sheet_name='Chart')
            
            # ç¢ºä¿ Resolution, DetectionLimit å­˜åœ¨
            if 'Resolution' not in df_charts.columns: 
                df_charts['Resolution'] = np.nan
            if 'DetectionLimit' not in df_charts.columns: 
                df_charts['DetectionLimit'] = np.nan
                
            # Target, UCL, LCL ç‚ºå¿…é ˆæ¬„ä½
            required_columns = ['Target', 'UCL', 'LCL']
            for col in required_columns:
                if col not in df_charts.columns:
                    raise ValueError(f"ç¼ºå°‘å¿…é ˆæ¬„ä½: '{col}'")
                    
            # æª¢æŸ¥å¿…é ˆæ¬„ä½æ˜¯å¦æœ‰æ•¸å€¼
            for col in required_columns:
                if df_charts[col].isna().any():
                    missing_rows = df_charts[df_charts[col].isna()].index.tolist()
                    raise ValueError(f"å¿…é ˆæ¬„ä½ '{col}' åœ¨ç¬¬ {missing_rows} è¡Œæœ‰ç¼ºå¤±å€¼")
                
            return df_charts
        except Exception as e:
            print(f"Error loading chart information from {filepath}: {e}")
            return pd.DataFrame()

    def find_matching_file(self, raw_data_directory, group_name, chart_name):
        """æ ¹æ“šå‘½åè¦å‰‡åŒ¹é…å°æ‡‰çš„ Raw Data CSV"""
        pattern_base = f"{group_name}_{chart_name}"
        
        for filename in os.listdir(raw_data_directory):
            if filename.endswith('.csv') and filename.startswith(pattern_base):
                return os.path.join(raw_data_directory, filename)
        return None

    def plot_control_chart(self, chart_data, chart_info, suggest_ucl, suggest_lcl,
                        static_ucl, static_lcl, cl_center, pattern, output_dir='output_charts', 
                        max_x_labels=10):
        """
        ç¹ªè£½ç®¡åˆ¶åœ– (SPC Chart)
        - å›ºå®šè¼¸å‡º 800x450 åƒç´ ï¼Œé«˜ DPI ä¿æŒæ¸…æ™°
        - åŒ…å«æ‰€æœ‰ç®¡åˆ¶ç·šã€å»ºè­°ç·šã€éœæ…‹ç·šèˆ‡æ¨™è¨»
        """
        import os
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates

        os.makedirs(output_dir, exist_ok=True)

        # ğŸ¯ å›ºå®šæœ€çµ‚è¼¸å‡ºå¤§å°ï¼š800x450 åƒç´ ï¼ˆä¸è®Šå¤§ï¼‰
        target_width_px, target_height_px = 900, 430
        dpi = 100  # é«˜ DPI ä¿æŒç·šæ¢å¹³æ»‘
        fig = plt.figure(figsize=(target_width_px / dpi, target_height_px / dpi), dpi=dpi)
        ax = fig.add_subplot(111)

        # å…¨åŸŸæŠ—é‹¸é½’è¨­å®š
        plt.rcParams['lines.antialiased'] = True
        plt.rcParams['patch.antialiased'] = True
        fig.patch.set_antialiased(True)

        # ========== ç¹ªåœ–ä¸»é«” ==========
        # X è»¸è³‡æ–™ï¼šä½¿ç”¨ç­‰è·ä½ç½®ï¼Œä½†é¡¯ç¤ºå¯¦éš›æ™‚é–“æ¨™ç±¤
        x = range(len(chart_data))  # ç­‰è·ä½ç½®
        
        if 'date' in chart_data.columns:
            # è½‰æ›æ—¥æœŸä¸¦ä½¿ç”¨çµ±ä¸€æ ¼å¼ yyyy/m/d hh:mm
            dates = pd.to_datetime(chart_data['date'])
            date_format = '%Y/%m/%d %H:%M'

            # è¨­å®š X è»¸æ¨™ç±¤ç‚ºå¯¦éš›æ™‚é–“ï¼Œä½†ä½ç½®æ˜¯ç­‰è·çš„
            date_labels = [d.strftime(date_format) for d in dates]
            
            # è‡ªå‹•è¨ˆç®—æœ€ä½³æ¨™ç±¤æ•¸é‡ï¼ˆå‚ç›´é¡¯ç¤ºï¼Œä¸»è¦è€ƒæ…®è¦–è¦ºå¯†åº¦ï¼‰
            n_points = len(chart_data)
            
            # å‚ç›´é¡¯ç¤ºå…è¨±æ›´å¤šæ¨™ç±¤ï¼Œç›¡é‡å¤šé¡¯ç¤ºæ™‚é–“è³‡è¨Š
            if n_points <= 15:
                optimal_labels = n_points  # å°‘æ–¼ 15 å€‹é»ï¼Œå…¨éƒ¨é¡¯ç¤º
            elif n_points <= 50:
                optimal_labels = min(n_points, int(n_points / 1))  # é¡¯ç¤ºç´„ 2/3 çš„é»
            else:
                optimal_labels = min(n_points, 30)  # æœ€å¤šé¡¯ç¤º 30 å€‹æ¨™ç±¤
            
            # ç¢ºä¿è‡³å°‘é¡¯ç¤º 2 å€‹æ¨™ç±¤ï¼ˆé¦–å°¾ï¼‰
            optimal_labels = max(2, optimal_labels)
            
            # ç¸½æ˜¯ç¢ºä¿åˆ»åº¦ä½ç½®åœ¨è¦–è¦ºä¸Šå‡å‹»åˆ†å¸ƒ
            if n_points <= optimal_labels:
                # é»æ•¸å°‘æ–¼æœ€ä½³æ¨™ç±¤æ•¸ï¼Œå…¨éƒ¨é¡¯ç¤º
                tick_positions = list(x)  # x æœ¬èº«å°±æ˜¯ range(n_points)ï¼Œå·²ç¶“ç­‰è·
                tick_labels = date_labels
            else:
                # åœ¨ç­‰è·çš„ X è»¸ä½ç½®ä¸Šå‡å‹»é¸æ“‡åˆ»åº¦
                # x æ˜¯ range(n_points)ï¼Œå³ [0, 1, 2, ..., n_points-1]
                # æˆ‘å€‘è¦åœ¨é€™å€‹ç­‰è·åºåˆ—ä¸Šå‡å‹»é¸æ“‡ optimal_labels å€‹ä½ç½®
                
                # è¨ˆç®—å‡å‹»é–“éš”
                x_min = 0
                x_max = n_points - 1
                uniform_positions = np.linspace(x_min, x_max, optimal_labels)
                
                # å°‡æµ®é»ä½ç½®å››æ¨äº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•¸ä½ç½®
                tick_indices = [int(round(pos)) for pos in uniform_positions]
                
                # å»é™¤é‡è¤‡ä¸¦ç¢ºä¿åœ¨æœ‰æ•ˆç¯„åœå…§
                tick_indices = sorted(list(set(tick_indices)))
                tick_indices = [idx for idx in tick_indices if 0 <= idx < n_points]
                
                # ç”Ÿæˆå°æ‡‰çš„ä½ç½®å’Œæ¨™ç±¤
                tick_positions = tick_indices  # ç›´æ¥ä½¿ç”¨æ•´æ•¸ä½ç½®ï¼Œé€™äº›å·²ç¶“æ˜¯ç­‰è·çš„
                tick_labels = [date_labels[i] for i in tick_indices]
            
            ax.set_xticks(tick_positions)
            ax.set_xticklabels(tick_labels, rotation=90, ha='center', fontsize=9)  # rotation=90 å‚ç›´é¡¯ç¤º

        y = chart_data['value'].values
        
        # æ‰€æœ‰æ¨¡å¼çµ±ä¸€ç¹ªè£½ï¼šè—è‰²é€£ç·š
        ax.plot(x, y, 'bo-', markersize=3, linewidth=1, alpha=0.8, antialiased=True)

        # åˆ¤æ–·æ˜¯å¦åªé¡¯ç¤º Sug (ç•¶ Ori èˆ‡ Sug å·®è·å¤ªå¤§æ™‚)
        show_only_sug = False
        if not np.isnan(suggest_ucl) and not np.isnan(suggest_lcl):
            ori_range = abs(chart_info['UCL'] - chart_info['LCL'])
            sug_range = abs(suggest_ucl - suggest_lcl)
            # å¦‚æœ Ori ç¯„åœæ˜¯ Sug ç¯„åœçš„ 3 å€ä»¥ä¸Šï¼Œåªé¡¯ç¤º Sug
            if ori_range > sug_range * 3:
                show_only_sug = True

        # å„ç¨®ç®¡åˆ¶ç·š
        # Hard Rule æ™‚ä¸é¡¯ç¤º Target ç·šï¼Œé¿å…è¦–è¦ºæ··äº‚
        if "Hard Rule" not in pattern:
            ax.axhline(y=chart_info['Target'], color='gray', linestyle='-', linewidth=1)
        
        # åªåœ¨å·®è·ä¸å¤§æ™‚é¡¯ç¤º Ori UCL/LCL
        if not show_only_sug:
            ax.axhline(y=chart_info['UCL'], color='red', linestyle='--', linewidth=2)
            ax.axhline(y=chart_info['LCL'], color='red', linestyle='--', linewidth=2)

        if not np.isnan(suggest_ucl):
            ax.axhline(y=suggest_ucl, color='#555555', linestyle='-', linewidth=1.5)
        if not np.isnan(suggest_lcl):
            ax.axhline(y=suggest_lcl, color='#555555', linestyle='-', linewidth=1.5)

        # Static ç·šå·²ç§»é™¤ï¼Œä¸å†ç¹ªè£½

        # ======= æ¨™é¡Œ =======
        # ä½¿ç”¨è¨ˆç®—çµæœçš„ patternï¼Œè€Œé Excel çš„ ExpectedPattern
        sug_ucl_text = f"Sug_UCL: {suggest_ucl:.3f}" if not np.isnan(suggest_ucl) else "Sug_UCL: N/A"
        sug_lcl_text = f"Sug_LCL: {suggest_lcl:.3f}" if not np.isnan(suggest_lcl) else "Sug_LCL: N/A"
        title = (f"{chart_info['GroupName']}@{chart_info['ChartName']}@{chart_info['Characteristics']}\n"
                f"Pattern: {pattern} | Data Cnt: {len(chart_data)} | {sug_ucl_text} | {sug_lcl_text}")
        ax.set_title(title, fontsize=11)
        ax.grid(False)

        # ======= èª¿æ•´ Y è»¸ç¯„åœï¼ˆç‰¹æ®Šè™•ç† Constant/Near Constant/Hard Ruleï¼‰=======
        if pattern in ["Constant", "Near Constant"] or "Hard Rule" in pattern:
            # Constant/Near Constant/Hard Rule æ¨¡å¼ï¼šåŒ…å«æ‰€æœ‰é»ï¼Œä½†åˆç†è¨­å®š Y è»¸ç¯„åœ
            y_data_min = y.min()
            y_data_max = y.max()
            
            # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœç¯„åœå¤ªå°ï¼ˆæ‰€æœ‰é»éƒ½å¾ˆæ¥è¿‘ï¼‰ï¼Œé©ç•¶æ“´å±• Y è»¸
            data_range = y_data_max - y_data_min
            if data_range < 1e-6:  # å¹¾ä¹æ˜¯å¸¸æ•¸
                center = (y_data_min + y_data_max) / 2
                margin = max(abs(center) * 0.1, 1.0)  # è‡³å°‘ 1 å€‹å–®ä½çš„ç¯„åœ
                y_data_min = center - margin
                y_data_max = center + margin
            print(f"[{pattern}] åŒ…å«æ‰€æœ‰é»ï¼ŒY è»¸ç¯„åœ: [{y_data_min:.3f}, {y_data_max:.3f}]")
        else:
            # ä¸€èˆ¬æ¨¡å¼ï¼šä½¿ç”¨å››åˆ†ä½æ•¸æ³•æ’é™¤æ¥µç«¯å€¼
            q1 = np.percentile(y, 25)
            q3 = np.percentile(y, 75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            # éæ¿¾æ‰ç•°å¸¸å€¼å¾Œçš„æ•¸æ“šç¯„åœ
            y_filtered = y[(y >= lower_bound) & (y <= upper_bound)]
            if len(y_filtered) > 0:
                y_data_min = y_filtered.min()
                y_data_max = y_filtered.max()
            else:
                # å¦‚æœå…¨éƒ¨éƒ½æ˜¯ç•°å¸¸å€¼ï¼Œé‚„æ˜¯ä½¿ç”¨åŸå§‹ç¯„åœ
                y_data_min = y.min()
                y_data_max = y.max()
        
        # åˆå§‹åŒ– all_linesï¼ŒHard Rule æ™‚ä¸åŒ…å« Target
        all_lines = [y_data_min, y_data_max]
        if "Hard Rule" not in pattern:
            all_lines.append(chart_info['Target'])
        
        # æ ¹æ“šæ˜¯å¦åªé¡¯ç¤º Sug ä¾†æ±ºå®šåŒ…å«å“ªäº›ç·š
        if show_only_sug:
            # åªé¡¯ç¤º Sug æ™‚ï¼Œä¸åŒ…å« Ori UCL/LCL
            for v in [suggest_ucl, suggest_lcl]:
                if not np.isnan(v):
                    all_lines.append(v)
        else:
            # æ­£å¸¸æƒ…æ³ï¼ŒåŒ…å«æ‰€æœ‰ç·š
            all_lines.extend([chart_info['UCL'], chart_info['LCL']])
            for v in [suggest_ucl, suggest_lcl]:
                if not np.isnan(v):
                    all_lines.append(v)
        
        # è¨ˆç®— Y è»¸ç¯„åœï¼Œè™•ç† Hard Rule æƒ…æ³ä¸‹æ‰€æœ‰ç·šé‡ç–Šçš„å•é¡Œ
        if max(all_lines) == min(all_lines):
            # æ‰€æœ‰ç·šé‡ç–Šæ™‚ï¼Œä½¿ç”¨æ•¸æ“šç¯„åœè¨­å®š Y è»¸
            center_line = max(all_lines)
            data_range = y_data_max - y_data_min
            if data_range == 0:
                # æ•¸æ“šä¹Ÿæ˜¯å¸¸æ•¸æ™‚ï¼Œä½¿ç”¨å›ºå®šç¯„åœ
                y_margin = abs(center_line) * 0.1 if center_line != 0 else 1.0
            else:
                y_margin = data_range * 0.2
            ax.set_ylim(center_line - y_margin, center_line + y_margin)
        else:
            y_margin = (max(all_lines) - min(all_lines)) * 0.1
            ax.set_ylim(min(all_lines) - y_margin, max(all_lines) + y_margin)

        plt.tight_layout(rect=[0, 0, 0.85, 1])

        # ======= å¤–å´æ¨™è¨» =======
        annotations = []
        
        # Hard Rule æ™‚ä¸é¡¯ç¤º Target æ¨™è¨»
        if "Hard Rule" not in pattern:
            annotations.append((chart_info['Target'], f"Target = {chart_info['Target']:.3f}", 'gray', 'normal'))

        # æª¢æŸ¥é‡ç–Šç·šä¸¦åˆä½µæ¨™è¨»
        tolerance = 1e-6  # åˆ¤æ–·ç·šé‡ç–Šçš„å®¹å·®å€¼
        
        # æ”¶é›†æ‰€æœ‰UCLç·šçš„è³‡è¨Š
        ucl_lines = []
        if not show_only_sug and not np.isnan(chart_info['UCL']):
            ucl_lines.append(('Ori', chart_info['UCL'], 'red', 'normal'))
        if not np.isnan(suggest_ucl):
            ucl_lines.append(('Sug', suggest_ucl, 'black', 'bold'))
        
        # æ”¶é›†æ‰€æœ‰LCLç·šçš„è³‡è¨Š
        lcl_lines = []
        if not show_only_sug and not np.isnan(chart_info['LCL']):
            lcl_lines.append(('Ori', chart_info['LCL'], 'red', 'normal'))
        if not np.isnan(suggest_lcl):
            lcl_lines.append(('Sug', suggest_lcl, 'black', 'bold'))
        
        # è™•ç†UCLé‡ç–Š
        ucl_groups = []
        for line in ucl_lines:
            placed = False
            for group in ucl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                ucl_groups.append([line])
        
        # è™•ç†LCLé‡ç–Š
        lcl_groups = []
        for line in lcl_lines:
            placed = False
            for group in lcl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                lcl_groups.append([line])
        
        # æª¢æŸ¥ UCL å’Œ LCL ä¹‹é–“çš„é‡ç–Šï¼ˆè™•ç†å¸¸æ•¸/å¡å®šå€¼æƒ…æ³ï¼‰
        all_cl_groups = []
        
        # å°‡æ‰€æœ‰ UCL å’Œ LCL ç·šåˆä½µåˆ°ä¸€èµ·æª¢æŸ¥
        all_lines = []
        for name, value, color, weight in ucl_lines:
            all_lines.append((name, value, color, weight, 'UCL'))
        for name, value, color, weight in lcl_lines:
            all_lines.append((name, value, color, weight, 'LCL'))
        
        # é‡æ–°åˆ†çµ„ï¼ŒåŒ…æ‹¬è·¨ UCL/LCL çš„é‡ç–Š
        for line in all_lines:
            placed = False
            for group in all_cl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                all_cl_groups.append([line])
        
        # ç”Ÿæˆåˆä½µå¾Œçš„æ¨™è¨»
        for group in all_cl_groups:
            if len(group) == 1:
                name, value, color, weight, cl_type = group[0]
                annotations.append((value, f"{name} {cl_type} = {value:.3f}", color, weight))
            else:
                value = group[0][1]
                # ä½¿ç”¨æœ€é‡è¦çš„é¡è‰²ï¼ˆå„ªå…ˆé †åºï¼šorange > red > purpleï¼‰
                color = 'orange' if any(line[2] == 'orange' for line in group) else \
                       'red' if any(line[2] == 'red' for line in group) else 'purple'
                weight = 'bold' if any(line[3] == 'bold' for line in group) else 'normal'
                
                # åˆ†åˆ¥æ”¶é›† UCL å’Œ LCL çš„åç¨±
                ucl_names = [line[0] for line in group if line[4] == 'UCL']
                lcl_names = [line[0] for line in group if line[4] == 'LCL']
                
                # ç”Ÿæˆåˆä½µæ¨™ç±¤
                if ucl_names and lcl_names:
                    # UCL å’Œ LCL éƒ½æœ‰é‡ç–Šï¼Œé¡¯ç¤ºç‚º Sug_UCL=Sug_LCL æ ¼å¼
                    ucl_part = '='.join(ucl_names) + '_UCL' if len(ucl_names) > 1 else ucl_names[0] + '_UCL'
                    lcl_part = '='.join(lcl_names) + '_LCL' if len(lcl_names) > 1 else lcl_names[0] + '_LCL'
                    combined_label = f"{ucl_part}={lcl_part}"
                elif ucl_names:
                    # åªæœ‰ UCL é‡ç–Š
                    combined_label = '='.join(ucl_names) + ' UCL'
                else:
                    # åªæœ‰ LCL é‡ç–Š
                    combined_label = '='.join(lcl_names) + ' LCL'
                
                annotations.append((value, f"{combined_label} = {value:.3f}", color, weight))

        for y_val, text, color, weight in annotations:
            ax.text(1.02, y_val, text, transform=ax.get_yaxis_transform(),
                    color=color, fontsize=9, va='center', fontweight=weight, clip_on=False)

        # ======= è¼¸å‡ºåœ–æª” - ä¿å­˜ç‚º SVG å‘é‡æ ¼å¼ =======
        # SVG æ˜¯å‘é‡åœ–å½¢ï¼Œç„¡è«–å¦‚ä½•ç¸®æ”¾éƒ½å®Œç¾æ¸…æ™°
        filename_svg = os.path.join(output_dir, f"{chart_info['GroupName']}_{chart_info['ChartName']}.svg")
        plt.savefig(filename_svg, format='svg', bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        
        # åŒæ™‚ä¿å­˜ PNG ä½œç‚ºå‚™ç”¨
        filename_png = os.path.join(output_dir, f"{chart_info['GroupName']}_{chart_info['ChartName']}.png")
        plt.savefig(filename_png, dpi=200, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close()

        return filename_svg  # å„ªå…ˆè¿”å› SVG è·¯å¾‘

    def process_single_chart_data(self, chart_info_row, raw_data_df):
        """å°‡ Chart è¨­å®šèˆ‡åŸå§‹æ•¸æ“šåˆä½µä¸¦é‹è¡Œæ ¸å¿ƒ SOP è¨ˆç®—"""
        
        group_name = chart_info_row.get('GroupName', 'N/A')
        chart_name = chart_info_row.get('ChartName', 'N/A')
        
        print(f"    [Debug] é–‹å§‹è™•ç† {group_name}_{chart_name}")
        print(f"    [Debug] åŸå§‹ CSV æ•¸æ“š shape: {raw_data_df.shape}")
        print(f"    [Debug] åŸå§‹ CSV æ¬„ä½: {list(raw_data_df.columns)}")
        if len(raw_data_df) > 0 and 'point_val' in raw_data_df.columns:
            try:
                print(f"    [Debug] æ•¸æ“šç¯„åœ: {raw_data_df['point_val'].min():.4f} ~ {raw_data_df['point_val'].max():.4f}")
            except Exception as e:
                print(f"    [Debug] ç„¡æ³•è¨ˆç®—æ•¸æ“šç¯„åœ: {e}")
        else:
            print(f"    [Debug] CSV ç‚ºç©ºæˆ–ç¼ºå°‘ point_val æ¬„ä½")
        
        # 1. æ¬„ä½é‡å‘½å
        raw_data_df.rename(columns={'point_time': 'date', 'point_val': 'value'}, inplace=True)
        
        # 2. å°‡æ‰€æœ‰ç›¸é—œåƒæ•¸å¾ Chart Info å‚³éçµ¦ Raw Data DataFrame
        usl = chart_info_row.get('USL')
        lsl = chart_info_row.get('LSL')
        
        raw_data_df['USL'] = usl
        raw_data_df['LSL'] = lsl
        raw_data_df['DetectionLimit'] = chart_info_row.get('DetectionLimit')
        raw_data_df['Target'] = chart_info_row.get('Target')
        raw_data_df['UCL'] = chart_info_row.get('UCL')
        raw_data_df['LCL'] = chart_info_row.get('LCL')
        
        # 3. å‹•æ…‹è¨ˆç®— oos_flag - æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šæª¢æŸ¥å“ªå€‹è¦æ ¼é™
        characteristic = chart_info_row.get('Characteristics', 'Nominal')
        print(f"    [Debug] USL: {usl}, LSL: {lsl}, ç‰¹æ€§: {characteristic}")
        
        # æ ¹æ“šä¸åŒç‰¹æ€§é¡å‹è¨­å®š OOS æª¢æŸ¥é‚è¼¯
        if characteristic == 'Bigger':
            # Bigger chart åªéœ€è¦ LSLï¼Œä¸æª¢æŸ¥ USL
            if lsl is not None and not np.isnan(lsl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] < lsl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Bigger chart - åªæª¢æŸ¥ LSLï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Bigger chart - ç„¡æœ‰æ•ˆ LSLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
        elif characteristic == 'Smaller':
            # Smaller chart åªéœ€è¦ USLï¼Œä¸æª¢æŸ¥ LSL  
            if usl is not None and not np.isnan(usl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] > usl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Smaller chart - åªæª¢æŸ¥ USLï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Smaller chart - ç„¡æœ‰æ•ˆ USLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
        else:
            # Nominal æˆ–å…¶ä»–é¡å‹ï¼šæª¢æŸ¥é›™é‚Šè¦æ ¼é™
            if (usl is not None and not np.isnan(usl)) and \
               (lsl is not None and not np.isnan(lsl)) and \
               (usl > lsl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] > usl) | (raw_data_df['value'] < lsl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Nominal chart - æª¢æŸ¥é›™é‚Šè¦æ ¼é™ï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Nominal chart - ç„¡æœ‰æ•ˆ USL/LSLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
            
        print(f"    [Debug] æº–å‚™é€²å…¥æ ¸å¿ƒè¨ˆç®—ï¼Œç‰¹æ€§: {chart_info_row.get('Characteristics', 'Nominal')}")
        
        try:
            # 4. é‹è¡Œæ ¸å¿ƒè¨ˆç®—
            results = self.process_chart(
                df=raw_data_df,
                value_col='value',
                date_col='date',
                oos_col='oos_flag',
                characteristic=chart_info_row.get('Characteristics', 'Nominal')
            )
            
            # 5. æ ¼å¼åŒ–è¼¸å‡º
            final_output = chart_info_row.to_dict()
            final_output.update(results)
            final_output['Status'] = 'Success'
            
            # è®“èˆŠ UCL/LCL æ¬„ä½ä»ç‚º Suggest çš„å€¼
            final_output['Original UCL'] = final_output['Suggest UCL']
            final_output['Original LCL'] = final_output['Suggest LCL']
            
            # å››æ¨äº”å…¥ - ä½† Hard Rule ä¿æŒåŸå§‹ç²¾åº¦
            is_hard_rule = final_output.get('Pattern') == "Hard Rule Applied"
            
            if not is_hard_rule:  # åªæœ‰é Hard Rule æ‰é€²è¡Œå››æ¨äº”å…¥
                for key in ['Original UCL', 'Original LCL', 'Static UCL', 'Static LCL', 
                           'Suggest UCL', 'Suggest LCL', 'CL_Center']:
                    if key in final_output and not np.isnan(final_output.get(key, np.nan)):
                        final_output[key] = round(final_output[key], 4)
                
                # å››æ¨äº”å…¥ Sigma ç›¸é—œæ¬„ä½
                for sigma_key in ['Sigma_Est', 'Sigma_Est_Upper', 'Sigma_Est_Lower']:
                    if sigma_key in final_output and not np.isnan(final_output.get(sigma_key, np.nan)):
                        final_output[sigma_key] = round(final_output[sigma_key], 6)
                
                # å››æ¨äº”å…¥ K å€æ•¸æ¬„ä½
                for k_set_key in ['Original_UCL_K_Set', 'Original_LCL_K_Set',
                                 'Suggest_UCL_K_Set', 'Suggest_LCL_K_Set',
                                 'Ori_K_Set', 'Sug_K_Set']:
                    if k_set_key in final_output and not np.isnan(final_output.get(k_set_key, np.nan)):
                        final_output[k_set_key] = round(final_output[k_set_key], 3)
            
            # 6. ç”Ÿæˆç®¡åˆ¶åœ–
            try:
                plot_filename = self.plot_control_chart(
                    chart_data=raw_data_df,
                    chart_info=chart_info_row,
                    suggest_ucl=final_output['Suggest UCL'],
                    suggest_lcl=final_output['Suggest LCL'],
                    static_ucl=final_output['Static UCL'],
                    static_lcl=final_output['Static LCL'],
                    cl_center=final_output['CL_Center'],
                    pattern=final_output['Pattern']
                )
                final_output['PlotFile'] = plot_filename
            except Exception as plot_error:
                print(f"     [Warning] ç¹ªåœ–å¤±æ•—: {plot_error}")
                final_output['PlotFile'] = 'Plot Failed'
            
            return final_output
            
        except Exception as e:
            import traceback
            print(f"     [Error] é‹è¡Œæ ¸å¿ƒè¨ˆç®—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"     [Error] è©³ç´°éŒ¯èª¤è¿½è¸ª:")
            traceback.print_exc()
            return {
                'ChartName': chart_info_row['ChartName'], 
                'Status': 'Calculation Error', 
                'ErrorMessage': str(e),
                'PlotFile': 'Calculation Error'
            }

    def run_calculation(self, output_filename='CL_Calculation_Results.xlsx'):
        """åŸ·è¡Œå®Œæ•´çš„ CL è¨ˆç®—æµç¨‹"""
        
        if not self.chart_info_path or not os.path.exists(self.chart_info_path):
            raise ValueError(f"åœ–è¡¨è³‡è¨Šæª”æ¡ˆä¸å­˜åœ¨: {self.chart_info_path}")
            
        if not self.raw_data_dir or not os.path.exists(self.raw_data_dir):
            raise ValueError(f"åŸå§‹æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {self.raw_data_dir}")

        print("--- 1. è¼‰å…¥åœ–è¡¨é…ç½® ---")
        all_charts_info = self.load_chart_information(self.chart_info_path)
        if all_charts_info.empty:
            raise ValueError("ç„¡æ³•è¼‰å…¥æœ‰æ•ˆçš„åœ–è¡¨é…ç½®")

        self.results = []
        
        print(f"--- 2. è™•ç† {len(all_charts_info)} å¼µåœ–è¡¨çš„æ•¸æ“š ---")
        
        for index, chart_info in all_charts_info.iterrows():
            group_name = chart_info.get('GroupName', 'N/A')
            chart_name = chart_info.get('ChartName', 'N/A')
            
            print(f"  > è™•ç† Chart: {group_name}_{chart_name}...")
            
            filepath = self.find_matching_file(self.raw_data_dir, group_name, chart_name)
            
            if filepath is None:
                print(f"    [Warning] æœªæ‰¾åˆ°åŒ¹é…çš„åŸå§‹æ•¸æ“šæ–‡ä»¶ã€‚è·³éã€‚")
                result = chart_info.to_dict()
                result['Status'] = 'No Raw Data'
                result['PlotFile'] = 'No Raw Data'
                self.results.append(result)
                continue

            try:
                raw_df = pd.read_csv(filepath)
                results_dict = self.process_single_chart_data(chart_info, raw_df)
                self.results.append(results_dict)
                
            except Exception as e:
                print(f"    [Error] è®€å–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                error_result = chart_info.to_dict()
                error_result['Status'] = 'File Read Error'
                error_result['ErrorMessage'] = str(e)
                error_result['PlotFile'] = 'File Read Error'
                self.results.append(error_result)
                
        # --- 3. è¼¸å‡ºçµæœæ–‡ä»¶ ---
        
        df_output = pd.DataFrame(self.results)
        
        # èª¿æ•´è¼¸å‡ºæ¬„ä½é †åº
        output_cols_priority = [
            'GroupName', 'ChartName', 'ChartID', 'Material_no', 
            'Status', 'ErrorMessage',
            'Suggest UCL', 'Suggest LCL', 
            'Static UCL', 'Static LCL',
            'UCL', 'LCL',
            'Original UCL', 'Original LCL',
            'CL_Center', 'Sigma_Est', 'Sigma_Est_Upper', 'Sigma_Est_Lower',
            'Original_UCL_K_Set', 'Original_LCL_K_Set',
            'Suggest_UCL_K_Set', 'Suggest_LCL_K_Set',
            'Ori_K_Set', 'Sug_K_Set',
            'Ori_OOC_Count', 'Static_OOC_Count', 'Final_OOC_Count', 'Total_Adj_Units',
            'Target', 'USL', 'LSL', 
            'DetectionLimit', 
            'Pattern', 'Resolution_Estimated', 'Characteristics', 
            'TightenNeeded', 'Original_Tolerance', 'New_Tolerance', 'Diff_Ratio_%', 'Tighten_Threshold_%',
            'DataCountUsed', 'HardRule',
            'PlotFile' 
        ]
        
        existing_output_cols = [col for col in output_cols_priority if col in df_output.columns]
        df_output = df_output[existing_output_cols]
        
        df_output.to_excel(output_filename, index=False)
        
        print("\n--- 4. æµç¨‹çµæŸ ---")
        print(f"è¨ˆç®—çµæœå·²æˆåŠŸè¼¸å‡ºè‡³ï¼š{output_filename}")
        
        return df_output

    def get_results(self):
        """å–å¾—è¨ˆç®—çµæœ"""
        return self.results if hasattr(self, 'results') else []

# === åŸ·è¡Œå…¥å£ ===
if __name__ == '__main__':
    calculator = CLTightenCalculator(
        chart_info_path='input/All_Chart_Information.xlsx',
        raw_data_dir='input/raw_charts/'
    )
    calculator.run_calculation()