import pandas as pd
import numpy as np
import os

# === è¨­å®š matplotlib å¾Œç«¯ï¼Œç¢ºä¿æ‰“åŒ…ç’°å¢ƒä¸‹çš„å…¼å®¹æ€§ ===
import matplotlib
try:
    matplotlib.use('Agg')  # ä½¿ç”¨ Agg å¾Œç«¯ï¼Œé©åˆç„¡é¡¯ç¤ºç’°å¢ƒ
except:
    pass

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

from math import gcd, floor, ceil
from functools import reduce
from scipy.stats import skew, median_abs_deviation, kurtosis, norm, rankdata
import scipy.stats as stats

# è¨­å®šä¸­æ–‡å­—é«”ï¼ˆæ·»åŠ ç•°å¸¸è™•ç†ï¼‰
try:
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"[Warning] å­—é«”è¨­å®šå¤±æ•—ï¼Œä½¿ç”¨é è¨­å­—é«”: {e}")
    plt.rcParams['font.family'] = 'DejaVu Sans'


def transform_johnson_slifker_shapiro_full(data):
    """
    ä¿®æ­£ç‰ˆ: ä½¿ç”¨ Slifker-Shapiro (1980) æ¼”ç®—æ³•é€²è¡Œ Johnson è½‰æ›
    ä¿®æ­£é‡é»: 
    1. SU ä½¿ç”¨æ­£ç¢ºçš„ arccosh è¨ˆç®— delta
    2. SL ä½¿ç”¨æ­£ç¢ºçš„ log(x-xi) å…¬å¼ (ç„¡åˆ†æ¯)
    3. SB/å…¶ä»–æƒ…æ³ ä½¿ç”¨ Rank-based INT é¿å…å´©æ½°
    """
    data = np.array(data)
    n = len(data)
    
    if n < 10: # æ•¸æ“šå¤ªå°‘ä¸é©åˆç”¨ Slifker
        # å›å‚³ç°¡å–®çš„ Z-score
        return (data - np.mean(data)) / (np.std(data, ddof=1) + 1e-9), "Insufficient_Data"
    
    try:
        # Step 1: è¨ˆç®— Percentiles (z = 0.524)
        z_val = 0.524
        cdf_vals = norm.cdf([-3*z_val, -z_val, z_val, 3*z_val])
        
        # ä½¿ç”¨ linear æ’å€¼
        x_quants = np.percentile(data, cdf_vals * 100)
        x_neg_3z, x_neg_z, x_z, x_3z = x_quants[0], x_quants[1], x_quants[2], x_quants[3]
        
        # Step 2: è¨ˆç®— m, n, p
        m = x_3z - x_z
        n_val = x_neg_z - x_neg_3z
        p = x_z - x_neg_z
        
        if p <= 0 or m <= 0 or n_val <= 0:
             raise ValueError("Invalid quantile distance")
        
        # Step 3: è¨ˆç®— QR
        QR = (m * n_val) / (p**2)
        
        # Step 4: ç­–ç•¥åˆ†æ”¯
        
        # === Case A: Johnson SU (Unbounded) ===
        if QR > 1.05:
            system_type = "SU"
            
            # [ä¿®æ­£] ä½¿ç”¨ arccosh è¨ˆç®— delta
            cosh_arg = 0.5 * (m/p + n_val/p)
            if cosh_arg < 1: cosh_arg = 1.0001
            
            eta = 2 * z_val
            delta = eta / np.arccosh(cosh_arg)
            
            # è¨ˆç®— gamma
            sinh_arg = (n_val/p - m/p) / (2 * np.sqrt(QR - 1))
            gamma = delta * np.arcsinh(sinh_arg)
            
            # è¨ˆç®— lambda å’Œ xi
            lambda_param = (2 * p * np.sqrt(QR - 1)) / (m/p + n_val/p - 2)
            xi = (x_z + x_neg_z) / 2 - (p * (n_val/p - m/p)) / (2 * (m/p + n_val/p - 2))
            
            if lambda_param <= 0: raise ValueError("Lambda <= 0")
            
            # [å…¬å¼] SU: arcsinh
            transformed = gamma + delta * np.arcsinh((data - xi) / lambda_param)
            return transformed, system_type

        # === Case B: Johnson SL (Lognormal) ===
        elif 0.95 <= QR <= 1.05:
            system_type = "SL"
            
            # [ä¿®æ­£] SL åƒæ•¸ä¼°è¨ˆå…¬å¼
            eta = 2 * z_val
            # é€™è£¡ç”¨ m/p è¿‘ä¼¼ n/p
            ratio = m / p 
            if ratio <= 1: ratio = 1.0001
            
            delta = eta / np.log(ratio)
            
            # è¨ˆç®— xi (ä¸‹é™)
            # Slifker å° SL çš„ xi ä¼°è¨ˆ:
            xi = 0.5 * (x_z + x_neg_z) - 0.5 * p * (ratio + 1) / (ratio - 1)
            
            # è¨ˆç®— gamma
            # å¾ z = gamma + delta * ln(x_z - xi) åæ¨
            if (x_z - xi) <= 0: raise ValueError("Invalid SL gamma param")
            gamma = z_val - delta * np.log(x_z - xi)
            
            # [å®‰å…¨æ€§æª¢æŸ¥] ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½å¤§æ–¼ä¸‹é™ xi
            safe_data = data - xi
            if np.any(safe_data <= 0):
                # å¦‚æœæ•¸æ“šé•èƒŒ SL å‡è¨­ (æœ‰å€¼ <= ä¸‹é™)ï¼Œè½‰ç”¨ Rank
                raise ValueError("Data violates SL lower bound")
                
            # [å…¬å¼] SL: log(X - xi)  <--- æ³¨æ„ï¼šé€™è£¡æ²’æœ‰åˆ†æ¯ï¼
            transformed = gamma + delta * np.log(safe_data)
            return transformed, system_type

        else:
            # QR < 0.95 (SB) æˆ–å…¶ä»–æƒ…æ³
            # ç‚ºäº†ç³»çµ±ç©©å®šï¼Œçµ±ä¸€ä½¿ç”¨ Rank-based INT
            raise ValueError("QR indicates SB or Normal, fallback to Rank INT")
            
    except Exception as e:
        # Fallback: Rank-based Inverse Normal Transformation
        # é€™æ˜¯æœ€å®‰å…¨çš„å…œåº•æ–¹æ¡ˆ
        system_type = "Rank_INT"
        ranks = rankdata(data, method='average')
        probabilities = (ranks - 0.375) / (n + 0.25)
        transformed = norm.ppf(probabilities)
        transformed = np.clip(transformed, -6, 6)
        
        return transformed, system_type


class CLTightenCalculator:
    """Control Limit Tighten Calculator - ç®¡åˆ¶ç·šæ”¶ç·Šè¨ˆç®—å™¨"""
    
    def __init__(self, chart_info_path=None, raw_data_dir=None, start_date=None, end_date=None):
        """
        åˆå§‹åŒ– CL Tighten Calculator
        
        Args:
            chart_info_path: Chart è³‡è¨Šæª”æ¡ˆè·¯å¾‘
            raw_data_dir: åŸå§‹æ•¸æ“šç›®éŒ„è·¯å¾‘
            start_date: è‡ªè¨‚èµ·å§‹æ—¥æœŸ (datetime object)
            end_date: è‡ªè¨‚çµæŸæ—¥æœŸ (datetime object)
        """
        self.chart_info_path = chart_info_path
        self.raw_data_dir = raw_data_dir
        self.start_date = start_date
        self.end_date = end_date
        self.results = []
        
    # === Utility Functions ===
    
    def compute_resolution(self, values):
        """
        SOP 1.3: ä¼°è¨ˆè³‡æ–™è§£æåº¦ (Resolution) - å·¥æ¥­ç´šæŠ—å™ªç‰ˆ
        ç­–ç•¥ï¼šäº‹å‰æ¸…ç† (Pre-rounding) -> æ•´æ•¸ GCD -> äº‹å¾Œé–å®š (Post-rounding)
        """
        if len(values) < 2: 
            return None
        
        # 1. æ’åºä¸¦å»é‡
        sorted_vals = sorted(list(set(values)))
        if len(sorted_vals) < 2: 
            return None
        
        # === ã€é—œéµæ­¥é©Ÿ 1ã€‘äº‹å‰æ¸…ç†ï¼šè¨ˆç®—å·®å€¼æ™‚ç›´æ¥æ¿¾é™¤é›œè¨Š ===
        # ä½¿ç”¨ round(x, 10) å°‡ 0.09999999999999987 å¼·åˆ¶ä¿®æ­£ç‚º 0.1
        # é€™èƒ½ä¿è­‰é€²å…¥ GCD çš„æ•¸å­—æ˜¯ä¹¾æ·¨çš„
        diffs = []
        for i, j in zip(sorted_vals[:-1], sorted_vals[1:]):
            diff = j - i
            # å¦‚æœå·®å€¼æ¥µå°ï¼ˆå¯èƒ½æ˜¯æµ®é»æ•¸èª¤å·®é€ æˆçš„ 0ï¼‰ï¼Œå¿½ç•¥å®ƒ
            if diff < 1e-9:
                continue
            # é—œéµï¼šåœ¨é€™è£¡å°±å…ˆä¿®æ•´æ•¸å­—
            diffs.append(round(diff, 10))
            
        # å»é‡ï¼Œæ¸›å°‘è¨ˆç®—é‡
        unique_diffs = sorted(list(set(diffs)))
        if not unique_diffs: 
            return None
        
        # ========== å‹•æ…‹åˆ¤æ–·æ”¾å¤§å€ç‡ ==========
        # æ‰¾å‡ºæœ€å°çš„å·®å€¼ï¼Œæ±ºå®šè¦æ”¾å¤§å¤šå°‘å€æ‰èƒ½è®Šæˆæ•´æ•¸
        min_val = unique_diffs[0]
        
        # å°‹æ‰¾æœ€ä½³ scale_factor (è®“æ‰€æœ‰å·®å€¼è®Šæˆæ•´æ•¸çš„æœ€å°å€ç‡)
        scale_factor = 1
        found_scale = False
        
        # å˜—è©¦å¾ 10^0 åˆ° 10^8
        for p in range(9):
            factor = 10 ** p
            # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å·®å€¼ä¹˜ä¸Š factor å¾Œéƒ½æ¥è¿‘æ•´æ•¸ (èª¤å·®å°æ–¼ 1e-5)
            # ä½¿ç”¨ 1e-5 æ˜¯å› ç‚ºå‰é¢å·²ç¶“ round éäº†ï¼Œé€™è£¡å¯ä»¥å¯¬é¬†ä¸€é»
            if all(abs(d * factor - round(d * factor)) < 1e-5 for d in unique_diffs):
                scale_factor = factor
                found_scale = True
                break
        
        if not found_scale:
            # å¦‚æœæ‰¾ä¸åˆ°å®Œç¾å€ç‡ï¼Œä½¿ç”¨æ ¹æ“šå°æ•¸ä½æ•¸æ¨ç®—çš„å€ç‡ï¼ˆæœ€å¤§ä¿åº•ï¼‰
            # æ‰¾å‡ºæœ€å¤šå°æ•¸ä½æ•¸çš„æ•¸
            max_decimals = 0
            for val in unique_diffs:
                s = f"{val:.10f}".rstrip('0')
                if '.' in s:
                    max_decimals = max(max_decimals, len(s.split('.')[1]))
            scale_factor = 10 ** min(max_decimals, 8)

        # ========== æ•´æ•¸ GCD è¨ˆç®— ==========
        try:
            # æ”¾å¤§ä¸¦è½‰ç‚ºæ•´æ•¸
            scaled_diffs = [int(round(d * scale_factor)) for d in unique_diffs]
            
            if not scaled_diffs:
                return None
            
            # è¨ˆç®— GCD
            res_scaled = reduce(gcd, scaled_diffs)
            
            # ç¸®å°å›æµ®é»æ•¸
            resolution = res_scaled / scale_factor
            
            # é˜²å‘†ï¼šè§£æåº¦ä¸æ‡‰å¤§æ–¼æœ€å°å·®å€¼
            if resolution > min_val:
                resolution = min_val
            
            # === ã€é—œéµæ­¥é©Ÿ 2ã€‘äº‹å¾Œé–å®šï¼šå†æ¬¡æ¸…ç†çµæœ ===
            # å› ç‚ºé™¤æ³•å¯èƒ½åˆå¼•å…¥æ¥µå¾®å°çš„èª¤å·®ï¼Œæœ€å¾Œå†ä¸€æ¬¡ Round
            # é€™è£¡å°±æ˜¯ä½ åŸæœ¬æƒ³è¦çš„ã€Œæ‰¾åˆ°æœ€æ¥è¿‘ã€çš„å‹•ä½œ
            resolution = round(resolution, 10)
            
            return resolution
            
        except Exception as e:
            print(f"    [Warning] compute_resolution è¨ˆç®—å¤±æ•—: {e}ï¼Œè¿”å› None")
            return None

    def robust_zscore_sop2(self, values):
        """SOP 2.4/4.2: è¨ˆç®— Robust Z-score (zi)"""
        med = np.median(values)
        sd = np.std(values, ddof=1)  # ä½¿ç”¨æ¨£æœ¬æ¨™æº–å·®
        if sd == 0: 
            return np.zeros(len(values))
            
        mad = median_abs_deviation(values) 
        
        if mad == 0:
            mean_dev = np.mean(np.abs(values - med))
            if mean_dev == 0: 
                return np.zeros(len(values))
            z = 0.7979 * np.abs(values - med) / mean_dev
        else:
            z = 0.6745 * np.abs(values - med) / mad
        return z

    def compute_CB(self, values):
        """æ¨™æº– Bimodality Coefficient (BC)"""
        N = len(values)
        if N < 4: 
            return 0 
        sk = skew(values, bias=False)
        ku = kurtosis(values, fisher=True, bias=False)
        return (sk**2 + 1) / (ku+3)

    def compute_robust_sigma(self, values):
        """SOP 4.1: ä¾è³‡æ–™ç­†æ•¸è¨ˆç®— UR/LR (Robust Sigma)"""
        N = len(values)
        P = np.percentile
        median_val = np.median(values)
        
        if N < 4: 
            return np.std(values, ddof=1), np.std(values, ddof=1)
            
        # SOP 4.1 Robust Sigma ä¼°è¨ˆé‚è¼¯
        if N >= 10000:
            UR = (P(values, 99.9) - median_val)/3.09
            LR = (median_val - P(values, 0.1))/3.09
        elif 3000 <= N < 10000:
            UR = (P(values, 99.5) - median_val)/2.576
            LR = (median_val - P(values, 0.5))/2.576
        elif 300 <= N < 3000:
            UR = (P(values, 99) - median_val)/2.326
            LR = (median_val - P(values, 1))/2.326
        elif 100 <= N < 300:
            UR = (P(values, 97) - median_val)/1.881
            LR = (median_val - P(values, 3))/1.881
        else: 
            UR = (P(values, 95) - median_val)/1.645
            LR = (median_val - P(values, 5))/1.645
            
        # ç¢ºä¿ä¸æœƒè¿”å›è² å€¼
        return max(0, UR), max(0, LR)

    # === SOP 1: Data Integrity ===

    def data_integrity(self, df, date_col, value_col, oos_col):
        """SOP 1.1 & 1.2: ç¯©é¸æœ‰æ•ˆè³‡æ–™ä¸¦æ’é™¤ OOS
        
        å¦‚æœè¨­å®šäº†è‡ªè¨‚æ—¥æœŸç¯„åœ (self.start_date å’Œ self.end_date)ï¼Œå‰‡ä½¿ç”¨è‡ªè¨‚ç¯„åœ
        å¦å‰‡ä½¿ç”¨é è¨­çš„æœ€è¿‘2å¹´
        """
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        
        # åˆ¤æ–·æ˜¯å¦ä½¿ç”¨è‡ªè¨‚æ—¥æœŸç¯„åœ
        if self.start_date is not None and self.end_date is not None:
            # ä½¿ç”¨è‡ªè¨‚æ—¥æœŸç¯„åœ
            cutoff_start = pd.Timestamp(self.start_date)
            cutoff_end = pd.Timestamp(self.end_date)
            df_filtered = df[(df[date_col] >= cutoff_start) & (df[date_col] <= cutoff_end)].dropna(subset=[value_col])
            print(f"    ä½¿ç”¨è‡ªè¨‚æ—¥æœŸç¯„åœ: {cutoff_start.date()} è‡³ {cutoff_end.date()}")
        else:
            # ä½¿ç”¨é è¨­çš„æœ€è¿‘2å¹´
            cutoff = pd.Timestamp.today() - pd.DateOffset(years=2)
            df_filtered = df[df[date_col] >= cutoff].dropna(subset=[value_col])
            print(f"    ä½¿ç”¨é è¨­æ—¥æœŸç¯„åœ: æœ€è¿‘2å¹´ (å¾ {cutoff.date()} èµ·)")
        
        if oos_col in df_filtered.columns:
            # SOP 1.2: æ’é™¤ OOS é»
            df_filtered = df_filtered[~df_filtered[oos_col].astype(bool)]
        
        values = df_filtered[value_col].values
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
        if len(values) == 0:
            print(f"    [Warning] ç¶“éç¯©é¸å¾Œæ²’æœ‰æœ‰æ•ˆæ•¸æ“šé»")
            return values, None
            
        resolution = self.compute_resolution(values)
        return values, resolution

    def calculate_decimals_from_resolution(self, resolution):
        """âœ… æ­£ç¢ºè¨ˆç®— resolution çš„å°æ•¸ä½æ•¸ï¼ˆæ™ºèƒ½è™•ç†æµ®é»èª¤å·®ï¼‰"""
        from decimal import Decimal
        
        if resolution == 0:
            return 0
        elif isinstance(resolution, int):
            return 0
        
        try:
            # ğŸ”§ [æ™ºèƒ½ä¿®æ­£] å…ˆæª¢æŸ¥æ˜¯å¦æ¥è¿‘å¸¸è¦‹çš„ "ä¹¾æ·¨" resolution å€¼
            # å¸¸è¦‹å€¼ï¼š1, 0.1, 0.01, 0.001, 0.0001, 5, 0.5, 0.05, 0.005, 0.0005
            common_resolutions = [
                (1, 0), (0.5, 1), (0.1, 1), (0.05, 2), (0.01, 2),
                (0.005, 3), (0.001, 3), (0.0005, 4), (0.0001, 4),
                (0.00005, 5), (0.00001, 5), (0.000001, 6)
            ]
            
            # æª¢æŸ¥æ˜¯å¦éå¸¸æ¥è¿‘æŸå€‹å¸¸è¦‹å€¼ï¼ˆå®¹å¿åº¦ 1e-10ï¼‰
            for common_val, decimals in common_resolutions:
                if abs(resolution - common_val) < 1e-10:
                    print(f"    [Resolution ä¿®æ­£] {resolution:.18f} â†’ {common_val} (decimals={decimals})")
                    return decimals
            
            # å¦‚æœä¸æ˜¯å¸¸è¦‹å€¼ï¼Œä½¿ç”¨ Decimal é€²è¡Œç²¾ç¢ºè¨ˆç®—
            res_decimal = Decimal(str(resolution))
            # as_tuple() è¿”å› (sign, digits, exponent)
            # exponent ç‚ºè² å€¼æ™‚è¡¨ç¤ºå°æ•¸ä½æ•¸
            sign, digits, exponent = res_decimal.as_tuple()
            
            if exponent >= 0:
                # æ•´æ•¸æˆ–ç§‘å­¸è¨˜è™Ÿï¼Œç„¡å°æ•¸ä½
                return 0
            else:
                # exponent ç‚ºè² å€¼ï¼Œå°æ•¸ä½æ•¸ = abs(exponent)
                return -exponent
        except Exception as e:
            print(f"Warning: Failed to calculate decimals from {resolution}: {e}")
            return 4  # é è¨­ç‚º 4 ä½å°æ•¸

    def apply_resolution_precision(self, value, resolution, value_name="value"):
        """
        âœ… çµ±ä¸€çš„ç²¾åº¦é–å®šå‡½æ•¸ï¼šæ ¹æ“š resolution å°‡æ•¸å€¼å°é½Šåˆ°æ­£ç¢ºçš„å°æ•¸ä½æ•¸
        
        Args:
            value: è¦è™•ç†çš„æ•¸å€¼
            resolution: è³‡æ–™è§£æåº¦
            value_name: æ•¸å€¼åç¨±ï¼ˆç”¨æ–¼ debug è¼¸å‡ºï¼‰
            
        Returns:
            å°é½Šå¾Œçš„æ•¸å€¼
        """
        if value is None or np.isnan(value):
            return value
            
        if resolution is None or resolution <= 0:
            return value  # ç„¡æ•ˆ resolutionï¼Œä¿æŒåŸå€¼
        
        try:
            decimals = self.calculate_decimals_from_resolution(resolution)
            old_value = float(value)
            new_value = round(old_value, decimals)
            
            # å¦‚æœæ˜¯ 0 ä½å°æ•¸ï¼Œè½‰ç‚ºæ•´æ•¸
            if decimals == 0:
                new_value = int(new_value)
            
            # Debug è¼¸å‡ºï¼ˆåªåœ¨æœ‰è®ŠåŒ–æ™‚ï¼‰
            if abs(old_value - float(new_value)) > 1e-10:
                print(f"    [Precision] {value_name}: {old_value:.10f} â†’ {new_value} (decimals={decimals})")
            
            return new_value
        except Exception as e:
            print(f"    [Warning] Failed to apply precision to {value_name}: {e}")
            return value

    # === Hard Rule Function ===

    def apply_discrete_hard_rules(self, values, resolution, N):
        if N < 4:
            return None, None, False, None 

        values = np.array(values)
        val_counts = pd.Series(values).value_counts()
        
        if val_counts.empty:
            return None, None, False, None

        mode_val = val_counts.idxmax()
        category_num = len(val_counts)
        
        # Rule 1: If Constant or non mode data# = 1
        non_mode_count = N - val_counts.max()
        if non_mode_count == 0 or non_mode_count == 1:
            return mode_val, mode_val, True, "Hard Rule 1: Constant/Near Constant"

        min_val = np.min(values)
        max_val = np.max(values)
        
        # Rule 2: If data value category# = 2
        if category_num == 2:
            return max_val, min_val, True, "Hard Rule 2: Two Categories"

        # Rule 3: If data value category# = 3 and max â€“ min = 2*resolution
        if category_num == 3 and resolution is not None and resolution > 0:
            if abs((max_val - min_val) - 2 * resolution) < 1e-6: 
                return max_val, min_val, True, "Hard Rule 3: Three Categories Spaced by Resolution"
                
        return None, None, False, None

    # === SOP 2 & 3: Pattern Diagnosis ===

    def data_prep_for_pattern(self, values):
            """
            Pattern è­˜åˆ¥å‰çš„æ•¸æ“šé è™•ç†ã€‚
            ä¿®æ”¹ç´€éŒ„: å°‡ Johnson SB fit (MLE) æ›¿æ›ç‚º Quantile Regression ä»¥æå‡ç©©å®šæ€§ã€‚
            """
            values = np.array(values) # ç¢ºä¿æ˜¯ array
            N_orig = len(values)
            if N_orig == 0: 
                return values
            if N_orig < 4: 
                return values
                
            val_counts = pd.Series(values).value_counts()
            mode_val = val_counts.idxmax()
            mode_count = val_counts.max()
            non_mode_count = N_orig - mode_count
            
            # SOP 2.1: æ•¸æ“šé¸æ“‡ (Mode Balancing)
            if non_mode_count / N_orig < 0.25:
                max_mode_use = min(mode_count, non_mode_count * 3)
                
                # ç‰¹æ®Šæƒ…æ³ï¼šç•¶æ‰€æœ‰æ•¸æ“šéƒ½ç›¸åŒæ™‚ï¼ˆnon_mode_count = 0ï¼‰ï¼Œç›´æ¥è¿”å›åŸæ•¸æ“š
                if non_mode_count == 0:
                    print(f"    [Debug] data_prep_for_pattern: æ‰€æœ‰æ•¸æ“šéƒ½ç›¸åŒ ({mode_val})ï¼Œç›´æ¥è¿”å›åŸæ•¸æ“š")
                    return values
                    
                mode_data = values[values == mode_val][:max_mode_use]
                non_mode_data = values[values != mode_val]
                w = np.concatenate((mode_data, non_mode_data))
            else:
                w = values.copy()
                
            # æª¢æŸ¥ w æ˜¯å¦ç‚ºç©º
            if len(w) == 0:
                print(f"    [Debug] data_prep_for_pattern: é è™•ç†å¾Œæ•¸æ“šç‚ºç©ºï¼Œè¿”å›åŸæ•¸æ“š")
                return values
            
            # SOP 2.3: Johnson Transformation using Slifker-Shapiro Method
            try:
                y, transform_type = transform_johnson_slifker_shapiro_full(w)
                print(f"    [Debug] ä½¿ç”¨ {transform_type} è½‰æ›æ–¹æ³•")
            except Exception as e:
                print(f"    [Debug] è½‰æ›å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æ•¸æ“š: {e}")
                y = w  # è‹¥é€£ Rank éƒ½å¤±æ•—(æ¥µå°‘è¦‹)ï¼Œå›é€€åŸæ•¸æ“š
                
            # SOP 2.4: Robust Z-score
            z = self.robust_zscore_sop2(y)
            
            # SOP 2.5: é‚Šç•Œéæ¿¾æ³• (ä½¿ç”¨ SOP å®šç¾©çš„ 6.0 é–¾å€¼)
            # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ 6.0 (åŸç¨‹å¼ç¢¼è¨­å®š)ï¼Œèˆ‡ outlier_filter çš„ 4.5 ä¸åŒ
            normal_mask = z <= 6.0
            
            if not np.any(normal_mask):
                return values  # å…¨è¢«åˆ¤ç•°å¸¸ï¼Œä¸éæ¿¾
            
            # ç”¨ w çš„æ­£å¸¸ç¯„åœç¯©é¸åŸå§‹ values
            min_normal = np.min(w[normal_mask])
            max_normal = np.max(w[normal_mask])
            
            # ç”¨é‚Šç•Œç¯©é¸åŸå§‹ values
            values_filtered = values[(values >= min_normal) & (values <= max_normal)]
            
            # æª¢æŸ¥æ¿¾é™¤æ¯”ä¾‹ï¼ˆç”¨åŸå§‹ values è¨ˆç®—ï¼‰
            filter_ratio = (N_orig - len(values_filtered)) / N_orig
            
            if filter_ratio > 0.05:
                # è¶…é 5%ï¼Œåªæ¿¾é™¤å…·æœ‰æœ€å¤§ Z-score çš„å€¼ï¼ˆw ä¸­çš„æœ€æ¥µç«¯å€¼ï¼‰
                max_z_value = np.max(z)
                max_z_mask = (z == max_z_value)
                max_outlier_vals = w[max_z_mask]  # æ‰€æœ‰æœ€å¤§ ZI å°æ‡‰çš„ w å€¼
                unique_max_vals = np.unique(max_outlier_vals)  # å»é‡
                
                # å¾åŸå§‹æ•¸æ“šä¸­ç§»é™¤æ‰€æœ‰é€™äº› w å€¼
                # æ³¨æ„ï¼šé€™æœƒç§»é™¤ values ä¸­æ‰€æœ‰ç­‰æ–¼é€™äº›å€¼çš„é»ï¼Œå³ä½¿åŒå€¼æœ‰å¤šå€‹
                values_filtered = values.copy()
                for outlier_val in unique_max_vals:
                    values_filtered = values_filtered[values_filtered != outlier_val]
            
            return values_filtered

    def pattern_diagnosis(self, values, resolution=None):
        N = len(values)
        if N < 4: 
            return "Insufficient Data", np.nan, np.nan
        
        sigma = np.std(values, ddof=1)
        val_counts = pd.Series(values).value_counts()
        category_num = len(val_counts)
        sk = skew(values, bias=False)
        cb = self.compute_CB(values)
        
        if 4 <= N < 20: 
            if sigma == 0: 
                return "Constant", sk, cb
            
            near_constant_cond = (N - val_counts.max() == 1)
            attribute_cond = (category_num / N <= 1/3 and category_num <= 5)
            
            if near_constant_cond: 
                return "Near Constant", sk, cb
            if attribute_cond: 
                return "Attribute", sk, cb
            if category_num / N >= 1/2:
                if sk > 0.6: 
                    return "Skew-Right", sk, cb
                if sk < -0.6: 
                    return "Skew-Left", sk, cb
            return "Normal", sk, cb
        
        if N >= 20: 
            if sigma == 0: 
                return "Constant", sk, cb
                
            near_constant_cond = (val_counts.max() / N >= 0.9)
            
            attr_cond_A = (category_num / N <= 1/3 and category_num <= 5)
            attr_cond_B = (resolution and sigma != 0 and resolution / (6*sigma) >= 0.1 and category_num < 10)
            attr_cond_C = (N >= 30 and category_num <= 10)
            attribute_cond = (attr_cond_A or attr_cond_B or attr_cond_C)
            
            if near_constant_cond: 
                return "Near Constant", sk, cb
            if attribute_cond: 
                return "Attribute", sk, cb
                
            if cb > 0.6 and -0.6 <= sk <= 0.6: 
                return "Bimodal", sk, cb
                
            # åˆ¤æ–·åæ–œ
            if sk > 0.6:
                return "Skew-Right", sk, cb
            elif sk < -0.6:
                return "Skew-Left", sk, cb

            # å…¶é¤˜æƒ…æ³è¦–ç‚ºæ­£å¸¸åˆ†å¸ƒ
            return "Normal", sk, cb

    # === SOP 4: Outlier Exclusion ===
    def outlier_filter(self, values, pattern):
            values = np.array(values)
            N_orig = len(values)
            if N_orig == 0 or np.std(values, ddof=1) == 0: 
                return values
            
            print(f"\n    ===== Outlier Filter Debug (Pattern: {pattern}) =====")
            print(f"    åŸå§‹æ•¸æ“šç­†æ•¸: {N_orig}")
            print(f"    åŸå§‹æ•¸æ“šç¯„åœ: [{np.min(values):.6f}, {np.max(values):.6f}]")
            print(f"    åŸå§‹æ•¸æ“šçµ±è¨ˆ: Mean={np.mean(values):.6f}, Median={np.median(values):.6f}, Std={np.std(values, ddof=1):.6f}")
            
            if pattern in ["Skew-Right","Skew-Left"]:
                # (é€™éƒ¨åˆ†ä¿æŒåŸæœ¬é‚è¼¯)
                print(f"\n    --- Skew Pattern æ¿¾é™¤æµç¨‹ ---")
                median_val = np.median(values)
                U_R, L_R = self.compute_robust_sigma(values)
                
                print(f"    Median: {median_val:.6f}")
                print(f"    Robust Sigma: U_R={U_R:.6f}, L_R={L_R:.6f}")
                print(f"    Upper Threshold: {median_val + 6*U_R:.6f}")
                print(f"    Lower Threshold: {median_val - 6*L_R:.6f}")
                
                mask = (values <= median_val + 6*U_R) & (values >= median_val - 6*L_R)
                values_pre_filtered = values[mask]
                
                outliers = values[~mask]
                if len(outliers) > 0:
                    print(f"    è¢« 6Ïƒ æ¿¾é™¤çš„é» ({len(outliers)}): {sorted(outliers)[:10]}..." if len(outliers) > 10 else f"    è¢« 6Ïƒ æ¿¾é™¤çš„é» ({len(outliers)}): {sorted(outliers)}")
                else:
                    print(f"    ç„¡é»è¢« 6Ïƒ æ¿¾é™¤")
                
                filter_ratio = (N_orig - len(values_pre_filtered)) / N_orig
                print(f"    æ¿¾é™¤æ¯”ä¾‹: {filter_ratio*100:.2f}% ({N_orig - len(values_pre_filtered)}/{N_orig})")
                
                if filter_ratio <= 0.05:  # 5% ä»¥å…§ï¼Œä½¿ç”¨æ¿¾é™¤å¾Œçš„çµæœ
                    print(f"    âœ“ æ¿¾é™¤æ¯”ä¾‹ â‰¤ 5%ï¼Œä½¿ç”¨æ¿¾é™¤å¾Œçš„çµæœ")
                    print(f"    æœ€çµ‚ä¿ç•™æ•¸æ“šç­†æ•¸: {len(values_pre_filtered)}")
                    print(f"    æœ€çµ‚æ•¸æ“šç¯„åœ: [{np.min(values_pre_filtered):.6f}, {np.max(values_pre_filtered):.6f}]")
                    return values_pre_filtered
                else:  # è¶…é 5%ï¼Œåªæ¿¾é™¤æ‰€æœ‰æœ€å¤§æ¯”ä¾‹çš„é»
                    print(f"    âœ— æ¿¾é™¤æ¯”ä¾‹ > 5%ï¼Œæ”¹ç”¨æœ€å¤§æ¯”ä¾‹æ¿¾é™¤æ³•")
                    ratio_upper = (values - median_val) / U_R
                    ratio_lower = (median_val - values) / L_R
                    ratio = np.maximum(ratio_upper, ratio_lower) 
                    max_ratio_value = np.max(ratio)
                    max_ratio_mask = ratio == max_ratio_value
                    max_outliers = values[max_ratio_mask]
                    
                    print(f"    æœ€å¤§æ¯”ä¾‹å€¼: {max_ratio_value:.4f}")
                    print(f"    æœ€å¤§æ¯”ä¾‹å°æ‡‰çš„é» ({len(max_outliers)}): {sorted(max_outliers)}")
                    
                    values_filtered = values[~max_ratio_mask]
                    print(f"    æœ€çµ‚ä¿ç•™æ•¸æ“šç­†æ•¸: {len(values_filtered)}")
                    print(f"    æœ€çµ‚æ•¸æ“šç¯„åœ: [{np.min(values_filtered):.6f}, {np.max(values_filtered):.6f}]")
                    return values_filtered
            
            else:
                # SOP 4.2: Other Pattern - Johnson Transformation + Boundary Filter Method
                print(f"\n    --- Other Pattern Johnson è½‰æ› + é‚Šç•Œéæ¿¾æ³• ---")
                
                val_counts = pd.Series(values).value_counts()
                mode_val = val_counts.idxmax()
                mode_count = val_counts.max()
                non_mode_count = N_orig - mode_count
                
                print(f"    Mode å€¼: {mode_val:.6f} (å‡ºç¾ {mode_count} æ¬¡)")
                print(f"    Non-Mode æ•¸é‡: {non_mode_count}")
                print(f"    Non-Mode æ¯”ä¾‹: {non_mode_count/N_orig*100:.2f}%")
                
                # ========== Step 1: Mode å¹³è¡¡è™•ç† (SOP 2.1) ==========
                if non_mode_count / N_orig < 0.25:
                    print(f"\n    Step 1: Mode å¹³è¡¡è™•ç† (Non-Mode < 25%)")
                    max_mode_use = min(mode_count, non_mode_count * 3) if non_mode_count > 0 else mode_count
                    
                    if non_mode_count == 0:
                        print(f"    âš  æ‰€æœ‰æ•¸æ“šç›¸åŒï¼Œç„¡éœ€æ¿¾é™¤")
                        return values
                    
                    print(f"    Mode ä½¿ç”¨æ•¸é‡: {max_mode_use} (åŸ {mode_count})")
                    print(f"    Non-Mode ä½¿ç”¨æ•¸é‡: {non_mode_count}")
                    
                    mode_data = values[values == mode_val][:max_mode_use]
                    non_mode_data = values[values != mode_val]
                    w = np.concatenate((mode_data, non_mode_data))
                    
                    print(f"    å¹³è¡¡å¾Œ w æ•¸é‡: {len(w)} (Mode: {len(mode_data)}, Non-Mode: {len(non_mode_data)})")
                else:
                    print(f"\n    Step 1: ç„¡éœ€ Mode å¹³è¡¡ (Non-Mode â‰¥ 25%)")
                    w = values.copy()
                
                if len(w) == 0:
                    print(f"    âš  w ç‚ºç©ºï¼Œç„¡éœ€æ¿¾é™¤")
                    return values
                
                print(f"    w æ•¸æ“šç¯„åœ: [{np.min(w):.6f}, {np.max(w):.6f}]")
                
                # ========== Step 2: [ä¿®æ”¹] ç§»é™¤ Min-Max æ¨™æº–åŒ–ï¼Œç›´æ¥ä½¿ç”¨ Slifker-Shapiro ==========
                print(f"\n    Step 2: Johnson Transformation using Slifker-Shapiro Method")
                
                try:
                    y, transform_type = transform_johnson_slifker_shapiro_full(w)
                    print(f"    [Debug-Johnson] ä½¿ç”¨ {transform_type} è½‰æ›æ–¹æ³•")
                    print(f"    [Debug-Johnson] y (Z-score) ç¯„åœ = [{np.min(y):.4f}, {np.max(y):.4f}]")
                except Exception as e:
                    print(f"    [Error-Johnson] Johnson è½‰æ›å¤±æ•—: {e}ï¼Œå›é€€åˆ°åŸå§‹ w")
                    y = w
                
                # ========== Step 3: Robust Z-score (SOP 2.4) ==========
                print(f"\n    Step 3: Robust Z-score è¨ˆç®—")
                z = self.robust_zscore_sop2(y)
                print(f"    Robust Z-score ç¯„åœ: [{np.min(z):.4f}, {np.max(z):.4f}]")
                print(f"    Z-score å‰ 10 å€‹å€¼: {z[:10]}")
                
                max_z_idx = np.argmax(z)
                print(f"    æœ€å¤§ Z-score: {z[max_z_idx]:.4f} (ç´¢å¼• {max_z_idx})")
                print(f"    å°æ‡‰ w å€¼: {w[max_z_idx]:.6f}")
                
                # ========== Step 4: é‚Šç•Œéæ¿¾æ³• (SOP 2.5) ==========
                print(f"\n    Step 4: é‚Šç•Œéæ¿¾æ³•")
                normal_mask = z <= 4.5
                outlier_count = (~normal_mask).sum()
                
                print(f"    Z > 4.5 çš„ç•°å¸¸é»æ•¸é‡: {outlier_count}")
                
                if outlier_count > 0:
                    outlier_z = z[~normal_mask]
                    outlier_w = w[~normal_mask]
                    print(f"    ç•°å¸¸é» Z-scores: {outlier_z}")
                    print(f"    ç•°å¸¸é»å°æ‡‰ w å€¼: {outlier_w}")
                
                if not np.any(normal_mask):
                    print(f"    âš  æ‰€æœ‰é»éƒ½è¢«åˆ¤ç‚ºç•°å¸¸ï¼Œä¸é€²è¡Œæ¿¾é™¤")
                    return values 
                
                # âœ… ç”¨ w çš„æ­£å¸¸ç¯„åœä¾†éæ¿¾åŸå§‹ values
                min_normal = np.min(w[normal_mask])
                max_normal = np.max(w[normal_mask])
                
                print(f"    æ­£å¸¸æ•¸æ“šé‚Šç•Œ: [{min_normal:.6f}, {max_normal:.6f}]")
                
                values_filtered = values[(values >= min_normal) & (values <= max_normal)]
                filtered_out = values[(values < min_normal) | (values > max_normal)]
                
                print(f"    åŸå§‹æ•¸æ“šä¸­è¢«æ¿¾é™¤çš„é» ({len(filtered_out)}): {sorted(filtered_out)}")
                
                filter_ratio = (N_orig - len(values_filtered)) / N_orig
                print(f"    æ¿¾é™¤æ¯”ä¾‹: {filter_ratio*100:.2f}% ({N_orig - len(values_filtered)}/{N_orig})")
                
                if filter_ratio > 0.05:
                    print(f"    âœ— æ¿¾é™¤æ¯”ä¾‹ > 5%ï¼Œæ”¹ç”¨æœ€å¤§ Z-score æ¿¾é™¤æ³•")
                    # æ‰¾å‡ºæ‰€æœ‰å…·æœ‰æœ€å¤§ Z-score çš„ w å€¼
                    max_z_value = np.max(z)
                    max_z_mask = (z == max_z_value)
                    max_outlier_vals = w[max_z_mask]  # æ‰€æœ‰æœ€å¤§ ZI å°æ‡‰çš„ w å€¼
                    unique_max_vals = np.unique(max_outlier_vals)  # å»é‡
                    
                    print(f"    æœ€å¤§ Z-score: {max_z_value:.4f}")
                    print(f"    å…·æœ‰æœ€å¤§ ZI çš„ w å€¼ ({len(unique_max_vals)}): {unique_max_vals}")
                    
                    # å¾åŸå§‹æ•¸æ“šä¸­ç§»é™¤æ‰€æœ‰é€™äº› w å€¼
                    values_filtered = values.copy()
                    for outlier_val in unique_max_vals:
                        max_outlier_mask = values_filtered == outlier_val
                        max_outlier_points = values_filtered[max_outlier_mask]
                        if len(max_outlier_points) > 0:
                            print(f"    æ¿¾é™¤ w={outlier_val:.6f} çš„é» ({len(max_outlier_points)} å€‹)")
                        values_filtered = values_filtered[~max_outlier_mask]
                else:
                    print(f"    âœ“ æ¿¾é™¤æ¯”ä¾‹ â‰¤ 5%ï¼Œä½¿ç”¨é‚Šç•Œæ¿¾é™¤çµæœ")
                
                print(f"    æœ€çµ‚ä¿ç•™æ•¸æ“šç­†æ•¸: {len(values_filtered)}")
                print(f"    æœ€çµ‚æ•¸æ“šç¯„åœ: [{np.min(values_filtered):.6f}, {np.max(values_filtered):.6f}]")
                print(f"    ===== Outlier Filter å®Œæˆ =====\n")
                
                return values_filtered

    # === SOP 5: Control Limit Calculation ===

    def get_k_value(self, N, characteristic, pattern='Normal', kurtosis_value=None):
        """
        SOP 5.2: ä¾è³‡æ–™ç­†æ•¸å’Œç‰¹æ€§æ±ºå®š k å€¼
        
        âœ… ç‰¹æ®Šé‚è¼¯ï¼šç•¶ Kurtosis > 1 ä¸” Pattern = Normal æ™‚ï¼Œå„åŠ  1 sigma
        - N >= 30: 3Ïƒ â†’ 4Ïƒ
        - 16 <= N <= 29: 4Ïƒ â†’ 5Ïƒ
        - 4 <= N <= 15: 5Ïƒ â†’ 6Ïƒ
        """
        # åŸºç¤ k å€¼
        if N >= 30:
            base_k = 3.0
        elif 16 <= N <= 29: 
            base_k = 4.0
        elif 4 <= N <= 15: 
            base_k = 5.0
        else:
            base_k = 3.0
        
        # ğŸ”¥ ç‰¹æ®Šé‚è¼¯ï¼šKurtosis > 1 ä¸” Pattern = Normal æ™‚ï¼ŒåŠ  1Ïƒ
        if pattern == 'Normal' and kurtosis_value is not None and kurtosis_value > 1:
            final_k = base_k + 1.0  # å„åŠ  1Ïƒï¼š3â†’4, 4â†’5, 5â†’6
            print(f"    [Kurtosis +1Ïƒ] Pattern={pattern}, Kurtosis={kurtosis_value:.3f} > 1")
            print(f"    [Kurtosis +1Ïƒ] N={N}, Base k={base_k:.1f} â†’ Final k={final_k:.1f}")
            return final_k
        
        return base_k

    def calc_CL(self, values, pattern, resolution=None, characteristic='Nominal', kurtosis_value=None):
        """SOP 5.1 & 5.3: è¨ˆç®— UCL/LCL"""
        N = len(values)
        if N < 4: 
            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan
            
        median_val = np.median(values)
        mean_val = np.mean(values)
        std_val = np.std(values, ddof=1)
        k = self.get_k_value(N, characteristic, pattern, kurtosis_value)
        
        # é å…ˆè¨ˆç®— Robust Sigma å’Œ ECDF 3-sigma å‚™ç”¨å€¼
        UR_robust, LR_robust = self.compute_robust_sigma(values)
        UCL3_ecdf, LCL3_ecdf = np.nan, np.nan 
        
        # --- 1. è™•ç† Normal æ¨¡å¼ (Mean +/- k*Std) ---
        if pattern == "Normal": 
            UCL = mean_val + k * std_val
            LCL = mean_val - k * std_val
            center_line = mean_val
            sigma_for_spec = std_val
        
        # --- 2. è™•ç† Skew æ¨¡å¼ (Median +/- k*RobustSigma) ---
        elif pattern in ["Skew-Right", "Skew-Left"]: 
            UCL = median_val + k * UR_robust
            LCL = median_val - k * LR_robust
            center_line = median_val
            # Skew æ¨¡å¼ï¼šä¸Šä¸‹åˆ†é–‹ä½¿ç”¨å„è‡ªçš„ sigmaï¼ˆä¸å–æœ€å¤§å€¼ï¼‰
            sigma_upper = UR_robust
            sigma_lower = LR_robust
            
        # --- 3. è™•ç† ECDF ç›¸é—œæ¨¡å¼ (Bimodal, Attribute, Constant, Near Constant) ---
        else:
            # 3.1 ECDF: å¼·åˆ¶ä½¿ç”¨ ECDF ç¢ºå®šåŸºç¤ 3-sigma æ°´ä½ (UCL3, LCL3)
            p_low_3sigma = 0.135
            p_high_3sigma = 99.865
            
            try:
                UCL3_ecdf = np.percentile(values, p_high_3sigma)
                LCL3_ecdf = np.percentile(values, p_low_3sigma)
            except Exception:
                UCL3_ecdf = median_val + 3 * UR_robust
                LCL3_ecdf = median_val - 3 * LR_robust

            # 3.2 Tolerance æ“´å±•é‚è¼¯ (ä¿®æ”¹ï¼šä¸Šä¸‹åˆ†åˆ¥è¨˜ä½å„è‡ªçš„ tolerance)
            T_upper = UCL3_ecdf - median_val
            T_lower = median_val - LCL3_ecdf
            
            # ä¸Šä¸‹å„è‡ªæ ¹æ“š k å€æ•¸æ”¾å¤§
            if T_upper > 0:
                T_upper_new = T_upper * (k / 3.0)
                UCL = median_val + T_upper_new
            else:
                UCL = UCL3_ecdf
            
            if T_lower > 0:
                T_lower_new = T_lower * (k / 3.0)
                LCL = median_val - T_lower_new
            else:
                LCL = LCL3_ecdf
            
            center_line = median_val
            # å°æ–¼ECDFæ¨¡å¼ï¼Œä½¿ç”¨T_upperå’ŒT_lowerçš„å¹³å‡å€¼é™¤ä»¥3ä½œç‚ºsigma
            sigma_for_spec = (T_upper + T_lower) / 6.0 if (T_upper > 0 and T_lower > 0) else max(UR_robust, LR_robust)
        
        # === æ–°éœ€æ±‚1: è¨ˆç®— Sug USL/LSL (center line Â± 6*sigma) ===
        # Skew æ¨¡å¼ï¼šä¸Šä¸‹åˆ†é–‹è¨ˆç®—ï¼ˆä½¿ç”¨å„è‡ªçš„ sigmaï¼‰
        if pattern in ["Skew-Right", "Skew-Left"]:
            Sug_USL = center_line + 6 * sigma_upper
            Sug_LSL = center_line - 6 * sigma_lower
        # Normal å’Œ ECDF æ¨¡å¼ï¼šä½¿ç”¨çµ±ä¸€çš„ sigma
        else:
            Sug_USL = center_line + 6 * sigma_for_spec
            Sug_LSL = center_line - 6 * sigma_for_spec
        
        # è¿”å›åŸå§‹è¨ˆç®—å€¼ï¼ˆä¸åœ¨æ­¤è™•åš resolution èª¿æ•´ï¼‰
        return UCL, LCL, UR_robust, LR_robust, UCL3_ecdf, LCL3_ecdf, Sug_USL, Sug_LSL

    # === SOP 6: Control Limit Adjustment & Tighten ===

    def adjust_CL_based_on_OOC(self, values, UCL, LCL, pattern, resolution, sigma_est_u, sigma_est_l, max_adj_units=2, characteristic='Nominal'):
        values = np.array(values)  # ç¡®ä¿ values æ˜¯ numpy array
        current_UCL, current_LCL = UCL, LCL
        N = len(values)
        
        print(f"    [Debug OOC] === é–‹å§‹ OOC é€€æ ¼èª¿æ•´ ===")
        print(f"    [Debug OOC] è¼¸å…¥æ•¸æ“šé•·åº¦ N: {N}")
        print(f"    [Debug OOC] åˆå§‹ UCL: {UCL:.6f}, LCL: {LCL:.6f}")
        print(f"    [Debug OOC] Pattern: {pattern}, Characteristic: {characteristic}")
        print(f"    [Debug OOC] Resolution: {resolution}")
        print(f"    [Debug OOC] Sigma_est_u: {sigma_est_u:.6f}, Sigma_est_l: {sigma_est_l:.6f}")
        print(f"    [Debug OOC] æ•¸æ“šç¯„åœ: min={np.min(values):.6f}, max={np.max(values):.6f}")
        
        sigma_u = sigma_est_u if sigma_est_u > 1e-9 else 1e-9
        sigma_l = sigma_est_l if sigma_est_l > 1e-9 else 1e-9

        if pattern == "Constant":
            adj_u = adj_l = 0.0
        elif pattern in ["Near Constant", "Attribute"]:
            # Discrete æ¨¡å¼ï¼šæ¯æ¬¡åŠ  1 resolutionï¼Œç„¡å›ºå®šè¿­ä»£æ¬¡æ•¸ä¸Šé™
            if resolution is not None and resolution > 0:
                adj_u = adj_l = resolution
            else:
                adj_u = adj_l = 0.0
        else:
            # Continuous æ¨¡å¼ (Normal, Skew-Right, Skew-Left, Bimodal)ï¼šæ¯æ¬¡ 0.25Ïƒ
            adj_u = 0.25 * sigma_u
            adj_l = 0.25 * sigma_l
        
        print(f"    [Debug OOC] èª¿æ•´é‡ adj_u: {adj_u:.6f}, adj_l: {adj_l:.6f}")
            
        # æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šæ˜¯å¦åªèª¿æ•´å–®é‚Š
        adjust_ucl = True
        adjust_lcl = True
        
        if characteristic == 'Smaller':
            # Smaller åªéœ€è¦ tighten UCL (ä¸Šé™)
            adjust_lcl = False
            adj_l = 0.0  # ä¸èª¿æ•´ LCL
        elif characteristic == 'Bigger':
            # Bigger åªéœ€è¦ tighten LCL (ä¸‹é™)
            adjust_ucl = False
            adj_u = 0.0  # ä¸èª¿æ•´ UCL
        
        print(f"    [Debug OOC] adjust_ucl: {adjust_ucl}, adjust_lcl: {adjust_lcl}")
            
        # è¨­å®šæœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼ˆå®‰å…¨ä¸Šé™ï¼Œé˜²æ­¢ç„¡é™å¾ªç’°ï¼‰
        max_iterations = 100
        
        initial_ooc_count = 0
        final_ooc_count = 0
        total_adj_units = 0
        
        # è¨˜éŒ„é€€æ ¼å‰çš„åˆå§‹å€¼ï¼ˆç”¨æ–¼è¨ˆç®—ç´¯ç©é€€æ ¼é‡ï¼‰
        initial_UCL = current_UCL
        initial_LCL = current_LCL

        for i in range(max_iterations):
            upper_ooc_mask = (values > current_UCL)
            lower_ooc_mask = (values < current_LCL)
            
            upper_ooc_count = np.sum(upper_ooc_mask)
            lower_ooc_count = np.sum(lower_ooc_mask)
            
            #String --- [é‚è¼¯ä¿®æ­£] æ ¹æ“šç‰¹æ€§åªè¨ˆç®—ã€Œæœ‰æ•ˆã€çš„ OOC ---
            # åŸå› ï¼šé¿å… Smaller ç‰¹æ€§æ™‚ï¼Œå› ç„¡æ³•èª¿æ•´ LCLï¼Œå°è‡´ä¸‹ç•Œ OOC è®“è¿´åœˆç„¡æ³•æ»¿è¶³åœæ­¢æ¢ä»¶è€Œç©ºè½‰ 100 æ¬¡
            if characteristic == 'Smaller':
                # æœ›å°ç‰¹æ€§ï¼šåªé—œæ³¨ Upper OOC (éå¤§)ï¼Œå¿½ç•¥ Lower OOC
                total_ooc_count = upper_ooc_count
                
                # Debug æç¤ºï¼šå¦‚æœæœ‰å¾ˆå¤š Lower OOC ä½†è¢«å¿½ç•¥
                if lower_ooc_count > 0:
                    print(f"    [Debug OOC] Smaller ç‰¹æ€§å¿½ç•¥ {lower_ooc_count} å€‹ Lower OOC")
                    
            elif characteristic == 'Bigger':
                # æœ›å¤§ç‰¹æ€§ï¼šåªé—œæ³¨ Lower OOC (éå°)ï¼Œå¿½ç•¥ Upper OOC
                total_ooc_count = lower_ooc_count
                
                # Debug æç¤º
                if upper_ooc_count > 0:
                    print(f"    [Debug OOC] Bigger ç‰¹æ€§å¿½ç•¥ {upper_ooc_count} å€‹ Upper OOC")
                    
            else:
                # Nominal (æœ›ç›®) æˆ–å…¶ä»–ï¼šä¸Šä¸‹ç•Œ OOC éƒ½ç®—
                total_ooc_count = upper_ooc_count + lower_ooc_count
            # ----------------------------------------------------
            
            if i == 0: 
                initial_ooc_count = total_ooc_count
                print(f"    [Debug OOC] åˆå§‹ OOC count: {initial_ooc_count} (upper: {upper_ooc_count}, lower: {lower_ooc_count})")
            
            ooc_percent = total_ooc_count / N
            
            print(f"    [Debug OOC] è¿­ä»£ {i+1}: UCL={current_UCL:.6f}, LCL={current_LCL:.6f}, OOC={total_ooc_count} ({ooc_percent*100:.2f}%)")
            
            # åœæ­¢æ¢ä»¶ 1ï¼šOOC% â‰¤ 0.3% æˆ– OOC é»æ•¸ < 2
            if ooc_percent <= 0.003 or total_ooc_count < 2: 
                final_ooc_count = total_ooc_count
                print(f"    [Debug OOC] è¿­ä»£ {i+1}: é”åˆ°åœæ­¢æ¢ä»¶ (OOCâ‰¤0.3% æˆ– <2é»)ï¼Œåœæ­¢èª¿æ•´")
                break
            
            # åŸ·è¡Œé€€æ ¼å‰ï¼Œæª¢æŸ¥ç´¯ç©é€€æ ¼é‡æ˜¯å¦æœƒè¶…é Â±2Ïƒ é™åˆ¶
            should_stop = False
            
            if upper_ooc_count > 0 and adj_u > 0 and adjust_ucl:
                # è¨ˆç®—é€€æ ¼å¾Œçš„ç´¯ç©é‡
                cumulative_adj_u = (current_UCL + adj_u) - initial_UCL
                
                # æª¢æŸ¥æ˜¯å¦è¶…é +2Ïƒ
                if cumulative_adj_u > 2 * sigma_est_u:
                    print(f"    [Debug] è¿­ä»£ {i+1}: UCL ç´¯ç©é€€æ ¼ {cumulative_adj_u:.4f} è¶…é +2Ïƒ ({2*sigma_est_u:.4f})ï¼Œåœæ­¢èª¿æ•´")
                    should_stop = True
                else:
                    current_UCL += adj_u
                    if pattern in ["Near Constant", "Attribute"]:
                        total_adj_units += 1
                    else:
                        total_adj_units += adj_u / sigma_u
            
            if lower_ooc_count > 0 and adj_l > 0 and adjust_lcl:
                # è¨ˆç®—é€€æ ¼å¾Œçš„ç´¯ç©é‡
                cumulative_adj_l = initial_LCL - (current_LCL - adj_l)
                
                # æª¢æŸ¥æ˜¯å¦è¶…é -2Ïƒ
                if cumulative_adj_l > 2 * sigma_est_l:
                    print(f"    [Debug] è¿­ä»£ {i+1}: LCL ç´¯ç©é€€æ ¼ {cumulative_adj_l:.4f} è¶…é -2Ïƒ ({2*sigma_est_l:.4f})ï¼Œåœæ­¢èª¿æ•´")
                    should_stop = True
                else:
                    current_LCL -= adj_l
                    if pattern in ["Near Constant", "Attribute"]:
                        total_adj_units += 1
                    else:
                        total_adj_units += adj_l / sigma_l
            
            # åœæ­¢æ¢ä»¶ 2ï¼šé”åˆ° Â±2Ïƒ ä¸Šé™
            if should_stop:
                final_ooc_count = total_ooc_count
                break
            
            # ğŸŒ€ [Bug Fix] åœæ­¢æ¢ä»¶ 3ï¼šæ²’æœ‰ã€Œæœ‰æ•ˆçš„ã€OOC äº†ï¼ˆä¿®æ­£å–®é‚Šè¦æ ¼æ­»é‚è¼¯ï¼‰
            # åŸé‚è¼¯ï¼šif upper_ooc_count == 0 and lower_ooc_count == 0 åœ¨å–®é‚Šè¦æ ¼ä¸‹æ°¸é ç„¡æ³•æ»¿è¶³
            # æ–°é‚è¼¯ï¼šæª¢æŸ¥ total_ooc_countï¼ˆå·²æ ¹æ“šç‰¹æ€§éæ¿¾éï¼‰
            if total_ooc_count == 0:
                final_ooc_count = 0
                print(f"    [Debug OOC] è¿­ä»£ {i+1}: æœ‰æ•ˆ OOC = 0ï¼Œåœæ­¢èª¿æ•´")
                break
        
        # æœ€çµ‚ç¡¬æ€§é™åˆ¶ï¼šç¢ºä¿ Suggest CL ä¸è¶…é Static CL Â± 2Ïƒ
        max_ucl_allowed = initial_UCL + 2 * sigma_est_u
        min_lcl_allowed = initial_LCL - 2 * sigma_est_l
        
        if current_UCL > max_ucl_allowed:
            print(f"    [Warning] UCL è¶…é Static+2Ïƒ ä¸Šé™ ({current_UCL:.4f} > {max_ucl_allowed:.4f})ï¼Œå¼·åˆ¶é™åˆ¶")
            current_UCL = max_ucl_allowed
        
        if current_LCL < min_lcl_allowed:
            print(f"    [Warning] LCL è¶…é Static-2Ïƒ ä¸‹é™ ({current_LCL:.4f} < {min_lcl_allowed:.4f})ï¼Œå¼·åˆ¶é™åˆ¶")
            current_LCL = min_lcl_allowed

        # SOP 6.3: æœ€çµ‚ CL æ ¹æ“š resolution ä¿®æ­£
        if resolution is not None and not np.isnan(current_UCL) and not np.isnan(current_LCL):
            # âœ… ä½¿ç”¨æ–°çš„æ–¹æ³•ç²¾ç¢ºè¨ˆç®— decimalsï¼ˆé¿å… rstrip('0') çš„å•é¡Œï¼‰
            decimals = self.calculate_decimals_from_resolution(resolution)
            
            if decimals >= 0:
                power_of_10 = 10**decimals
                print(f"    [DEBUG SOP 6.3] resolution={resolution}, decimals={decimals}, power_of_10={power_of_10}")
                print(f"    [DEBUG SOP 6.3] Before: UCL={current_UCL:.8f}, LCL={current_LCL:.8f}")
                
                # ğŸ›‘ [Bug Fix] åŠ å…¥ epsilon é¿å…æµ®é»ç²¾åº¦é™·é˜±
                # é˜²æ­¢ 10.4999...99 è¢« floor ç„¡æ¢ä»¶æ¨å»æˆ 10.4
                epsilon = 1e-9
                current_UCL = floor(current_UCL * power_of_10 + epsilon) / power_of_10
                current_LCL = ceil(current_LCL * power_of_10 - epsilon) / power_of_10
                
                # âœ… [é—œéµé–å®š] å†æ¬¡ round ä»¥å¾¹åº•æ¶ˆé™¤ Python æµ®é»æ•¸å¾®å¹…é›œè¨Š (å¦‚ 20.22000000001)
                current_UCL = round(float(current_UCL), decimals)
                current_LCL = round(float(current_LCL), decimals)
                
                print(f"    [DEBUG SOP 6.3] After: UCL={current_UCL:.8f}, LCL={current_LCL:.8f}")

            if current_LCL > current_UCL: 
                current_LCL = current_UCL
        
        print(f"    [Debug OOC] === OOC é€€æ ¼èª¿æ•´çµæŸ ===")
        print(f"    [Debug OOC] Raw UCL (é€€æ ¼å¾Œï¼Œcapping å‰): {current_UCL:.6f}, LCL: {current_LCL:.6f}")
        print(f"    [Debug OOC] Static OOC count: {initial_ooc_count}")
        print(f"    [Debug OOC] âš ï¸ Final OOC count å°‡åœ¨ capping å¾Œé‡æ–°è¨ˆç®—")
        print(f"    [Debug OOC] Total adjustment units: {total_adj_units:.4f}")

        # âš ï¸ æ³¨æ„ï¼šé€™è£¡è¿”å›çš„ final_ooc_count æ˜¯æš«æ™‚å€¼ï¼Œæœƒåœ¨ capping å¾Œé‡æ–°è¨ˆç®—
        final_ooc_count = 0  # æš«æ™‚è¨­ç‚º 0ï¼Œå¯¦éš›å€¼å°‡åœ¨ä¸»å‡½æ•¸ä¸­é‡æ–°è¨ˆç®—
        return current_UCL, current_LCL, initial_ooc_count, final_ooc_count, total_adj_units

    def check_tighten(self, original_tol, new_tol, data_count):
        """SOP 6.4: åˆ¤æ–·æ˜¯å¦éœ€è¦ tighten (å®¹å·®æ¯”å°)"""
        tighten_flag, _, _ = self.check_tighten_with_details(original_tol, new_tol, data_count)
        return tighten_flag
    
    def check_tighten_with_details(self, original_tol, new_tol, data_count):
        """SOP 6.4: åˆ¤æ–·æ˜¯å¦éœ€è¦ tighten (å®¹å·®æ¯”å°) - è¿”å›è©³ç´°è³‡è¨Š"""
        N_pct_table = [(125,15),(70,18),(45,20),(30,25),(15,30),(10,35),(0,40)]
        N_pct = 40 
        for n_threshold, pct in N_pct_table:
            if data_count > n_threshold:
                N_pct = pct
                break
        
        if original_tol <= 0: 
            return False, np.nan, N_pct
        
        # è¨ˆç®—è®ŠåŒ–ç‡ï¼ˆæ­£å€¼è¡¨ç¤ºæ”¶ç·Šï¼Œè² å€¼è¡¨ç¤ºæ”¾å¯¬ï¼‰
        diff_ratio = (original_tol - new_tol) / original_tol * 100
        
        # é‚è¼¯ï¼šåªæœ‰ç•¶å®¹å·®æ”¶ç·Šï¼ˆnew_tol < original_tolï¼‰ä¸”è®ŠåŒ–ç‡ > N% æ™‚ï¼Œæ‰åˆ¤å®šç‚º TightenNeeded
        # å¦‚æœæ”¾å¯¬ï¼ˆnew_tol > original_tolï¼‰ï¼Œå‰‡ diff_ratio ç‚ºè² å€¼ï¼Œä¸æœƒè§¸ç™¼ tighten
        tighten_flag = diff_ratio > N_pct
        
        return tighten_flag, diff_ratio, N_pct

    # === æ ¸å¿ƒæµç¨‹åŒ…è£ ===
    
    def process_chart(self, df, value_col, date_col, oos_col, characteristic):
        """ä¸»é«” SOP æµç¨‹ (SOP 1-6)"""
        
        # 1. Data Integrity (SOP 1)
        values_orig, resolution = self.data_integrity(df.copy(), date_col, value_col, oos_col)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
        if len(values_orig) == 0:
            return {
                "Pattern": "No Valid Data",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": None,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "Raw UCL After Resolution": np.nan,
                "Raw LCL After Resolution": np.nan,
                "Sug USL": np.nan,
                "Sug LSL": np.nan,
                "TightenNeeded": False,
                "TotalDataCount": 0,
                "DataCountUsed": 0,
                "HardRule": "None",
                "DetectionLimit": np.nan,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Initial_OOC_Count": 0,
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # æª¢æŸ¥æ•¸æ“šé»æ˜¯å¦ < 4 å€‹
        if len(values_orig) < 4:
            print(f"    [Warning] æ•¸æ“šé»ä¸è¶³ ({len(values_orig)} < 4)ï¼Œè·³éè¨ˆç®—")
            
            # è®€å–å¿…è¦åƒæ•¸
            detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
            original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
            original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
            
            # è¨ˆç®— Ori OOC Count
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            return {
                "Pattern": "Insufficient Data",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "Raw UCL After Resolution": np.nan,
                "Raw LCL After Resolution": np.nan,
                "Sug USL": np.nan,
                "Sug LSL": np.nan,
                "TightenNeeded": False,
                "TotalDataCount": len(values_orig),
                "DataCountUsed": len(values_orig),
                "HardRule": "Insufficient Data",
                "DetectionLimit": detection_limit,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": 0,
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # 1.5. Hard Rule Check (å„ªå…ˆæª¢æŸ¥ï¼Œåœ¨ Pattern Diagnosis ä¹‹å‰)
        print(f"    [Debug] åŸå§‹æ•¸æ“šé»æ•¸é‡: {len(values_orig)}")
        print(f"    [Debug] åŸå§‹æ•¸æ“šç¯„åœ: {np.min(values_orig):.4f} ~ {np.max(values_orig):.4f}")
        
        UCL_hr, LCL_hr, rule_satisfied, rule_applied_name = self.apply_discrete_hard_rules(
            values_orig, resolution, len(values_orig))
        
        # è®€å–é¡å¤–åƒæ•¸
        detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
        original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
        original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
        target_val = df['Target'].iloc[0] if 'Target' in df.columns and len(df)>0 and pd.notna(df['Target'].iloc[0]) else np.nan

        if rule_satisfied:
            print(f"    [Debug] Hard Rule æ»¿è¶³: {rule_applied_name}")
            
            # æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šä½¿ç”¨ Hard Rule çš„å“ªäº›ç®¡åˆ¶ç·š
            if characteristic == 'Bigger':
                # Bigger: åªä½¿ç”¨ Hard Rule çš„ LCLï¼ŒUCL ä¿æŒåŸå§‹å€¼
                suggest_ucl_hr = original_ucl if not pd.isna(original_ucl) else UCL_hr
                suggest_lcl_hr = LCL_hr
                print(f"    [Debug] Bigger ç‰¹æ€§: åªèª¿æ•´ LCL = {LCL_hr:.4f}, UCL ä¿æŒåŸå§‹ = {suggest_ucl_hr:.4f}")
            elif characteristic == 'Smaller':
                # Smaller: åªä½¿ç”¨ Hard Rule çš„ UCLï¼ŒLCL ä¿æŒåŸå§‹å€¼
                suggest_ucl_hr = UCL_hr
                suggest_lcl_hr = original_lcl if not pd.isna(original_lcl) else LCL_hr
                print(f"    [Debug] Smaller ç‰¹æ€§: åªèª¿æ•´ UCL = {UCL_hr:.4f}, LCL ä¿æŒåŸå§‹ = {suggest_lcl_hr:.4f}")
            else:
                # Nominal: é›™é‚Šéƒ½ä½¿ç”¨ Hard Rule
                suggest_ucl_hr = UCL_hr
                suggest_lcl_hr = LCL_hr
                print(f"    [Debug] Nominal ç‰¹æ€§: é›™é‚Šéƒ½èª¿æ•´ UCL = {UCL_hr:.4f}, LCL = {LCL_hr:.4f}")
            
            # Hard Rule æˆç«‹æ™‚ï¼ŒCL_Center æ‡‰ç‚ºæœ€çµ‚ UCL å’Œ LCL çš„ä¸­é»
            cl_center_hr = (suggest_ucl_hr + suggest_lcl_hr) / 2
            
            # ========== ç°¡åŒ–ç‰ˆï¼šåªç‚ºäº† Tighten Check è¨ˆç®— Tolerance (ä½¿ç”¨éŒ¨é»é‚è¼¯) ==========
            # æ±ºå®šä¸€å€‹å›ºå®šçš„ã€ŒéŒ¨é» (Anchor)ã€
            # Hard Rule 1: ä½¿ç”¨ Target æˆ–é è¨­å€¼ï¼ˆå› ç‚ºæ˜¯å¸¸æ•¸ï¼‰
            # Hard Rule 2 å’Œ 3: ä½¿ç”¨ median
            if rule_applied_name == "Hard Rule 1: Constant/Near Constant":
                # Hard Rule 1 ä½¿ç”¨ Target æˆ–é è¨­å€¼
                if not pd.isna(target_val):
                    anchor = target_val
                else:
                    anchor = 0.0  # å°æ–¼ Smaller é è¨­ç‚º 0
                    # å°æ–¼ Bigger è‹¥ç„¡ Targetï¼Œä½¿ç”¨æ•¸æ“šæœ€å¤§å€¼
                    if characteristic == 'Bigger' and len(values_orig) > 0:
                        anchor = np.max(values_orig)
            else:
                # Hard Rule 2 å’Œ 3 ä½¿ç”¨ median
                anchor = np.median(values_orig)
            
            tighten_needed_hr = False
            original_tol = np.nan
            new_tol = np.nan
            diff_ratio = np.nan
            tighten_threshold = np.nan
            
            print(f"    [Debug] Hard Rule Tighten åˆ†æé–‹å§‹... (ç‰¹æ€§={characteristic}, éŒ¨é»={anchor:.4f})")
            
            # Hard Rule 1 (å¸¸æ•¸/è¿‘å¸¸æ•¸) çš„ç‰¹æ®Šè™•ç†ï¼šåªè¦ç®¡åˆ¶ç·šæ¯”åŸæœ¬ç·Šå°± tighten
            if rule_applied_name == "Hard Rule 1: Constant/Near Constant":
                print(f"    [Debug] Hard Rule 1: å¸¸æ•¸/è¿‘å¸¸æ•¸æ¨¡å¼")
                if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                    # æµ®é»æ•¸ç²¾åº¦ä¿®æ­£ï¼šround åˆ°åˆç†ä½æ•¸é¿å…å¾®å°èª¤å·®
                    decimals = 10  # ä¿ç•™ 10 ä½å°æ•¸
                    suggest_ucl_hr_rounded = round(suggest_ucl_hr, decimals)
                    suggest_lcl_hr_rounded = round(suggest_lcl_hr, decimals)
                    original_ucl_rounded = round(original_ucl, decimals)
                    original_lcl_rounded = round(original_lcl, decimals)
                    
                    # æª¢æŸ¥æ˜¯å¦æ¯”åŸæœ¬æ›´ç·Šï¼ˆUCL æ›´å°æˆ– LCL æ›´å¤§ï¼‰
                    is_tighter = False
                    if characteristic == 'Nominal':
                        is_tighter = (suggest_ucl_hr_rounded <= original_ucl_rounded) or (suggest_lcl_hr_rounded >= original_lcl_rounded)
                    elif characteristic == 'Smaller':
                        is_tighter = (suggest_ucl_hr_rounded <= original_ucl_rounded)
                    elif characteristic == 'Bigger':
                        is_tighter = (suggest_lcl_hr_rounded >= original_lcl_rounded)
                    
                    tighten_needed_hr = is_tighter
                    print(f"    [Debug] Hard Rule 1: Control Limit æ¯”è¼ƒçµæœ = {'æ›´ç·Š' if is_tighter else 'æœªæ›´ç·Š'}ï¼ŒTightenNeeded = {tighten_needed_hr}")
                    print(f"    [Debug] Hard Rule 1: UCL {suggest_ucl_hr_rounded:.10f} vs {original_ucl_rounded:.10f}, LCL {suggest_lcl_hr_rounded:.10f} vs {original_lcl_rounded:.10f}")
                else:
                    # ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼Œç›´æ¥ tighten
                    tighten_needed_hr = True
                    print(f"    [Debug] Hard Rule 1: ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼ŒTightenNeeded = Yes")
            
            # Hard Rule 2 å’Œ 3: éœ€è¦é€²è¡Œ tolerance åˆ¤æ–·
            elif rule_applied_name in ["Hard Rule 2: Two Categories", "Hard Rule 3: Three Categories Spaced by Resolution"]:
                print(f"    [Debug] {rule_applied_name}: ä½¿ç”¨ Tolerance åˆ¤æ–·")
                
                if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                    # æ ¹æ“šç‰¹æ€§è¨ˆç®— Tolerance (å¯¬åº¦)
                    if characteristic == 'Nominal':
                        # é›™é‚Šï¼šç›´æ¥ç®—è·é›¢
                        original_tol = original_ucl - original_lcl
                        new_tol = suggest_ucl_hr - suggest_lcl_hr
                        print(f"    [Debug] Nominal ç‰¹æ€§: åŸå§‹ tolerance={original_tol:.6f}, æ–° tolerance={new_tol:.6f}")
                        
                    elif characteristic == 'Smaller':
                        # æœ›å°ï¼šåªçœ‹ UCL åˆ°éŒ¨é»çš„è·é›¢
                        original_tol = original_ucl - anchor
                        new_tol = suggest_ucl_hr - anchor
                        print(f"    [Debug] Smaller ç‰¹æ€§ (éŒ¨é»={anchor:.4f}): åŸå§‹ tolerance={original_tol:.6f}, æ–° tolerance={new_tol:.6f}")
                        
                    elif characteristic == 'Bigger':
                        # æœ›å¤§ï¼šåªçœ‹ éŒ¨é» åˆ° LCL çš„è·é›¢
                        original_tol = anchor - original_lcl
                        new_tol = anchor - suggest_lcl_hr
                        print(f"    [Debug] Bigger ç‰¹æ€§ (éŒ¨é»={anchor:.4f}): åŸå§‹ tolerance={original_tol:.6f}, æ–° tolerance={new_tol:.6f}")
                        
                    else:
                        # é è¨­ Nominal
                        original_tol = original_ucl - original_lcl
                        new_tol = suggest_ucl_hr - suggest_lcl_hr
                        print(f"    [Debug] æœªçŸ¥ç‰¹æ€§ï¼Œé è¨­ Nominal: åŸå§‹ tolerance={original_tol:.6f}, æ–° tolerance={new_tol:.6f}")
                    
                    # ä½¿ç”¨ SOP 6.4 é‚è¼¯é€²è¡Œ Tighten æª¢æŸ¥ï¼ˆå®¹å·®æ¯”å°ï¼‰
                    if original_tol > 1e-9 and new_tol > 1e-9:
                        tighten_needed_hr, diff_ratio, tighten_threshold = self.check_tighten_with_details(
                            original_tol, new_tol, len(values_orig)
                        )
                        
                        print(f"    [Debug] Hard Rule Tolerance æ¯”å°çµæœ:")
                        print(f"    [Debug]   åŸå§‹ tolerance: {original_tol:.6f}")
                        print(f"    [Debug]   æ–° tolerance: {new_tol:.6f}")
                        print(f"    [Debug]   è®ŠåŒ–ç‡: {diff_ratio:.2f}%")
                        print(f"    [Debug]   Tighten é–¾å€¼ (N%): {tighten_threshold:.0f}%")
                        print(f"    [Debug]   TightenNeeded: {'Yes' if tighten_needed_hr else 'No'}")
                        
                        if tighten_needed_hr:
                            print(f"    [Debug] âœ“ Hard Rule: Tolerance æ”¶ç·Š {diff_ratio:.2f}% > {tighten_threshold:.0f}% é–¾å€¼ï¼ŒTightenNeeded = Yes")
                        else:
                            print(f"    [Debug] âœ— Hard Rule: Tolerance è®ŠåŒ– {diff_ratio:.2f}% â‰¤ {tighten_threshold:.0f}% é–¾å€¼ï¼ŒTightenNeeded = No")
                    else:
                        print(f"    [Debug] Hard Rule: Tolerance ç„¡æ•ˆ (original_tol={original_tol:.6f}, new_tol={new_tol:.6f})")
                else:
                    # æ²’æœ‰åŸå§‹ç®¡åˆ¶ç·šæ™‚ï¼ŒHard Rule è§¸ç™¼å³ç‚ºéœ€è¦ tighten
                    tighten_needed_hr = True
                    print(f"    [Debug] Hard Rule: ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼ŒTightenNeeded = Yes")
            else:
                # å…¶ä»–æœªçŸ¥çš„ Hard Ruleï¼ˆä¿éšªèµ·è¦‹ï¼‰
                if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                    tighten_needed_hr = False
                    print(f"    [Debug] æœªçŸ¥ Hard Rule é¡å‹ï¼Œé è¨­ TightenNeeded = No")
                else:
                    tighten_needed_hr = True
                    print(f"    [Debug] æœªçŸ¥ Hard Rule é¡å‹ä½†ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼ŒTightenNeeded = Yes")
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            # è¨ˆç®— Static/Final OOC Count (ä½¿ç”¨ Hard Rule çš„ç®¡åˆ¶ç·š)
            static_ooc_count = 0
            static_upper_ooc = np.sum(values_orig > suggest_ucl_hr)
            static_lower_ooc = np.sum(values_orig < suggest_lcl_hr)
            static_ooc_count = static_upper_ooc + static_lower_ooc
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            # è¨ˆç®— Static/Final OOC Count (ä½¿ç”¨ Hard Rule çš„ç®¡åˆ¶ç·š)
            static_ooc_count = 0
            static_upper_ooc = np.sum(values_orig > suggest_ucl_hr)
            static_lower_ooc = np.sum(values_orig < suggest_lcl_hr)
            static_ooc_count = static_upper_ooc + static_lower_ooc
            
            # ğŸ“ [Bug Fix] Hard Rule èˆ‡ SOP 6.6 çš„é‚è¼¯è¡çªæª¢æŸ¥
            # ç¢ºä¿ Hard Rule ä¹Ÿéµå®ˆã€Œåªèƒ½æ”¶ç·Šã€ä¸èƒ½æ”¾å¯¬ã€åŸå‰‡
            clamped = False  # è¿½è¹¤æ˜¯å¦ç™¼ç”Ÿ clamp
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                # æª¢æŸ¥æ˜¯å¦é•å Tighten-only åŸå‰‡ï¼ˆä½¿ç”¨ round é¿å…æµ®é»æ•¸ç²¾åº¦å•é¡Œï¼‰
                decimals = 10
                suggest_ucl_hr_rounded = round(suggest_ucl_hr, decimals)
                suggest_lcl_hr_rounded = round(suggest_lcl_hr, decimals)
                original_ucl_rounded = round(original_ucl, decimals)
                original_lcl_rounded = round(original_lcl, decimals)
                
                if characteristic == 'Nominal' or characteristic == 'Smaller':
                    if suggest_ucl_hr_rounded > original_ucl_rounded:
                        print(f"    [Warning] Hard Rule UCL ({suggest_ucl_hr:.10f}) > Original UCL ({original_ucl:.10f})ï¼Œå¼·åˆ¶ clamp è‡³åŸå§‹å€¼")
                        suggest_ucl_hr = original_ucl
                        clamped = True
                
                if characteristic == 'Nominal' or characteristic == 'Bigger':
                    if suggest_lcl_hr_rounded < original_lcl_rounded:
                        print(f"    [Warning] Hard Rule LCL ({suggest_lcl_hr:.10f}) < Original LCL ({original_lcl:.10f})ï¼Œå¼·åˆ¶ clamp è‡³åŸå§‹å€¼")
                        suggest_lcl_hr = original_lcl
                        clamped = True
                
                # âš ï¸ é—œéµï¼šå¦‚æœç™¼ç”Ÿäº† clampï¼ˆç•Œé™è®Šå¯¬ï¼‰ï¼Œå¼·åˆ¶ tighten_needed = False
                if clamped:
                    tighten_needed_hr = False
                    print(f"    [Warning] æª¢æ¸¬åˆ°ç®¡åˆ¶ç·šè®Šå¯¬ï¼Œå¼·åˆ¶è¨­å®š TightenNeeded = False")
                
                # é‡æ–°è¨ˆç®— cl_center å’Œ toleranceï¼ˆå› å¯èƒ½è¢« clamp éï¼‰
                cl_center_hr = (suggest_ucl_hr + suggest_lcl_hr) / 2
                
                if characteristic == 'Nominal':
                    new_tol = suggest_ucl_hr - suggest_lcl_hr
                elif characteristic == 'Smaller':
                    new_tol = suggest_ucl_hr - anchor
                elif characteristic == 'Bigger':
                    new_tol = anchor - suggest_lcl_hr
            
            # âš ï¸ Hard Rule è¿”å›å‰ï¼šç‰¹åˆ¥èªªæ˜ä¸å¥—ç”¨ Resolution ç²¾åº¦ä¿®æ­£
            print(f"\n    [Hard Rule Return] Pattern: {rule_applied_name}")
            print(f"    [Hard Rule Return] âš ï¸ Hard Rule ä¸å¥—ç”¨ Resolution ç²¾åº¦ä¿®æ­£ï¼ˆé¿å…é€ æˆ OOCï¼‰")
            print(f"    [Hard Rule Return] Suggest UCL: {suggest_ucl_hr:.10f}")
            print(f"    [Hard Rule Return] Suggest LCL: {suggest_lcl_hr:.10f}\n")
            
            # Hard Rule æ»¿è¶³æ™‚ï¼Œè¿”å›çµæœ
            return {
                "Pattern": rule_applied_name,  # ç›´æ¥é¡¯ç¤ºå…·é«”çš„ Hard Rule
                "Skew": np.nan,  # Hard Rule ä¸éœ€è¦ Skew/CB
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": suggest_ucl_hr,  # âš ï¸ Hard Ruleï¼šä¿æŒåŸå§‹ç²¾åº¦ï¼Œä¸å¥—ç”¨ Resolution
                "Suggest LCL": suggest_lcl_hr,  # âš ï¸ Hard Ruleï¼šä¿æŒåŸå§‹ç²¾åº¦ï¼Œä¸å¥—ç”¨ Resolution
                "Static UCL": suggest_ucl_hr,
                "Static LCL": suggest_lcl_hr, 
                "TightenNeeded": tighten_needed_hr,  # åŸºæ–¼ SOP 6.4 Tolerance æ¯”å°åˆ¤å®š
                "TotalDataCount": len(values_orig),
                "DataCountUsed": len(values_orig),
                "HardRule": rule_applied_name,
                "DetectionLimit": detection_limit,
                "CL_Center": cl_center_hr,
                "Sigma_Est": 0.0, 
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,  # Hard Rule ç„¡sigmaæ¦‚å¿µ
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": static_ooc_count,
                "Final_OOC_Count": static_ooc_count,  # Hard Rule æ²’æœ‰é€€æ ¼ï¼ŒStatic = Final
                "Original_Tolerance": original_tol,  # âœ… æ–°å¢ï¼šåŸå§‹ tolerance
                "New_Tolerance": new_tol,  # âœ… æ–°å¢ï¼šæ–° tolerance
                "Diff_Ratio_%": diff_ratio,  # âœ… æ–°å¢ï¼šè®ŠåŒ–ç‡ç™¾åˆ†æ¯”
                "Tighten_Threshold_%": tighten_threshold  # âœ… æ–°å¢ï¼šTighten é–¾å€¼
            }
        
        print(f"    [Debug] Hard Rule ä¸æ»¿è¶³ï¼Œç¹¼çºŒé€²è¡Œ Pattern Diagnosis")
        
        # 2-4. Pattern Diagnosis & Outlier Filter
        print(f"    [Debug] åŸå§‹æ•¸æ“šé»æ•¸é‡: {len(values_orig)}")
        print(f"    [Debug] åŸå§‹æ•¸æ“šç¯„åœ: {np.min(values_orig):.4f} ~ {np.max(values_orig):.4f}")
        
        values_for_pattern = self.data_prep_for_pattern(values_orig)
        print(f"    [Debug] é è™•ç†å¾Œæ•¸æ“šé»æ•¸é‡: {len(values_for_pattern)}")
        
        pattern, skew_value, cb_value = self.pattern_diagnosis(values_for_pattern, resolution)
        print(f"    [Debug] è¨ºæ–·å‡ºçš„æ¨¡å¼: {pattern}, Skew: {skew_value:.4f}, CB: {cb_value:.4f}")
        
        values_filtered = self.outlier_filter(values_orig, pattern)
        N_filtered = len(values_filtered)
        print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šé»æ•¸é‡: {N_filtered}")
        if N_filtered > 0:
            print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šç¯„åœ: {np.min(values_filtered):.4f} ~ {np.max(values_filtered):.4f}")
        else:
            print(f"    [Debug] éæ¿¾å¾Œæ•¸æ“šç‚ºç©º!")
        
        # æª¢æŸ¥éæ¿¾å¾Œçš„æ•¸æ“šæ˜¯å¦ç‚ºç©º
        if N_filtered == 0:
            print(f"    [Warning] éæ¿¾å¾Œæ²’æœ‰æœ‰æ•ˆæ•¸æ“šé»ï¼Œè·³éè¨ˆç®—")
            
            # è®€å–å¿…è¦åƒæ•¸
            detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
            original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
            original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
            
            # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹æ•¸æ“šï¼Œå³ä½¿éæ¿¾å¾Œç‚ºç©º)
            ori_ooc_count = 0
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                ori_upper_ooc = np.sum(values_orig > original_ucl)
                ori_lower_ooc = np.sum(values_orig < original_lcl)
                ori_ooc_count = ori_upper_ooc + ori_lower_ooc
            
            return {
                "Pattern": "No Data After Filter",
                "Skew": np.nan,
                "CB": np.nan,
                "Resolution_Estimated": resolution,
                "Suggest UCL": np.nan,
                "Suggest LCL": np.nan,
                "Static UCL": np.nan,
                "Static LCL": np.nan,
                "Raw UCL After Resolution": np.nan,
                "Raw LCL After Resolution": np.nan,
                "Sug USL": np.nan,
                "Sug LSL": np.nan,
                "TightenNeeded": False,
                "TotalDataCount": len(values_orig),
                "DataCountUsed": N_filtered,
                "HardRule": "No Data After Filter",
                "DetectionLimit": detection_limit,
                "CL_Center": np.nan,
                "Sigma_Est": 0.0,
                "Sigma_Est_Upper": 0.0,
                "Sigma_Est_Lower": 0.0,
                "Original_UCL_K_Set": np.nan,
                "Original_LCL_K_Set": np.nan,
                "Suggest_UCL_K_Set": np.nan,
                "Suggest_LCL_K_Set": np.nan,
                "Ori_K_Set": np.nan,
                "Sug_K_Set": np.nan,
                "Total_Adj_Units": 0.0,
                "Ori_OOC_Count": ori_ooc_count,
                "Static_OOC_Count": 0,  # éæ¿¾å¾Œç„¡æ•¸æ“šï¼Œç„¡æ³•è¨ˆç®— Static
                "Final_OOC_Count": 0,
                "Original_Tolerance": np.nan,
                "New_Tolerance": np.nan,
                "Diff_Ratio_%": np.nan,
                "Tighten_Threshold_%": np.nan
            }
        
        # è®€å–é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å¾ŒçºŒè¨ˆç®—ï¼‰
        detection_limit = df['DetectionLimit'].iloc[0] if 'DetectionLimit' in df.columns and len(df)>0 else np.nan
        original_ucl = df['UCL'].iloc[0] if 'UCL' in df.columns and len(df)>0 else np.nan
        original_lcl = df['LCL'].iloc[0] if 'LCL' in df.columns and len(df)>0 else np.nan
        target_val = df['Target'].iloc[0] if 'Target' in df.columns and len(df)>0 and pd.notna(df['Target'].iloc[0]) else np.nan

        # ğŸ”¥ è¨ˆç®— Kurtosisï¼ˆç”¨æ–¼åˆ¤æ–·æ˜¯å¦éœ€è¦åŠ å€ sigmaï¼‰
        kurtosis_value = kurtosis(values_filtered, fisher=True, bias=False)  # è½‰æ›ç‚ºé Fisher å½¢å¼
        print(f"    [Debug] Kurtosis: {kurtosis_value:.4f}")

        # 5. Statistical Model Fitting (SOP 5)
        UCL_static, LCL_static, UR_robust, LR_robust, UCL3_ecdf, LCL3_ecdf, Sug_USL, Sug_LSL = self.calc_CL(
            values_filtered, pattern, resolution, characteristic, kurtosis_value
        )
        
        # æ‡‰ç”¨ resolution èª¿æ•´åˆ° Sug USL/LSL
        if resolution is not None and resolution > 0:
            Sug_USL = self.apply_resolution_precision(Sug_USL, resolution, "Sug_USL")
            Sug_LSL = self.apply_resolution_precision(Sug_LSL, resolution, "Sug_LSL")
        
        # æ±ºå®š CL Center å’Œ Sigma Est
        cl_center = np.nan
        sigma_est_u = np.nan
        sigma_est_l = np.nan
        
        if pattern == "Constant":
            cl_center = np.median(values_filtered)
            sigma_est_u = sigma_est_l = 0.0
        elif pattern == "Near Constant":
            # Near Constant æ”¹ç”¨ ECDF è¨ˆç®— sigmaï¼ˆèˆ‡ Attribute ç›¸åŒé‚è¼¯ï¼‰
            cl_center = np.median(values_filtered)
            T_upper_ecdf = UCL3_ecdf - cl_center
            T_lower_ecdf = cl_center - LCL3_ecdf
            if T_upper_ecdf > 1e-9 and T_lower_ecdf > 1e-9:
                sigma_est_u = T_upper_ecdf / 3.0
                sigma_est_l = T_lower_ecdf / 3.0
            else:
                sigma_est_u = UR_robust if UR_robust > 0 else 0.0
                sigma_est_l = LR_robust if LR_robust > 0 else 0.0
        elif pattern == "Normal":
            cl_center = np.mean(values_filtered)
            sigma_est_u = sigma_est_l = np.std(values_filtered, ddof=1)
        elif pattern in ["Skew-Right", "Skew-Left"]:
            cl_center = np.median(values_filtered)
            sigma_est_u = UR_robust
            sigma_est_l = LR_robust
        else: # ECDF ç›¸é—œæ¨¡å¼ (Bimodal, Attribute)
            cl_center = np.median(values_filtered)
            T_upper_ecdf = UCL3_ecdf - cl_center
            T_lower_ecdf = cl_center - LCL3_ecdf
            if T_upper_ecdf > 1e-9 and T_lower_ecdf > 1e-9:
                sigma_est_u = T_upper_ecdf / 3.0
                sigma_est_l = T_lower_ecdf / 3.0
            else:
                sigma_est_u = UR_robust if UR_robust > 0 else 0.0
                sigma_est_l = LR_robust if LR_robust > 0 else 0.0

        sigma_est_output = max(sigma_est_u, sigma_est_l)
        if not np.isfinite(sigma_est_output): 
            sigma_est_output = 0.0 

        # è¨ˆç®— Ori OOC Count (ä½¿ç”¨åŸå§‹ç®¡åˆ¶ç·š)
        ori_ooc_count = 0
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            ori_upper_ooc = np.sum(values_orig > original_ucl)
            ori_lower_ooc = np.sum(values_orig < original_lcl)
            ori_ooc_count = ori_upper_ooc + ori_lower_ooc

        # 6. Control Limit Adjustment (SOP 6.1/6.2/6.3/6.5)
        # ä½¿ç”¨åŸå§‹æ•¸æ“š (values_orig) ä¾†è¨ˆç®— OOC countï¼Œè€Œä¸æ˜¯éæ¿¾å¾Œçš„æ•¸æ“š
        UCL_suggest, LCL_suggest, static_ooc_count, final_ooc_count, total_adj_units = self.adjust_CL_based_on_OOC(
            values_orig, UCL_static, LCL_static, pattern, resolution, sigma_est_u, sigma_est_l, 2, characteristic
        )
        
        # === ä¿å­˜ Raw UCL/LCLï¼ˆå·²ç¶“é OOC é€€æ ¼ + resolution èª¿æ•´ï¼Œæœªç¶“é cappingï¼‰===
        # é€™äº›å€¼å¯ä»¥è¶…å‡ºåŸå§‹ç®¡åˆ¶ç·šï¼Œåæ˜ çµ±è¨ˆä¸Šçš„çœŸå¯¦å»ºè­°ï¼ˆåŒ…æ‹¬æ”¾å¯¬ï¼‰
        Raw_UCL_After_Resolution = UCL_suggest
        Raw_LCL_After_Resolution = LCL_suggest

        # 6.5. Detection Limit Rule (åƒ…é©ç”¨æ–¼ Smaller ç‰¹æ€§)
        if characteristic == 'Smaller' and not np.isnan(detection_limit):
            if UCL_suggest < detection_limit:
                UCL_suggest = detection_limit

        # 6.6. ç®¡åˆ¶ç·šå¯¬åº¦ç´„æŸ - ä¸å…è¨±å»ºè­°ç®¡åˆ¶ç·šæ¯”åŸå§‹ç®¡åˆ¶ç·šæ›´å¯¬
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            # Static UCL/LCL ç´„æŸï¼šä¸è¶…å‡ºåŸå§‹ç®¡åˆ¶ç·š
            if not np.isnan(UCL_static) and UCL_static > original_ucl:
                UCL_static = original_ucl
            if not np.isnan(LCL_static) and LCL_static < original_lcl:
                LCL_static = original_lcl
                
            # Suggest UCL/LCL ç´„æŸï¼šä¸è¶…å‡ºåŸå§‹ç®¡åˆ¶ç·š  
            if not np.isnan(UCL_suggest) and UCL_suggest > original_ucl:
                UCL_suggest = original_ucl
            if not np.isnan(LCL_suggest) and LCL_suggest < original_lcl:
                LCL_suggest = original_lcl
        
        # 6.6b. æ ¹æ“šç‰¹æ€§é¡å‹è¨­å®š Static UCL/LCL ç‚ºåŸå§‹å€¼
        if characteristic == 'Smaller':
            # Smaller ç‰¹æ€§ï¼šStatic LCL = åŸå§‹ LCL (åªéœ€è¦ USL)
            if not pd.isna(original_lcl):
                LCL_static = original_lcl
        elif characteristic == 'Bigger':
            # Bigger ç‰¹æ€§ï¼šStatic UCL = åŸå§‹ UCL (åªéœ€è¦ LSL)
            if not pd.isna(original_ucl):
                UCL_static = original_ucl
        
        # 6.7a. Attribute/Near Constant ç‰¹æ®Šé‚è¼¯ï¼šé¿å… UCL/LCL å¡åœ¨ max/min
        if pattern in ["Attribute", "Near Constant"] and resolution is not None and resolution > 0 and len(values_orig) > 0:
            print(f"    [Debug] é€²å…¥ Attribute/Near Constant ç‰¹æ®Šé‚è¼¯")
            print(f"    [Debug] values_orig é•·åº¦: {len(values_orig)}")
            print(f"    [Debug] resolution: {resolution}")
            
            max_val = np.max(values_orig)
            min_val = np.min(values_orig)
            print(f"    [Debug] max_val: {max_val:.6f}, min_val: {min_val:.6f}")
            
            # æš«å­˜èª¿æ•´å‰çš„å€¼
            temp_UCL_suggest = UCL_suggest
            temp_LCL_suggest = LCL_suggest
            print(f"    [Debug] èª¿æ•´å‰ UCL_suggest: {UCL_suggest:.6f}, LCL_suggest: {LCL_suggest:.6f}")
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦èª¿æ•´ï¼ˆUCL <= max æˆ– LCL >= minï¼‰
            need_adjust_ucl = not np.isnan(UCL_suggest) and UCL_suggest <= max_val
            need_adjust_lcl = not np.isnan(LCL_suggest) and LCL_suggest >= min_val
            print(f"    [Debug] need_adjust_ucl: {need_adjust_ucl}, need_adjust_lcl: {need_adjust_lcl}")
            
            if need_adjust_ucl:
                temp_UCL_suggest = UCL_suggest + resolution
                print(f"    [Debug] UCL <= max ({UCL_suggest:.6f} <= {max_val:.6f})ï¼Œèª¿æ•´ç‚º {temp_UCL_suggest:.6f}")
            
            if need_adjust_lcl:
                temp_LCL_suggest = LCL_suggest - resolution
                print(f"    [Debug] LCL >= min ({LCL_suggest:.6f} >= {min_val:.6f})ï¼Œèª¿æ•´ç‚º {temp_LCL_suggest:.6f}")
            
            # æª¢æŸ¥èª¿æ•´å¾Œæ˜¯å¦æ¯”åŸå§‹ç®¡åˆ¶ç·šæ›´å¯¬é¬†
            if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
                # è¨ˆç®—åŸå§‹å®¹å·®
                if characteristic == 'Nominal':
                    ori_tolerance = original_ucl - original_lcl
                    new_tolerance = temp_UCL_suggest - temp_LCL_suggest
                elif characteristic == 'Smaller':
                    ori_tolerance = original_ucl - cl_center
                    new_tolerance = temp_UCL_suggest - cl_center
                elif characteristic == 'Bigger':
                    ori_tolerance = cl_center - original_lcl
                    new_tolerance = cl_center - temp_LCL_suggest
                else:
                    ori_tolerance = original_ucl - original_lcl
                    new_tolerance = temp_UCL_suggest - temp_LCL_suggest
                
                # å¦‚æœèª¿æ•´å¾Œæ›´å¯¬é¬†ï¼Œä¸é€²è¡Œèª¿æ•´ï¼ˆä¿æŒèª¿æ•´å‰çš„å€¼ï¼‰
                if new_tolerance > ori_tolerance:
                    print(f"    [Debug] èª¿æ•´å¾Œå®¹å·® ({new_tolerance:.4f}) æ¯”åŸå§‹å®¹å·® ({ori_tolerance:.4f}) æ›´å¯¬ï¼Œä¸é€²è¡Œèª¿æ•´")
                    # ä¸æ›´æ–° UCL_suggest å’Œ LCL_suggestï¼Œä¿æŒèª¿æ•´å‰çš„å€¼
                    # UCL_suggest å’Œ LCL_suggest ç¶­æŒåŸæœ¬åœ¨ç¬¬1558-1564è¡Œç´„æŸå¾Œçš„å€¼
                else:
                    print(f"    [Debug] èª¿æ•´å¾Œå®¹å·® ({new_tolerance:.4f}) æœªæ¯”åŸå§‹å®¹å·® ({ori_tolerance:.4f}) æ›´å¯¬ï¼Œæ¡ç”¨èª¿æ•´å€¼")
                    UCL_suggest = temp_UCL_suggest
                    LCL_suggest = temp_LCL_suggest
            else:
                # æ²’æœ‰åŸå§‹ç®¡åˆ¶ç·šæ™‚ï¼Œç›´æ¥æ¡ç”¨èª¿æ•´å€¼
                print(f"    [Debug] ç„¡åŸå§‹ç®¡åˆ¶ç·šï¼Œç›´æ¥æ¡ç”¨èª¿æ•´å€¼")
                UCL_suggest = temp_UCL_suggest
                LCL_suggest = temp_LCL_suggest
            
        if characteristic == 'Smaller':
            # Smaller åªå…è¨± UCL æ”¶ç·Šï¼ŒLCL ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ (åªéœ€è¦ USL)
            if not pd.isna(original_lcl):
                LCL_suggest = original_lcl
        elif characteristic == 'Bigger':
            # Bigger åªå…è¨± LCL æ”¶ç·Šï¼ŒUCL ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ (åªéœ€è¦ LSL)
            if not pd.isna(original_ucl):
                UCL_suggest = original_ucl

        # ğŸ”§ [ä¿®æ­£] åœ¨æ‰€æœ‰ capping/adjustment å®Œæˆå¾Œï¼Œä½¿ç”¨æœ€çµ‚çš„ Suggest UCL/LCL é‡æ–°è¨ˆç®— Final_OOC_Count
        print(f"    [Debug Final OOC] === é‡æ–°è¨ˆç®— Final_OOC_Count (åŸºæ–¼æœ€çµ‚ Suggest UCL/LCL) ===")
        print(f"    [Debug Final OOC] æœ€çµ‚ Suggest UCL: {UCL_suggest:.6f}, LCL: {LCL_suggest:.6f}")
        
        final_upper_ooc = np.sum(values_orig > UCL_suggest)
        final_lower_ooc = np.sum(values_orig < LCL_suggest)
        
        # æ ¹æ“šç‰¹æ€§è¨ˆç®—æœ€çµ‚æœ‰æ•ˆçš„ OOC
        if characteristic == 'Smaller':
            final_ooc_count = final_upper_ooc
            print(f"    [Debug Final OOC] Smaller ç‰¹æ€§ï¼šåªè¨ˆç®—ä¸Šç•Œ OOC = {final_ooc_count}")
        elif characteristic == 'Bigger':
            final_ooc_count = final_lower_ooc
            print(f"    [Debug Final OOC] Bigger ç‰¹æ€§ï¼šåªè¨ˆç®—ä¸‹ç•Œ OOC = {final_ooc_count}")
        else:
            final_ooc_count = final_upper_ooc + final_lower_ooc
            print(f"    [Debug Final OOC] Nominal ç‰¹æ€§ï¼šä¸Šç•Œ {final_upper_ooc} + ä¸‹ç•Œ {final_lower_ooc} = {final_ooc_count}")

        # 7. Tighten åˆ¤å®š (SOP 6.4) 
        tighten_flag = False
        diff_ratio = np.nan
        tighten_threshold = np.nan
        original_tol = np.nan
        new_tol = np.nan
        
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            center_val = cl_center 
            
            if characteristic == 'Nominal':
                # Nominal: tolerance = UCL â€“ LCL
                original_tol = original_ucl - original_lcl
                new_tol = UCL_suggest - LCL_suggest
            elif characteristic == 'Smaller':
                # Smaller: tolerance = UCL â€“ CL_Center
                original_tol = original_ucl - center_val
                new_tol = UCL_suggest - center_val
                
            elif characteristic == 'Bigger':
                # Bigger: tolerance = CL_Center â€“ LCL
                original_tol = center_val - original_lcl
                new_tol = center_val - LCL_suggest
            else:
                # é è¨­ç‚º Nominal
                original_tol = original_ucl - original_lcl
                new_tol = UCL_suggest - LCL_suggest

            # é€²è¡Œ Tighten æª¢æŸ¥ï¼Œä¸¦å–å¾—è©³ç´°è³‡è¨Š
            if original_tol > 1e-9 and new_tol > 1e-9:
                tighten_flag, diff_ratio, tighten_threshold = self.check_tighten_with_details(
                    original_tol, new_tol, N_filtered
                )
        
        # 8. è¨ˆç®—ç¾è¡Œå’Œå»ºè­°ç®¡åˆ¶ç·šçš„Kå€æ•¸
        # ç¾è¡Œç®¡åˆ¶ç·šçš„Kå€æ•¸
        original_ucl_k_set = np.nan
        original_lcl_k_set = np.nan
        suggest_ucl_k_set = np.nan
        suggest_lcl_k_set = np.nan
        
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)) and not pd.isna(cl_center):
            if sigma_est_u > 1e-9:
                original_ucl_k_set = (original_ucl - cl_center) / sigma_est_u
                suggest_ucl_k_set = (UCL_suggest - cl_center) / sigma_est_u
            if sigma_est_l > 1e-9:
                original_lcl_k_set = (cl_center - original_lcl) / sigma_est_l
                suggest_lcl_k_set = (cl_center - LCL_suggest) / sigma_est_l

        # è¨ˆç®—Ori_k_setå’ŒSug_k_setï¼ˆæ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šè¨ˆç®—æ–¹å¼ï¼‰
        ori_k_set = np.nan
        sug_k_set = np.nan
        
        if characteristic == 'Bigger':
            # Bigger ç‰¹æ€§ï¼šåªè€ƒæ…® LCL çš„ K å€¼
            ori_k_set = original_lcl_k_set
            sug_k_set = suggest_lcl_k_set
        elif characteristic == 'Smaller':
            # Smaller ç‰¹æ€§ï¼šåªè€ƒæ…® UCL çš„ K å€¼
            ori_k_set = original_ucl_k_set
            sug_k_set = suggest_ucl_k_set
        else:
    
            # Nominal ç‰¹æ€§ï¼ˆé è¨­ï¼‰ï¼šå– UCL å’Œ LCL çš„ K å€¼æœ€å¤§å€¼
            if not pd.isna(original_ucl_k_set) and not pd.isna(original_lcl_k_set):
                ori_k_set = max(original_ucl_k_set, original_lcl_k_set)
            elif not pd.isna(original_ucl_k_set):
                ori_k_set = original_ucl_k_set
            elif not pd.isna(original_lcl_k_set):
                ori_k_set = original_lcl_k_set
                
            if not pd.isna(suggest_ucl_k_set) and not pd.isna(suggest_lcl_k_set):
                sug_k_set = max(suggest_ucl_k_set, suggest_lcl_k_set)
            elif not pd.isna(suggest_ucl_k_set):
                sug_k_set = suggest_ucl_k_set
            elif not pd.isna(suggest_lcl_k_set):
                sug_k_set = suggest_lcl_k_set

        # ==========================================================
        # 10. æœ€çµ‚å®ˆé–€å“¡æª¢æŸ¥ (Final Gatekeeper)
        # ç¢ºä¿ Suggest æ°¸é ä¸æ¯” Original å¯¬ï¼Œä¸”ç‹€æ…‹ä¸€è‡´
        # ==========================================================
        
        if not (pd.isna(original_ucl) or pd.isna(original_lcl)):
            # å»ºç«‹å¾®é‡å®¹å·®é¿å…æµ®é»æ•¸èª¤å·® (ä¾‹å¦‚ 10.0000000001)
            eps = 1e-11
            modified = False

            # --- UCL æª¢æŸ¥ (é‡å° Nominal èˆ‡ Smaller) ---
            if characteristic in ['Nominal', 'Smaller']:
                if UCL_suggest > original_ucl + eps:
                    print(f"    [Final Check] è­¦å‘Šï¼šUCL_suggest ({UCL_suggest}) å¯¬æ–¼ Ori ({original_ucl})ï¼Œå¼·åˆ¶ç¸®å›ã€‚")
                    UCL_suggest = original_ucl
                    modified = True

            # --- LCL æª¢æŸ¥ (é‡å° Nominal èˆ‡ Bigger) ---
            if characteristic in ['Nominal', 'Bigger']:
                if LCL_suggest < original_lcl - eps:
                    print(f"    [Final Check] è­¦å‘Šï¼šLCL_suggest ({LCL_suggest}) å¯¬æ–¼ Ori ({original_lcl})ï¼Œå¼·åˆ¶ç¸®å›ã€‚")
                    LCL_suggest = original_lcl
                    modified = True

            # --- å¦‚æœæœ‰è¢«ç¸®å›ï¼Œé‡æ–°è¨ˆç®— OOC é»æ•¸ ---
            if modified:
                final_ooc_count = np.sum((values_orig > UCL_suggest + eps) | (values_orig < LCL_suggest - eps))
                
                # é‡æ–°è¨ˆç®— Tolerance æ¯”å°
                if characteristic == 'Nominal':
                    new_tol = UCL_suggest - LCL_suggest
                    original_tol = original_ucl - original_lcl
                elif characteristic == 'Smaller':
                    new_tol = UCL_suggest - cl_center
                    original_tol = original_ucl - cl_center
                else: # Bigger
                    new_tol = cl_center - LCL_suggest
                    original_tol = cl_center - original_lcl
                
                # é‡æ–°åˆ¤å®šæ˜¯å¦éœ€è¦ Tighten
                if original_tol > 1e-9:
                    tighten_flag, diff_ratio, tighten_threshold = self.check_tighten_with_details(
                        original_tol, new_tol, N_filtered
                    )
                else:
                    tighten_flag = False

        # 9. è¼¸å‡ºçµæœ
        
        # ğŸ”’ [é—œéµ] çµ±ä¸€ç²¾åº¦é–å®šï¼šæ‰€æœ‰ CL ç›¸é—œæ•¸å€¼éƒ½å¥—ç”¨ Resolution ä¿®æ­£
        print(f"\n    [Precision Lock] === é–‹å§‹ç²¾åº¦é–å®š ===")
        print(f"    [Precision Lock] Pattern: {pattern}")
        print(f"    [Precision Lock] Resolution: {resolution}")
        
        if resolution is not None and resolution > 0:
            decimals = self.calculate_decimals_from_resolution(resolution)
            print(f"    [Precision Lock] Target Decimals: {decimals}")
            
            # å¥—ç”¨ç²¾åº¦é–å®šåˆ°æ‰€æœ‰ CL ç›¸é—œæ•¸å€¼
            UCL_suggest = self.apply_resolution_precision(UCL_suggest, resolution, "UCL_suggest")
            LCL_suggest = self.apply_resolution_precision(LCL_suggest, resolution, "LCL_suggest")
            UCL_static = self.apply_resolution_precision(UCL_static, resolution, "UCL_static")
            LCL_static = self.apply_resolution_precision(LCL_static, resolution, "LCL_static")
            cl_center = self.apply_resolution_precision(cl_center, resolution, "cl_center")
        else:
            print(f"    [Precision Lock] è·³éï¼ˆResolution ç„¡æ•ˆï¼‰")
        
        print(f"    [Precision Lock] === ç²¾åº¦é–å®šå®Œæˆ ===\n")
        
        return {
            "Pattern": pattern,
            "Skew": skew_value,
            "CB": cb_value,
            "Resolution_Estimated": resolution,
            "Suggest UCL": UCL_suggest,
            "Suggest LCL": LCL_suggest,
            "Static UCL": UCL_static,
            "Static LCL": LCL_static,
            "Raw UCL After Resolution": Raw_UCL_After_Resolution,
            "Raw LCL After Resolution": Raw_LCL_After_Resolution,
            "Sug USL": Sug_USL,
            "Sug LSL": Sug_LSL,
            "TightenNeeded": tighten_flag,
            "TotalDataCount": len(values_orig),
            "DataCountUsed": N_filtered,
            "HardRule": "None",
            "DetectionLimit": detection_limit,
            "CL_Center": cl_center,
            "Sigma_Est": sigma_est_output,
            "Sigma_Est_Upper": sigma_est_u,
            "Sigma_Est_Lower": sigma_est_l,
            "Original_UCL_K_Set": original_ucl_k_set,
            "Original_LCL_K_Set": original_lcl_k_set,
            "Suggest_UCL_K_Set": suggest_ucl_k_set,
            "Suggest_LCL_K_Set": suggest_lcl_k_set,
            "Ori_K_Set": ori_k_set,
            "Sug_K_Set": sug_k_set,
            "Total_Adj_Units": total_adj_units,
            "Ori_OOC_Count": ori_ooc_count,
            "Static_OOC_Count": static_ooc_count,
            "Final_OOC_Count": final_ooc_count,
            "Original_Tolerance": original_tol,
            "New_Tolerance": new_tol,
            "Diff_Ratio_%": diff_ratio,
            "Tighten_Threshold_%": tighten_threshold
        }

    # === æª”æ¡ˆ I/O èˆ‡æµç¨‹æ§åˆ¶ ===

    def load_chart_information(self, filepath):
        """è®€å– Chart è¨­å®šæª” (Excel)"""
        try:
            df_charts = pd.read_excel(filepath, sheet_name='Chart')
            
            # ç¢ºä¿ Resolution, DetectionLimit, tsmc_ucl, tsmc_lcl å­˜åœ¨
            if 'Resolution' not in df_charts.columns: 
                df_charts['Resolution'] = np.nan
            if 'DetectionLimit' not in df_charts.columns: 
                df_charts['DetectionLimit'] = np.nan
            if 'tsmc_ucl' not in df_charts.columns:
                df_charts['tsmc_ucl'] = np.nan
            if 'tsmc_lcl' not in df_charts.columns:
                df_charts['tsmc_lcl'] = np.nan
                
            # Target, UCL, LCL ç‚ºå¿…é ˆæ¬„ä½
            required_columns = ['Target', 'UCL', 'LCL']
            for col in required_columns:
                if col not in df_charts.columns:
                    raise ValueError(f"ç¼ºå°‘å¿…é ˆæ¬„ä½: '{col}'")
                    
            # æª¢æŸ¥å¿…é ˆæ¬„ä½æ˜¯å¦æœ‰æ•¸å€¼
            for col in required_columns:
                if df_charts[col].isna().any():
                    missing_rows = df_charts[df_charts[col].isna()].index.tolist()
                    raise ValueError(f"å¿…é ˆæ¬„ä½ '{col}' åœ¨ç¬¬ {missing_rows} è¡Œæœ‰ç¼ºå¤±å€¼")
                
            return df_charts
        except Exception as e:
            print(f"Error loading chart information from {filepath}: {e}")
            return pd.DataFrame()

    def find_matching_file(self, raw_data_directory, group_name, chart_name):
        """æ ¹æ“šå‘½åè¦å‰‡åŒ¹é…å°æ‡‰çš„ Raw Data CSV (ç”¨ _ åˆ†å‰²åšç²¾ç¢ºåŒ¹é…)"""
        group_name = str(group_name).strip()
        chart_name = str(chart_name).strip()
        
        # æ§‹å»ºç²¾ç¢ºåŒ¹é…çš„å‰ç¶´æ¨¡å¼
        pattern_prefix = f"{group_name}_{chart_name}"
        
        matching_files = []
        try:
            for filename in os.listdir(raw_data_directory):
                if not filename.endswith('.csv'):
                    continue
                
                # ç§»é™¤ .csv å‰¯æª”å
                filename_without_ext = filename[:-4]
                
                # æª¢æŸ¥æ˜¯å¦ä»¥ pattern_prefix é–‹é ­
                if filename_without_ext.startswith(pattern_prefix):
                    # æª¢æŸ¥å¾Œé¢æ˜¯å¦åªæœ‰ _ æˆ–æ²’æœ‰å…¶ä»–å­—ç¬¦ï¼ˆå…è¨±å°¾ç¢¼ï¼‰
                    remainder = filename_without_ext[len(pattern_prefix):]
                    if remainder == '' or remainder.startswith('_'):
                        matching_files.append(filename)
                        print(f"    [Debug] æ‰¾åˆ°åŒ¹é…æª”æ¡ˆ: {filename}")
        
        except Exception as e:
            print(f"    [Warning] æƒæç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
        
        if matching_files:
            # è‹¥æ‰¾åˆ°å¤šå€‹ç¬¦åˆçš„æª”æ¡ˆï¼Œå„ªå…ˆé¸æ“‡æœ€çŸ­çš„ï¼ˆæœ€æ¥è¿‘çš„åŒ¹é…ï¼‰
            matching_files.sort(key=len)
            selected_file = matching_files[0]
            
            if len(matching_files) > 1:
                print(f"    [Debug] æ‰¾åˆ°å¤šå€‹å€™é¸æª”æ¡ˆ: {matching_files}ï¼Œé¸æ“‡æœ€çŸ­çš„: {selected_file}")
            
            return os.path.join(raw_data_directory, selected_file)
        
        print(f"    [Warning] æœªæ‰¾åˆ°åŒ¹é…æª”æ¡ˆ (GroupName={group_name}, ChartName={chart_name})")
        print(f"    [Debug] æœŸæœ›çš„æª”æ¡ˆå‰ç¶´: {pattern_prefix}")
        return None

    def plot_control_chart(self, chart_data, chart_info, suggest_ucl, suggest_lcl,
                        static_ucl, static_lcl, cl_center, pattern, 
                        total_data_count=None, used_data_count=None,
                        output_dir='output_charts', max_x_labels=10,
                        tsmc_ucl=None, tsmc_lcl=None):
        """
        ç¹ªè£½ç®¡åˆ¶åœ– (SPC Chart)
        - å›ºå®šè¼¸å‡º 800x450 åƒç´ ï¼Œé«˜ DPI ä¿æŒæ¸…æ™°
        - åŒ…å«æ‰€æœ‰ç®¡åˆ¶ç·šã€å»ºè­°ç·šã€éœæ…‹ç·šèˆ‡æ¨™è¨»
        """
        import os
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates

        # ğŸ”¥ ä¿®å¾©å•é¡Œ2: æª¢æŸ¥ç©ºæ•¸æ“šï¼Œé¿å…é–ƒé€€
        if chart_data is None or len(chart_data) == 0:
            print(f"    [Warning] chart_data ç‚ºç©ºï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨")
            return None
        
        if 'value' not in chart_data.columns:
            print(f"    [Warning] chart_data ç¼ºå°‘ 'value' æ¬„ä½ï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨")
            return None
        
        # ç¢ºä¿æœ‰æœ‰æ•ˆæ•¸æ“šé»
        valid_data = chart_data['value'].dropna()
        if len(valid_data) == 0:
            print(f"    [Warning] chart_data æ²’æœ‰æœ‰æ•ˆçš„æ•¸æ“šé»ï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨")
            return None

        os.makedirs(output_dir, exist_ok=True)

        # ğŸ¯ å›ºå®šæœ€çµ‚è¼¸å‡ºå¤§å°ï¼š800x450 åƒç´ ï¼ˆä¸è®Šå¤§ï¼‰
        target_width_px, target_height_px = 900, 430
        dpi = 100  # é«˜ DPI ä¿æŒç·šæ¢å¹³æ»‘
        fig = plt.figure(figsize=(target_width_px / dpi, target_height_px / dpi), dpi=dpi)
        ax = fig.add_subplot(111)

        # å…¨åŸŸæŠ—é‹¸é½’è¨­å®š
        plt.rcParams['lines.antialiased'] = True
        plt.rcParams['patch.antialiased'] = True
        fig.patch.set_antialiased(True)

        # ========== ç¹ªåœ–ä¸»é«” ==========
        # X è»¸è³‡æ–™ï¼šä½¿ç”¨ç­‰è·ä½ç½®ï¼Œä½†é¡¯ç¤ºå¯¦éš›æ™‚é–“æ¨™ç±¤
        x = range(len(chart_data))  # ç­‰è·ä½ç½®
        
        if 'date' in chart_data.columns:
            # è½‰æ›æ—¥æœŸä¸¦ä½¿ç”¨çµ±ä¸€æ ¼å¼ yyyy/m/d hh:mm
            dates = pd.to_datetime(chart_data['date'])
            date_format = '%Y/%m/%d %H:%M'

            # è¨­å®š X è»¸æ¨™ç±¤ç‚ºå¯¦éš›æ™‚é–“ï¼Œä½†ä½ç½®æ˜¯ç­‰è·çš„
            date_labels = [d.strftime(date_format) for d in dates]
            
            # è‡ªå‹•è¨ˆç®—æœ€ä½³æ¨™ç±¤æ•¸é‡ï¼ˆå‚ç›´é¡¯ç¤ºï¼Œä¸»è¦è€ƒæ…®è¦–è¦ºå¯†åº¦ï¼‰
            n_points = len(chart_data)
            
            # å‚ç›´é¡¯ç¤ºå…è¨±æ›´å¤šæ¨™ç±¤ï¼Œç›¡é‡å¤šé¡¯ç¤ºæ™‚é–“è³‡è¨Š
            if n_points <= 15:
                optimal_labels = n_points  # å°‘æ–¼ 15 å€‹é»ï¼Œå…¨éƒ¨é¡¯ç¤º
            elif n_points <= 50:
                optimal_labels = min(n_points, int(n_points / 1))  # é¡¯ç¤ºç´„ 2/3 çš„é»
            else:
                optimal_labels = min(n_points, 30)  # æœ€å¤šé¡¯ç¤º 30 å€‹æ¨™ç±¤
            
            # ç¢ºä¿è‡³å°‘é¡¯ç¤º 2 å€‹æ¨™ç±¤ï¼ˆé¦–å°¾ï¼‰
            optimal_labels = max(2, optimal_labels)
            
            # ç¸½æ˜¯ç¢ºä¿åˆ»åº¦ä½ç½®åœ¨è¦–è¦ºä¸Šå‡å‹»åˆ†å¸ƒ
            if n_points <= optimal_labels:
                # é»æ•¸å°‘æ–¼æœ€ä½³æ¨™ç±¤æ•¸ï¼Œå…¨éƒ¨é¡¯ç¤º
                tick_positions = list(x)  # x æœ¬èº«å°±æ˜¯ range(n_points)ï¼Œå·²ç¶“ç­‰è·
                tick_labels = date_labels
            else:
                # åœ¨ç­‰è·çš„ X è»¸ä½ç½®ä¸Šå‡å‹»é¸æ“‡åˆ»åº¦
                # x æ˜¯ range(n_points)ï¼Œå³ [0, 1, 2, ..., n_points-1]
                # æˆ‘å€‘è¦åœ¨é€™å€‹ç­‰è·åºåˆ—ä¸Šå‡å‹»é¸æ“‡ optimal_labels å€‹ä½ç½®
                
                # è¨ˆç®—å‡å‹»é–“éš”
                x_min = 0
                x_max = n_points - 1
                uniform_positions = np.linspace(x_min, x_max, optimal_labels)
                
                # å°‡æµ®é»ä½ç½®å››æ¨äº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•¸ä½ç½®
                tick_indices = [int(round(pos)) for pos in uniform_positions]
                
                # å»é™¤é‡è¤‡ä¸¦ç¢ºä¿åœ¨æœ‰æ•ˆç¯„åœå…§
                tick_indices = sorted(list(set(tick_indices)))
                tick_indices = [idx for idx in tick_indices if 0 <= idx < n_points]
                
                # ç”Ÿæˆå°æ‡‰çš„ä½ç½®å’Œæ¨™ç±¤
                tick_positions = tick_indices  # ç›´æ¥ä½¿ç”¨æ•´æ•¸ä½ç½®ï¼Œé€™äº›å·²ç¶“æ˜¯ç­‰è·çš„
                tick_labels = [date_labels[i] for i in tick_indices]
            
            ax.set_xticks(tick_positions)
            ax.set_xticklabels(tick_labels, rotation=90, ha='center', fontsize=9)  # rotation=90 å‚ç›´é¡¯ç¤º

        y = chart_data['value'].values
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ ByTool æ¬„ä½ä¸¦ç¹ªè£½åˆ†è‰²åœ–
        if 'ByTool' in chart_data.columns:
            # éæ¿¾æ‰ NaN çš„ Tool è³‡æ–™
            chart_data_with_tool = chart_data[pd.notna(chart_data['ByTool'])].copy()
            
            if not chart_data_with_tool.empty:
                # æº–å‚™é¡è‰²æ˜ å°„
                tools = sorted([str(t) for t in chart_data_with_tool['ByTool'].unique() if pd.notna(t)])
                color_cycle = ['#2563eb', '#dc2626', '#16a34a', '#f59e0b', '#7c3aed', '#0891b2']
                tool_colors = {t: color_cycle[i % len(color_cycle)] for i, t in enumerate(tools)}
                
                # å…ˆç¹ªè£½é€£ç·šï¼ˆç°è‰²åº•ç·šï¼‰
                ax.plot(x, y, '-', color="#808182", linewidth=1, alpha=0.9, antialiased=True, zorder=1)
                
                # æŒ‰ Tool åˆ†è‰²ç¹ªè£½é»
                for tool in tools:
                    mask = chart_data['ByTool'].astype(str) == tool
                    if mask.any():
                        x_tool = [x[i] for i in range(len(x)) if mask.iloc[i]]
                        y_tool = [y[i] for i in range(len(y)) if mask.iloc[i]]
                        ax.scatter(x_tool, y_tool, color=tool_colors[tool], s=45, alpha=0.8, 
                                   label=tool, edgecolors='white', linewidth=0.5, zorder=3)
                
                # æ·»åŠ  legend
                ax.legend(loc='upper left', fontsize=7, frameon=True, ncol=3, 
                          framealpha=0.9, edgecolor='#d1d5db')
            else:
                # æ²’æœ‰æœ‰æ•ˆçš„ Tool è³‡æ–™ï¼Œä½¿ç”¨åŸå§‹ç¹ªè£½æ–¹å¼
                ax.plot(x, y, 'bo-', markersize=3, linewidth=1, alpha=0.8, antialiased=True)
        else:
            # æ²’æœ‰ ByTool æ¬„ä½ï¼Œä½¿ç”¨åŸå§‹ç¹ªè£½æ–¹å¼
            ax.plot(x, y, 'bo-', markersize=3, linewidth=1, alpha=0.8, antialiased=True)

        # åˆ¤æ–·æ˜¯å¦åªé¡¯ç¤º Sug (ç•¶ Ori èˆ‡ Sug å·®è·å¤ªå¤§æ™‚)
        show_only_sug = False
        if not np.isnan(suggest_ucl) and not np.isnan(suggest_lcl):
            ori_range = abs(chart_info['UCL'] - chart_info['LCL'])
            sug_range = abs(suggest_ucl - suggest_lcl)
            # å¦‚æœ Ori ç¯„åœæ˜¯ Sug ç¯„åœçš„ 3 å€ä»¥ä¸Šï¼Œåªé¡¯ç¤º Sug
            if ori_range > sug_range * 3:
                show_only_sug = True

        # å„ç¨®ç®¡åˆ¶ç·š
        # åˆ¤æ–· Target æ˜¯å¦è·é›¢ Suggest ç®¡åˆ¶ç·šå¤ªé ï¼ˆé¿å…å½±éŸ¿åœ–è¡¨ scaleï¼‰
        show_target = True
        if "Hard Rule" not in pattern and not np.isnan(suggest_ucl) and not np.isnan(suggest_lcl):
            target_val = chart_info['Target']
            sug_range = suggest_ucl - suggest_lcl
            
            # å¦‚æœ Target è¶…å‡º Suggest ç®¡åˆ¶ç·šç¯„åœçš„ 1.5 å€è·é›¢ï¼Œä¸é¡¯ç¤º
            if target_val > suggest_ucl + sug_range * 1.5 or target_val < suggest_lcl - sug_range * 1.5:
                show_target = False
                print(f"    [Debug] Target ({target_val:.3f}) è·é›¢ Suggest ç®¡åˆ¶ç·šå¤ªé ï¼Œä¸é¡¯ç¤ºæ–¼åœ–è¡¨")
        
        # Hard Rule æ™‚ä¸é¡¯ç¤º Target ç·šï¼Œé¿å…è¦–è¦ºæ··äº‚
        if show_target and "Hard Rule" not in pattern:
            ax.axhline(y=chart_info['Target'], color='gray', linestyle='-', linewidth=1)
        
        # åªåœ¨å·®è·ä¸å¤§æ™‚é¡¯ç¤º Ori UCL/LCL
        if not show_only_sug:
            ax.axhline(y=chart_info['UCL'], color='red', linestyle='--', linewidth=2)
            ax.axhline(y=chart_info['LCL'], color='red', linestyle='--', linewidth=2)

        if not np.isnan(suggest_ucl):
            ax.axhline(y=suggest_ucl, color='#555555', linestyle='-', linewidth=1.5)
        if not np.isnan(suggest_lcl):
            ax.axhline(y=suggest_lcl, color='#555555', linestyle='-', linewidth=1.5)

        # TSMC ç®¡åˆ¶ç·šï¼ˆç¶ è‰²è™›ç·šï¼‰
        if tsmc_ucl is not None and not np.isnan(tsmc_ucl):
            ax.axhline(y=tsmc_ucl, color='green', linestyle='--', linewidth=2, label='TSMC UCL')
        if tsmc_lcl is not None and not np.isnan(tsmc_lcl):
            ax.axhline(y=tsmc_lcl, color='green', linestyle='--', linewidth=2, label='TSMC LCL')

        # Static ç·šå·²ç§»é™¤ï¼Œä¸å†ç¹ªè£½

        # ======= æ¨™é¡Œ =======
        # ä½¿ç”¨è¨ˆç®—çµæœçš„ patternï¼Œè€Œé Excel çš„ ExpectedPattern
        sug_ucl_text = f"Sug_UCL: {suggest_ucl}" if not np.isnan(suggest_ucl) else "Sug_UCL: N/A"
        sug_lcl_text = f"Sug_LCL: {suggest_lcl}" if not np.isnan(suggest_lcl) else "Sug_LCL: N/A"
        
        # å°‡é»æ•¸çµ±è¨ˆæ‹†é–‹é¡¯ç¤ºï¼šTotal Cnt (å…©å¹´å…§ç¸½é»æ•¸) å’Œ Cal Cnt (å¯¦éš›è¨ˆç®—é»æ•¸)
        total_cnt = total_data_count if total_data_count is not None else len(chart_data)
        used_cnt = used_data_count if used_data_count is not None else len(chart_data)
        
        title = (f"{chart_info['GroupName']}@{chart_info['ChartName']}@{chart_info['Characteristics']}\n"
                f"Pattern: {pattern} | Total Cnt: {total_cnt} | Cal Cnt: {used_cnt} | {sug_ucl_text} | {sug_lcl_text}")
        ax.set_title(title, fontsize=11)
        ax.grid(False)

        # ======= èª¿æ•´ Y è»¸ç¯„åœï¼ˆç‰¹æ®Šè™•ç† Constant/Near Constant/Hard Ruleï¼‰=======
        if pattern in ["Constant", "Near Constant"] or "Hard Rule" in pattern:
            # Constant/Near Constant/Hard Rule æ¨¡å¼ï¼šåŒ…å«æ‰€æœ‰é»ï¼Œä½†åˆç†è¨­å®š Y è»¸ç¯„åœ
            y_data_min = y.min()
            y_data_max = y.max()
            
            # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœç¯„åœå¤ªå°ï¼ˆæ‰€æœ‰é»éƒ½å¾ˆæ¥è¿‘ï¼‰ï¼Œé©ç•¶æ“´å±• Y è»¸
            data_range = y_data_max - y_data_min
            if data_range < 1e-6:  # å¹¾ä¹æ˜¯å¸¸æ•¸
                center = (y_data_min + y_data_max) / 2
                margin = max(abs(center) * 0.1, 1.0)  # è‡³å°‘ 1 å€‹å–®ä½çš„ç¯„åœ
                y_data_min = center - margin
                y_data_max = center + margin
            print(f"[{pattern}] åŒ…å«æ‰€æœ‰é»ï¼ŒY è»¸ç¯„åœ: [{y_data_min:.3f}, {y_data_max:.3f}]")
        else:
            # ä¸€èˆ¬æ¨¡å¼ï¼šä½¿ç”¨å››åˆ†ä½æ•¸æ³•æ’é™¤æ¥µç«¯å€¼
            q1 = np.percentile(y, 25)
            q3 = np.percentile(y, 75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            # éæ¿¾æ‰ç•°å¸¸å€¼å¾Œçš„æ•¸æ“šç¯„åœ
            y_filtered = y[(y >= lower_bound) & (y <= upper_bound)]
            if len(y_filtered) > 0:
                y_data_min = y_filtered.min()
                y_data_max = y_filtered.max()
            else:
                # å¦‚æœå…¨éƒ¨éƒ½æ˜¯ç•°å¸¸å€¼ï¼Œé‚„æ˜¯ä½¿ç”¨åŸå§‹ç¯„åœ
                y_data_min = y.min()
                y_data_max = y.max()
        
        # åˆå§‹åŒ– all_linesï¼Œæ ¹æ“š show_target æ±ºå®šæ˜¯å¦åŒ…å« Target
        all_lines = [y_data_min, y_data_max]
        if show_target and "Hard Rule" not in pattern:
            all_lines.append(chart_info['Target'])
        
        # æ ¹æ“šæ˜¯å¦åªé¡¯ç¤º Sug ä¾†æ±ºå®šåŒ…å«å“ªäº›ç·š
        if show_only_sug:
            # åªé¡¯ç¤º Sug æ™‚ï¼Œä¸åŒ…å« Ori UCL/LCL
            for v in [suggest_ucl, suggest_lcl]:
                if not np.isnan(v):
                    all_lines.append(v)
        else:
            # æ­£å¸¸æƒ…æ³ï¼ŒåŒ…å«æ‰€æœ‰ç·š
            all_lines.extend([chart_info['UCL'], chart_info['LCL']])
            for v in [suggest_ucl, suggest_lcl]:
                if not np.isnan(v):
                    all_lines.append(v)
        
        # è¨ˆç®— Y è»¸ç¯„åœï¼Œè™•ç† Hard Rule æƒ…æ³ä¸‹æ‰€æœ‰ç·šé‡ç–Šçš„å•é¡Œ
        if max(all_lines) == min(all_lines):
            # æ‰€æœ‰ç·šé‡ç–Šæ™‚ï¼Œä½¿ç”¨æ•¸æ“šç¯„åœè¨­å®š Y è»¸
            center_line = max(all_lines)
            data_range = y_data_max - y_data_min
            if data_range == 0:
                # æ•¸æ“šä¹Ÿæ˜¯å¸¸æ•¸æ™‚ï¼Œä½¿ç”¨å›ºå®šç¯„åœ
                y_margin = abs(center_line) * 0.1 if center_line != 0 else 1.0
            else:
                y_margin = data_range * 0.2
            ax.set_ylim(center_line - y_margin, center_line + y_margin)
        else:
            y_margin = (max(all_lines) - min(all_lines)) * 0.1
            ax.set_ylim(min(all_lines) - y_margin, max(all_lines) + y_margin)

        plt.tight_layout(rect=[0, 0, 0.85, 1])

        # ======= å¤–å´æ¨™è¨» =======
        annotations = []
        
        # æ ¹æ“š show_target æ±ºå®šæ˜¯å¦é¡¯ç¤º Target æ¨™è¨»
        if show_target and "Hard Rule" not in pattern:
            annotations.append((chart_info['Target'], f"Target = {chart_info['Target']}", 'gray', 'normal'))

        # æª¢æŸ¥é‡ç–Šç·šä¸¦åˆä½µæ¨™è¨»
        tolerance = 1e-6  # åˆ¤æ–·ç·šé‡ç–Šçš„å®¹å·®å€¼
        
        # æ”¶é›†æ‰€æœ‰UCLç·šçš„è³‡è¨Š
        ucl_lines = []
        if not show_only_sug and not np.isnan(chart_info['UCL']):
            ucl_lines.append(('Ori', chart_info['UCL'], 'red', 'normal'))
        if not np.isnan(suggest_ucl):
            ucl_lines.append(('Sug', suggest_ucl, 'black', 'bold'))
        
        # æ”¶é›†æ‰€æœ‰LCLç·šçš„è³‡è¨Š
        lcl_lines = []
        if not show_only_sug and not np.isnan(chart_info['LCL']):
            lcl_lines.append(('Ori', chart_info['LCL'], 'red', 'normal'))
        if not np.isnan(suggest_lcl):
            lcl_lines.append(('Sug', suggest_lcl, 'black', 'bold'))
        
        # è™•ç†UCLé‡ç–Š
        ucl_groups = []
        for line in ucl_lines:
            placed = False
            for group in ucl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                ucl_groups.append([line])
        
        # è™•ç†LCLé‡ç–Š
        lcl_groups = []
        for line in lcl_lines:
            placed = False
            for group in lcl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                lcl_groups.append([line])
        
        # æª¢æŸ¥ UCL å’Œ LCL ä¹‹é–“çš„é‡ç–Šï¼ˆè™•ç†å¸¸æ•¸/å¡å®šå€¼æƒ…æ³ï¼‰
        all_cl_groups = []
        
        # å°‡æ‰€æœ‰ UCL å’Œ LCL ç·šåˆä½µåˆ°ä¸€èµ·æª¢æŸ¥
        all_lines = []
        for name, value, color, weight in ucl_lines:
            all_lines.append((name, value, color, weight, 'UCL'))
        for name, value, color, weight in lcl_lines:
            all_lines.append((name, value, color, weight, 'LCL'))
        
        # é‡æ–°åˆ†çµ„ï¼ŒåŒ…æ‹¬è·¨ UCL/LCL çš„é‡ç–Š
        for line in all_lines:
            placed = False
            for group in all_cl_groups:
                if abs(line[1] - group[0][1]) < tolerance:
                    group.append(line)
                    placed = True
                    break
            if not placed:
                all_cl_groups.append([line])
        
        # ç”Ÿæˆåˆä½µå¾Œçš„æ¨™è¨»
        for group in all_cl_groups:
            if len(group) == 1:
                name, value, color, weight, cl_type = group[0]
                annotations.append((value, f"{name} {cl_type} = {value}", color, weight))
            else:
                value = group[0][1]
                # ä½¿ç”¨æœ€é‡è¦çš„é¡è‰²ï¼ˆå„ªå…ˆé †åºï¼šorange > red > purpleï¼‰
                color = 'orange' if any(line[2] == 'orange' for line in group) else \
                       'red' if any(line[2] == 'red' for line in group) else 'purple'
                weight = 'bold' if any(line[3] == 'bold' for line in group) else 'normal'
                
                # åˆ†åˆ¥æ”¶é›† UCL å’Œ LCL çš„åç¨±
                ucl_names = [line[0] for line in group if line[4] == 'UCL']
                lcl_names = [line[0] for line in group if line[4] == 'LCL']
                
                # ç”Ÿæˆåˆä½µæ¨™ç±¤
                if ucl_names and lcl_names:
                    # UCL å’Œ LCL éƒ½æœ‰é‡ç–Šï¼Œé¡¯ç¤ºç‚º Sug_UCL=Sug_LCL æ ¼å¼
                    ucl_part = '='.join(ucl_names) + '_UCL' if len(ucl_names) > 1 else ucl_names[0] + '_UCL'
                    lcl_part = '='.join(lcl_names) + '_LCL' if len(lcl_names) > 1 else lcl_names[0] + '_LCL'
                    combined_label = f"{ucl_part}={lcl_part}"
                elif ucl_names:
                    # åªæœ‰ UCL é‡ç–Š
                    combined_label = '='.join(ucl_names) + ' UCL'
                else:
                    # åªæœ‰ LCL é‡ç–Š
                    combined_label = '='.join(lcl_names) + ' LCL'
                
                annotations.append((value, f"{combined_label} = {value}", color, weight))

        for y_val, text, color, weight in annotations:
            ax.text(1.02, y_val, text, transform=ax.get_yaxis_transform(),
                    color=color, fontsize=9, va='center', fontweight=weight, clip_on=False)

        # ======= è¼¸å‡ºåœ–æª” - åªä¿å­˜ PNG æ ¼å¼ =======
        filename_png = os.path.join(output_dir, f"{chart_info['GroupName']}_{chart_info['ChartName']}.png")
        
        # ğŸ“‰ [Bug Fix] ä¿®æ­£è¨˜æ†¶é«”æ´©æ¼ï¼šæ˜ç¢ºæŒ‡å®š fig ä¸¦æ”¾åœ¨ finally å€å¡Š
        try:
            # ä¿å­˜ PNGï¼ˆæ‰“åŒ…ç’°å¢ƒä¸‹æœ€ç©©å®šï¼Œä¸”ç‚ºå”¯ä¸€å¯¦éš›ä½¿ç”¨çš„æ ¼å¼ï¼‰
            plt.savefig(filename_png, dpi=200, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            print(f"[Info] PNG åœ–ç‰‡å·²ä¿å­˜: {filename_png}")
                
        except Exception as e:
            print(f"[Error] ç¹ªåœ–ä¿å­˜å¤±æ•—: {e}")
            raise e
        finally:
            # å¼·åˆ¶æ¸…ç† matplotlib è³‡æº
            plt.clf()  # æ¸…é™¤ç•¶å‰åœ–å½¢
            plt.close(fig)  # æ˜ç¢ºé—œé–‰ fig ç‰©ä»¶
            plt.close('all')  # é—œé–‰æ‰€æœ‰åœ–å½¢
            
            # å¼·åˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()

        return filename_png  # âœ… è¿”å› PNG è·¯å¾‘ï¼ˆå”¯ä¸€å¯¦éš›ä½¿ç”¨çš„æ ¼å¼ï¼‰

    def process_single_chart_data(self, chart_info_row, raw_data_df):
        """å°‡ Chart è¨­å®šèˆ‡åŸå§‹æ•¸æ“šåˆä½µä¸¦é‹è¡Œæ ¸å¿ƒ SOP è¨ˆç®—"""
        
        group_name = chart_info_row.get('GroupName', 'N/A')
        chart_name = chart_info_row.get('ChartName', 'N/A')
        
        print(f"    [Debug] é–‹å§‹è™•ç† {group_name}_{chart_name}")
        print(f"    [Debug] åŸå§‹ CSV æ•¸æ“š shape: {raw_data_df.shape}")
        print(f"    [Debug] åŸå§‹ CSV æ¬„ä½: {list(raw_data_df.columns)}")
        
        # 0. å¼·åˆ¶è½‰æ› point_val ç‚ºæ•¸å­—å‹åˆ¥ï¼ˆé›™é‡ä¿éšªï¼‰
        if 'point_val' in raw_data_df.columns:
            raw_data_df['point_val'] = pd.to_numeric(raw_data_df['point_val'], errors='coerce')
            # æª¢æŸ¥è½‰æ›å¾Œæ˜¯å¦æœ‰ NaNï¼ˆä»£è¡¨è½‰æ›å¤±æ•—ï¼‰
            nan_count = raw_data_df['point_val'].isna().sum()
            if nan_count > 0:
                print(f"    [Warning] point_val è½‰æ›å¾Œæœ‰ {nan_count} ç­†éæ•¸å­—è³‡æ–™è¢«è½‰ç‚º NaN")
        
        if len(raw_data_df) > 0 and 'point_val' in raw_data_df.columns:
            try:
                valid_vals = raw_data_df['point_val'].dropna()
                if len(valid_vals) > 0:
                    print(f"    [Debug] æ•¸æ“šç¯„åœ: {valid_vals.min():.4f} ~ {valid_vals.max():.4f} (æœ‰æ•ˆç­†æ•¸: {len(valid_vals)}/{len(raw_data_df)})")
                else:
                    print(f"    [Debug] è­¦å‘Šï¼šæ‰€æœ‰ point_val éƒ½æ˜¯ç„¡æ•ˆæ•¸å­—ï¼")
            except Exception as e:
                print(f"    [Debug] ç„¡æ³•è¨ˆç®—æ•¸æ“šç¯„åœ: {e}")
        else:
            print(f"    [Debug] CSV ç‚ºç©ºæˆ–ç¼ºå°‘ point_val æ¬„ä½")
        
        # 1. æ¬„ä½é‡å‘½å
        raw_data_df.rename(columns={'point_time': 'date', 'point_val': 'value'}, inplace=True)
        
        # 2. å°‡æ‰€æœ‰ç›¸é—œåƒæ•¸å¾ Chart Info å‚³éçµ¦ Raw Data DataFrame
        usl = chart_info_row.get('USL')
        lsl = chart_info_row.get('LSL')
        
        raw_data_df['USL'] = usl
        raw_data_df['LSL'] = lsl
        raw_data_df['DetectionLimit'] = chart_info_row.get('DetectionLimit')
        raw_data_df['Target'] = chart_info_row.get('Target')
        raw_data_df['UCL'] = chart_info_row.get('UCL')
        raw_data_df['LCL'] = chart_info_row.get('LCL')
        raw_data_df['tsmc_ucl'] = chart_info_row.get('tsmc_ucl', np.nan)
        raw_data_df['tsmc_lcl'] = chart_info_row.get('tsmc_lcl', np.nan)
        
        # 3. å‹•æ…‹è¨ˆç®— oos_flag - æ ¹æ“šç‰¹æ€§é¡å‹æ±ºå®šæª¢æŸ¥å“ªå€‹è¦æ ¼é™
        characteristic = chart_info_row.get('Characteristics', 'Nominal')
        print(f"    [Debug] USL: {usl}, LSL: {lsl}, ç‰¹æ€§: {characteristic}")
        
        # æ ¹æ“šä¸åŒç‰¹æ€§é¡å‹è¨­å®š OOS æª¢æŸ¥é‚è¼¯
        if characteristic == 'Bigger':
            # Bigger chart åªéœ€è¦ LSLï¼Œä¸æª¢æŸ¥ USL
            if lsl is not None and not np.isnan(lsl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] < lsl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Bigger chart - åªæª¢æŸ¥ LSLï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Bigger chart - ç„¡æœ‰æ•ˆ LSLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
        elif characteristic == 'Smaller':
            # Smaller chart åªéœ€è¦ USLï¼Œä¸æª¢æŸ¥ LSL  
            if usl is not None and not np.isnan(usl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] > usl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Smaller chart - åªæª¢æŸ¥ USLï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Smaller chart - ç„¡æœ‰æ•ˆ USLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
        else:
            # Nominal æˆ–å…¶ä»–é¡å‹ï¼šæª¢æŸ¥é›™é‚Šè¦æ ¼é™
            if (usl is not None and not np.isnan(usl)) and \
               (lsl is not None and not np.isnan(lsl)) and \
               (usl > lsl):
                raw_data_df['oos_flag'] = (raw_data_df['value'] > usl) | (raw_data_df['value'] < lsl)
                oos_count = raw_data_df['oos_flag'].sum()
                print(f"    [Debug] Nominal chart - æª¢æŸ¥é›™é‚Šè¦æ ¼é™ï¼ŒOOS é»æ•¸: {oos_count}/{len(raw_data_df)}")
            else:
                raw_data_df['oos_flag'] = False
                print(f"    [Debug] Nominal chart - ç„¡æœ‰æ•ˆ USL/LSLï¼Œè¨­ç½®æ‰€æœ‰é»ç‚ºé OOS")
            
        print(f"    [Debug] æº–å‚™é€²å…¥æ ¸å¿ƒè¨ˆç®—ï¼Œç‰¹æ€§: {chart_info_row.get('Characteristics', 'Nominal')}")
        
        try:
            # 4. é‹è¡Œæ ¸å¿ƒè¨ˆç®—
            results = self.process_chart(
                df=raw_data_df,
                value_col='value',
                date_col='date',
                oos_col='oos_flag',
                characteristic=chart_info_row.get('Characteristics', 'Nominal')
            )
            
            # 5. æ ¼å¼åŒ–è¼¸å‡º
            final_output = chart_info_row.to_dict()
            final_output.update(results)
            final_output['Status'] = 'Success'
            
            # è®“èˆŠ UCL/LCL æ¬„ä½ä»ç‚º Suggest çš„å€¼
            final_output['Original UCL'] = final_output['Suggest UCL']
            final_output['Original LCL'] = final_output['Suggest LCL']
            
            # ï¿½ [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] äºŒæ¬¡ç¢ºèªï¼šç¢ºä¿æ‰€æœ‰è¼¸å‡ºå€¼éƒ½ç¬¦åˆ Resolution
            res_val = results.get('Resolution_Estimated')
            pattern_str = str(final_output.get('Pattern', ''))
            is_hard_rule = pattern_str.startswith('Hard Rule')
            
            print(f"\n    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] === é–‹å§‹æª¢æŸ¥ ===")
            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] Pattern: {pattern_str}")
            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] Resolution: {res_val}")
            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] Is Hard Rule: {is_hard_rule}")
            
            if is_hard_rule:
                print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] âœ… Hard Rule è·³éç²¾åº¦ä¿®æ­£")
            elif res_val is None or res_val <= 0:
                print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] âš ï¸ Resolution ç„¡æ•ˆï¼Œè·³éç²¾åº¦ä¿®æ­£")
            else:
                target_decimals = self.calculate_decimals_from_resolution(res_val)
                print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] ç›®æ¨™å°æ•¸ä½æ•¸: {target_decimals}")
                
                # âœ… åªå°è¨ˆç®—ç”¢ç”Ÿçš„ CL å€¼é€²è¡ŒäºŒæ¬¡ç¢ºèªï¼ˆä¸åŒ…å« Target/USL/LSLï¼‰
                cl_cols_to_check = ['Suggest UCL', 'Suggest LCL', 'Static UCL', 'Static LCL', 
                                   'Original UCL', 'Original LCL', 'CL_Center']
                
                precision_issues_found = False
                
                for key in cl_cols_to_check:
                    if key in final_output and pd.notna(final_output[key]):
                        old_val = final_output[key]
                        
                        # æª¢æŸ¥æ˜¯å¦å·²ç¶“å°é½Šï¼ˆå®¹å¿æµ®é»èª¤å·®ï¼‰
                        expected_val = round(float(old_val), target_decimals)
                        
                        if abs(float(old_val) - float(expected_val)) > 1e-10:
                            precision_issues_found = True
                            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] âš ï¸ {key} ç²¾åº¦ä¸å°: {old_val:.12f} â†’ {expected_val}")
                            final_output[key] = expected_val
                            
                            # å¦‚æœæ˜¯ 0 ä½å°æ•¸ï¼Œè½‰ç‚ºæ•´æ•¸
                            if target_decimals == 0:
                                final_output[key] = int(expected_val)
                        else:
                            # ç²¾åº¦å·²å°é½Š
                            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] âœ… {key}: {old_val} (å·²å°é½Š)")
                
                if not precision_issues_found:
                    print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] âœ… æ‰€æœ‰ CL å€¼ç²¾åº¦å·²æ­£ç¢ºå°é½Š")
                
                # Sigma ç›¸é—œæ¬„ä½ï¼ˆç¶­æŒ 6 ä½å°æ•¸ï¼‰
                # Sigma ç›¸é—œæ¬„ä½ï¼ˆç¶­æŒ 6 ä½å°æ•¸ï¼‰
                for sigma_key in ['Sigma_Est', 'Sigma_Est_Upper', 'Sigma_Est_Lower']:
                    if sigma_key in final_output and not np.isnan(final_output.get(sigma_key, np.nan)):
                        final_output[sigma_key] = round(final_output[sigma_key], 6)
                
                # K å€æ•¸æ¬„ä½ï¼ˆç¶­æŒ 3 ä½å°æ•¸ï¼‰
                for k_set_key in ['Original_UCL_K_Set', 'Original_LCL_K_Set',
                                 'Suggest_UCL_K_Set', 'Suggest_LCL_K_Set',
                                 'Ori_K_Set', 'Sug_K_Set']:
                    if k_set_key in final_output and not np.isnan(final_output.get(k_set_key, np.nan)):
                        final_output[k_set_key] = round(final_output[k_set_key], 3)
            
            print(f"    [æœ€çµ‚ç²¾åº¦æª¢æŸ¥] === æª¢æŸ¥å®Œæˆ ===\n")
            
            # 6. ç”Ÿæˆç®¡åˆ¶åœ–
            try:
                # ğŸ”¥ ä¿®å¾©å•é¡Œ1: åªå‚³éç¶“éæ™‚é–“ç¯©é¸çš„æ•¸æ“šçµ¦ç¹ªåœ–å‡½æ•¸
                # æ ¹æ“šè¨­å®šçš„æ™‚é–“ç¯„åœç¯©é¸æ•¸æ“šç”¨æ–¼ç¹ªåœ–
                if 'date' in raw_data_df.columns:
                    # è½‰æ›æ—¥æœŸåˆ—
                    raw_data_df['date'] = pd.to_datetime(raw_data_df['date'], errors='coerce')
                    
                    # æ‡‰ç”¨èˆ‡ data_integrity ç›¸åŒçš„æ™‚é–“ç¯©é¸é‚è¼¯
                    if self.start_date is not None and self.end_date is not None:
                        # ä½¿ç”¨è‡ªè¨‚æ—¥æœŸç¯„åœ
                        cutoff_start = pd.Timestamp(self.start_date)
                        cutoff_end = pd.Timestamp(self.end_date)
                        filtered_chart_data = raw_data_df[
                            (raw_data_df['date'] >= cutoff_start) & 
                            (raw_data_df['date'] <= cutoff_end)
                        ].copy()
                        print(f"    [ç¹ªåœ–] ä½¿ç”¨è‡ªè¨‚æ—¥æœŸç¯„åœ: {cutoff_start.date()} è‡³ {cutoff_end.date()}ï¼Œè³‡æ–™ç­†æ•¸: {len(filtered_chart_data)}")
                    else:
                        # ä½¿ç”¨é è¨­çš„æœ€è¿‘2å¹´
                        cutoff = pd.Timestamp.today() - pd.DateOffset(years=2)
                        filtered_chart_data = raw_data_df[raw_data_df['date'] >= cutoff].copy()
                        print(f"    [ç¹ªåœ–] ä½¿ç”¨é è¨­æ—¥æœŸç¯„åœ: æœ€è¿‘2å¹´ï¼Œè³‡æ–™ç­†æ•¸: {len(filtered_chart_data)}")
                    
                    # æ’é™¤ OOS é»ï¼ˆèˆ‡è¨ˆç®—é‚è¼¯ä¸€è‡´ï¼‰
                    if 'oos_flag' in filtered_chart_data.columns:
                        filtered_chart_data = filtered_chart_data[~filtered_chart_data['oos_flag'].astype(bool)].copy()
                        print(f"    [ç¹ªåœ–] æ’é™¤ OOS é»å¾Œè³‡æ–™ç­†æ•¸: {len(filtered_chart_data)}")
                else:
                    # æ²’æœ‰æ—¥æœŸæ¬„ä½ï¼Œä½¿ç”¨å…¨éƒ¨æ•¸æ“š
                    filtered_chart_data = raw_data_df.copy()
                    print(f"    [ç¹ªåœ–] ç„¡æ—¥æœŸæ¬„ä½ï¼Œä½¿ç”¨å…¨éƒ¨æ•¸æ“š: {len(filtered_chart_data)}")
                
                # æª¢æŸ¥ç¯©é¸å¾Œæ˜¯å¦é‚„æœ‰æ•¸æ“š
                if len(filtered_chart_data) == 0:
                    print(f"     [Warning] ç¶“éæ™‚é–“ç¯©é¸å¾Œç„¡æ•¸æ“šï¼Œè·³éç¹ªåœ–")
                    final_output['PlotFile'] = 'No Data After Filtering'
                else:
                    plot_filename = self.plot_control_chart(
                        chart_data=filtered_chart_data,
                        chart_info=chart_info_row,
                        suggest_ucl=final_output['Suggest UCL'],
                        suggest_lcl=final_output['Suggest LCL'],
                        static_ucl=final_output['Static UCL'],
                        static_lcl=final_output['Static LCL'],
                        cl_center=final_output['CL_Center'],
                        pattern=final_output['Pattern'],
                        total_data_count=final_output.get('TotalDataCount', len(raw_data_df)),
                        used_data_count=final_output.get('DataCountUsed', 0),
                        tsmc_ucl=chart_info_row.get('tsmc_ucl', np.nan),
                        tsmc_lcl=chart_info_row.get('tsmc_lcl', np.nan)
                    )
                    final_output['PlotFile'] = plot_filename if plot_filename else 'Plot Failed'
            except Exception as plot_error:
                import traceback
                print(f"     [Warning] ç¹ªåœ–å¤±æ•—: {plot_error}")
                traceback.print_exc()
                final_output['PlotFile'] = 'Plot Failed'
            
            return final_output
            
        except Exception as e:
            import traceback
            print(f"     [Error] é‹è¡Œæ ¸å¿ƒè¨ˆç®—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"     [Error] è©³ç´°éŒ¯èª¤è¿½è¸ª:")
            traceback.print_exc()
            return {
                'ChartName': chart_info_row['ChartName'], 
                'Status': 'Calculation Error', 
                'ErrorMessage': str(e),
                'PlotFile': 'Calculation Error'
            }

    def run_calculation(self, output_filename='CL_Calculation_Results.xlsx', progress_callback=None):
        """åŸ·è¡Œå®Œæ•´çš„ CL è¨ˆç®—æµç¨‹"""
        
        if not self.chart_info_path or not os.path.exists(self.chart_info_path):
            raise ValueError(f"åœ–è¡¨è³‡è¨Šæª”æ¡ˆä¸å­˜åœ¨: {self.chart_info_path}")
            
        if not self.raw_data_dir or not os.path.exists(self.raw_data_dir):
            raise ValueError(f"åŸå§‹æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {self.raw_data_dir}")

        print("--- 1. è¼‰å…¥åœ–è¡¨é…ç½® ---")
        all_charts_info = self.load_chart_information(self.chart_info_path)
        if all_charts_info.empty:
            raise ValueError("ç„¡æ³•è¼‰å…¥æœ‰æ•ˆçš„åœ–è¡¨é…ç½®")

        self.results = []
        total_charts = len(all_charts_info)
        
        print(f"--- 2. è™•ç† {total_charts} å¼µåœ–è¡¨çš„æ•¸æ“š ---")
        
        for i, (index, chart_info) in enumerate(all_charts_info.iterrows()):
            # æ›´æ–°é€²åº¦
            if progress_callback:
                progress_callback(i + 1, total_charts)

            group_name = chart_info.get('GroupName', 'N/A')
            chart_name = chart_info.get('ChartName', 'N/A')
            
            print(f"  > è™•ç† Chart: {group_name}_{chart_name}...")
            
            filepath = self.find_matching_file(self.raw_data_dir, group_name, chart_name)
            
            if filepath is None:
                print(f"    [Warning] æœªæ‰¾åˆ°åŒ¹é…çš„åŸå§‹æ•¸æ“šæ–‡ä»¶ã€‚è·³éã€‚")
                result = chart_info.to_dict()
                result['Status'] = 'No Raw Data'
                result['PlotFile'] = 'No Raw Data'
                self.results.append(result)
                continue

            try:
                raw_df = pd.read_csv(filepath, float_precision='round_trip')
                
                # å¼·åˆ¶è½‰æ› point_val ç‚ºæ•¸å­—å‹åˆ¥ï¼ˆå®¹éŒ¯è™•ç†ï¼‰
                if 'point_val' in raw_df.columns:
                    raw_df['point_val'] = pd.to_numeric(raw_df['point_val'], errors='coerce')
                
                results_dict = self.process_single_chart_data(chart_info, raw_df)
                self.results.append(results_dict)
                
            except Exception as e:
                print(f"    [Error] è®€å–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                error_result = chart_info.to_dict()
                error_result['Status'] = 'File Read Error'
                error_result['ErrorMessage'] = str(e)
                error_result['PlotFile'] = 'File Read Error'
                self.results.append(error_result)
                
        # --- 3. æº–å‚™è¼¸å‡ºçµæœ ---
        
        df_output = pd.DataFrame(self.results)
        
        # èª¿æ•´è¼¸å‡ºæ¬„ä½é †åº
        output_cols_priority = [
            'Figure',  # âœ… æ–°å¢ï¼šåœ–è¡¨æ¬„ä½æ”¾åœ¨ç¬¬ä¸€ä½
            'GroupName', 'ChartName', 'ChartID', 'Material_no', 
            'Status', 'ErrorMessage',
            'Suggest UCL', 'Suggest LCL', 
            'Static UCL', 'Static LCL',
            'Raw UCL After Resolution', 'Raw LCL After Resolution',
            'Sug USL', 'Sug LSL',
            'UCL', 'LCL',
            'Original UCL', 'Original LCL',
            'CL_Center', 'Sigma_Est', 'Sigma_Est_Upper', 'Sigma_Est_Lower',
            'Original_UCL_K_Set', 'Original_LCL_K_Set',
            'Suggest_UCL_K_Set', 'Suggest_LCL_K_Set',
            'Ori_K_Set', 'Sug_K_Set',
            'Ori_OOC_Count', 'Static_OOC_Count', 'Final_OOC_Count', 'Total_Adj_Units',
            'Target', 'USL', 'LSL', 
            'DetectionLimit', 
            'Pattern', 'Resolution_Estimated', 'Characteristics', 
            'TightenNeeded', 'Original_Tolerance', 'New_Tolerance', 'Diff_Ratio_%', 'Tighten_Threshold_%',
            'TotalDataCount', 'DataCountUsed', 'HardRule',
            'PlotFile' 
        ]
        
        # æ·»åŠ ç©ºçš„ Figure æ¬„ä½ï¼ˆç¨å¾Œå¯ç”¨æ–¼æ’å…¥åœ–ç‰‡ï¼‰
        df_output.insert(0, 'Figure', '')
        
        existing_output_cols = [col for col in output_cols_priority if col in df_output.columns]
        df_output = df_output[existing_output_cols]
        
        print("\n--- 4. è¨ˆç®—å®Œæˆ ---")
        print(f"æˆåŠŸè™•ç† {len(df_output)} å¼µåœ–è¡¨")
        
        # æ³¨æ„ï¼šä¸å†è‡ªå‹•è¼¸å‡º Excel å ±å‘Š
        # å¦‚éœ€è¼¸å‡ºå ±å‘Šï¼Œè«‹ä½¿ç”¨ export_results() æ–¹æ³•
        
        return df_output

    def export_results(self, results_df, output_filename='CL_Calculation_Results.xlsx'):
        """
        å°‡è¨ˆç®—çµæœåŒ¯å‡ºç‚º Excel å ±å‘Šï¼ŒåŒ…å«åœ–è¡¨æ’å…¥
        
        Args:
            results_df: è¨ˆç®—çµæœçš„ DataFrameï¼ˆä¾†è‡ª run_calculation çš„è¿”å›å€¼ï¼‰
            output_filename: è¼¸å‡ºæª”æ¡ˆåç¨±
        
        Returns:
            bool: åŒ¯å‡ºæ˜¯å¦æˆåŠŸ
        """
        if results_df is None or results_df.empty:
            print("æ²’æœ‰å¯åŒ¯å‡ºçš„çµæœ")
            return False
            
        print("\n--- é–‹å§‹åŒ¯å‡º Excel å ±å‘Š ---")
        
        try:
            import xlsxwriter
            import math
            
            # é‡æ–°æ’åˆ—æ¬„ä½é †åºï¼šåœ–ç‰‡æ¬„ä½åœ¨ç¬¬ä¸€ä½
            columns = ['Figure'] + [c for c in results_df.columns if c != 'Figure']
            
            # å‰µå»ºå·¥ä½œç°¿å’Œå·¥ä½œè¡¨
            workbook = xlsxwriter.Workbook(output_filename)
            worksheet = workbook.add_worksheet('Results')
            
            # è¨­å®šæ¬„å¯¬
            worksheet.set_column(0, 0, 50)  # Figure æ¬„ä½è¨­å¯¬ 60
            for i in range(1, len(columns)):
                if columns[i] in ['GroupName', 'ChartName']:
                    worksheet.set_column(i, i, 25)
                elif 'UCL' in columns[i] or 'LCL' in columns[i] or 'USL' in columns[i] or 'LSL' in columns[i]:
                    worksheet.set_column(i, i, 15)
                elif columns[i] == 'PlotFile':
                    worksheet.set_column(i, i, 50)
                else:
                    worksheet.set_column(i, i, 18)
            
            # è¨­å®šæ ¼å¼
            bold = workbook.add_format({'bold': True, 'align': 'center', 'valign': 'vcenter'})
            cell_format = workbook.add_format({'align': 'center', 'valign': 'vcenter'})
            
            # å¯«å…¥æ¨™é¡Œ
            for col_idx, col_name in enumerate(columns):
                worksheet.write(0, col_idx, col_name, bold)
            
            # å¯«å…¥è³‡æ–™ & æ’å…¥åœ–ç‰‡
            image_count = 0
            
            for row_idx, (_, row) in enumerate(results_df.iterrows()):
                excel_row = row_idx + 1  # Excel è¡Œè™Ÿï¼ˆè·³éæ¨™é¡Œè¡Œï¼‰
                
                # è™•ç†åœ–ç‰‡æ’å…¥
                plot_file = row.get('PlotFile', '')
                print(f"    Row {row_idx} -> Excel Row {excel_row}: PlotFile = {plot_file}")
                
                if plot_file and isinstance(plot_file, str) and plot_file not in ['No Raw Data', 'Plot Failed', 'Calculation Error', 'File Read Error']:
                    if os.path.exists(plot_file):
                        try:
                            # è¨­å®šå›ºå®šè¡Œé«˜115åƒç´ ä»¥å®¹ç´åœ–ç‰‡
                            worksheet.set_row(excel_row, 115)
                            
                            # æ’å…¥åœ–ç‰‡ï¼ˆé‡å°900pxåŸå§‹åœ–ä½¿ç”¨å›ºå®šç¸®æ”¾æ¯”ä¾‹0.35ï¼‰
                            worksheet.insert_image(excel_row, 0, plot_file, {
                                'x_scale': 0.35,
                                'y_scale': 0.35,
                                'object_position': 1,
                                'y_offset': 10
                            })
                            image_count += 1
                            print(f"      âœ“ æˆåŠŸæ’å…¥åœ–ç‰‡åˆ° Excel è¡Œ {excel_row}: {plot_file} (ç¸®æ”¾æ¯”ä¾‹: 0.18)")
                        except Exception as e:
                            print(f"      âœ— ç„¡æ³•æ’å…¥åœ–ç‰‡ {plot_file}: {e}")
                    else:
                        print(f"      âœ— æª”æ¡ˆä¸å­˜åœ¨: {plot_file}")
                
                # å¯«å…¥å…¶ä»–æ¬„ä½è³‡æ–™ï¼ˆå¾ç¬¬2æ¬„é–‹å§‹ï¼‰
                for col_idx, col_name in enumerate(columns[1:], 1):
                    val = row.get(col_name, '')
                    
                    # è™•ç† NaN/Inf/None å•é¡Œ
                    if val is None:
                        val = 'N/A'
                    elif isinstance(val, float):
                        if math.isnan(val) or math.isinf(val):
                            val = 'N/A'
                    
                    worksheet.write(excel_row, col_idx, val, cell_format)
            
            # é—œé–‰å·¥ä½œç°¿
            workbook.close()
            
            print(f"\n--- Excel å ±å‘ŠåŒ¯å‡ºå®Œæˆ ---")
            print(f"ç¸½å…±æ’å…¥ {image_count} å¼µåœ–ç‰‡")
            print(f"è¨ˆç®—çµæœå·²æˆåŠŸè¼¸å‡ºè‡³ï¼š{output_filename}")
            print(f"  - å·²æ’å…¥åœ–è¡¨åˆ° Figure æ¬„ä½")
            print(f"  - å·²è‡ªå‹•èª¿æ•´æ¬„å¯¬")
            print(f"  - æ¨™é¡Œåˆ—å·²åŠ ç²—")
            
            return True
            
        except Exception as e:
            print(f"\n--- Excel å ±å‘ŠåŒ¯å‡ºå¤±æ•— ---")
            print(f"éŒ¯èª¤è¨Šæ¯ï¼š{e}")
            return False

    def get_results(self):
        """å–å¾—è¨ˆç®—çµæœ"""
        return self.results if hasattr(self, 'results') else []

# === åŸ·è¡Œå…¥å£ ===
if __name__ == '__main__':
    calculator = CLTightenCalculator(
        chart_info_path='input/All_Chart_Information.xlsx',
        raw_data_dir='input/raw_charts/'
    )
    # åŸ·è¡Œè¨ˆç®—ï¼ˆä¸æœƒè‡ªå‹•ç”Ÿæˆå ±å‘Šï¼‰
    results_df = calculator.run_calculation()
    
    # å¦‚æœéœ€è¦åŒ¯å‡ºå ±å‘Šï¼Œæ‰‹å‹•èª¿ç”¨
    # calculator.export_results(results_df, 'CL_Calculation_Results.xlsx')